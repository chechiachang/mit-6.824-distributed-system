6.824 2018 Lecture 1: Introduction

6.824: Distributed Systems Engineering

What is a distributed system?
  multiple cooperating computers
  storage for big web sites, MapReduce, peer-to-peer sharing, &c
  lots of critical infrastructure is distributed

Why distributed?
  to organize physically separate entities
  to achieve security via isolation
  to tolerate faults via replication
  to scale up throughput via parallel CPUs/mem/disk/net

But:
  complex: many concurrent parts
  must cope with partial failure
  tricky to realize performance potential

Why take this course?
  interesting -- hard problems, powerful solutions
  used by real systems -- driven by the rise of big Web sites
  active research area -- lots of progress + big unsolved problems
  hands-on -- you'll build serious systems in the labs

COURSE STRUCTURE

http://pdos.csail.mit.edu/6.824

Course staff:
  Malte Schwarzkopf, lecturer
  Robert Morris, lecturer
  Deepti Raghavan, TA
  Edward Park, TA
  Erik Nguyen, TA
  Anish Athalye, TA

Course components:
  lectures
  readings
  two exams
  labs
  final project (optional)
  TA office hours
  piazza for announcements and lab help

Lectures:
  big ideas, paper discussion, and labs

Readings:
  research papers, some classic, some new
  the papers illustrate key ideas and important details
  many lectures focus on the papers
  please read papers before class!
  each paper has a short question for you to answer
  and you must send us a question you have about the paper
  submit question&answer by midnight the night before

Exams:
  Mid-term exam in class
  Final exam during finals week

Lab goals:
  deeper understanding of some important techniques
  experience with distributed programming
  first lab is due a week from Friday
  one per week after that for a while

Lab 1: MapReduce
Lab 2: replication for fault-tolerance using Raft
Lab 3: fault-tolerant key/value store
Lab 4: sharded key/value store

Optional final project at the end, in groups of 2 or 3.
  The final project substitutes for Lab 4.
  You think of a project and clear it with us.
  Code, short write-up, short demo on last day.

Lab grades depend on how many test cases you pass
  we give you the tests, so you know whether you'll do well
  careful: if it often passes, but sometimes fails,
    chances are it will fail when we run it

Debugging the labs can be time-consuming
  start early
  come to TA office hours
  ask questions on Piazza

MAIN TOPICS

This is a course about infrastructure, to be used by applications.
  About abstractions that hide distribution from applications.
  Three big kinds of abstraction:
    Storage.
    Communication.
    Computation.
  [diagram: users, application servers, storage servers]

A couple of topics come up repeatedly.

Topic: implementation
  RPC, threads, concurrency control.

Topic: performance
  The dream: scalable throughput.
    Nx servers -> Nx total throughput via parallel CPU, disk, net.
    So handling more load only requires buying more computers.
  Scaling gets harder as N grows:
    Load im-balance, stragglers.
    Non-parallelizable code: initialization, interaction.
    Bottlenecks from shared resources, e.g. network.
  Note that some performance problems aren't easily attacked by scaling
    e.g. decreasing response time for a single user request
    might require programmer effort rather than just more computers

Topic: fault tolerance
  1000s of servers, complex net -> always something broken
  We'd like to hide these failures from the application.
  We often want:
    Availability -- app can make progress despite failures
    Durability -- app will come back to life when failures are repaired
  Big idea: replicated servers.
    If one server crashes, client can proceed using the other(s).

Topic: consistency
  General-purpose infrastructure needs well-defined behavior.
    E.g. "Get(k) yields the value from the most recent Put(k,v)."
  Achieving good behavior is hard!
    "Replica" servers are hard to keep identical.
    Clients may crash midway through multi-step update.
    Servers crash at awkward moments, e.g. after executing but before replying.
    Network may make live servers look dead; risk of "split brain".
  Consistency and performance are enemies.
    Consistency requires communication, e.g. to get latest Put().
    "Strong consistency" often leads to slow systems.
    High performance often imposes "weak consistency" on applications.
  People have pursued many design points in this spectrum.

CASE STUDY: MapReduce

Let's talk about MapReduce (MR) as a case study
  MR is a good illustration of 6.824's main topics
  and is the focus of Lab 1

MapReduce overview
  context: multi-hour computations on multi-terabyte data-sets
    e.g. analysis of graph structure of crawled web pages
    only practical with 1000s of computers
    often not developed by distributed systems experts
    distribution can be very painful, e.g. coping with failure
  overall goal: non-specialist programmers can easily split
    data processing over many servers with reasonable efficiency.
  programmer defines Map and Reduce functions
    sequential code; often fairly simple
  MR runs the functions on 1000s of machines with huge inputs
    and hides details of distribution

Abstract view of MapReduce
  input is divided into M files
  [diagram: maps generate rows of K-V pairs, reduces consume columns]
  Input1 -> Map -> a,1 b,1 c,1
  Input2 -> Map ->     b,1
  Input3 -> Map -> a,1     c,1
                    |   |   |
                    |   |   -> Reduce -> c,2
                    |   -----> Reduce -> b,2
                    ---------> Reduce -> a,2
  MR calls Map() for each input file, produces set of k2,v2
    "intermediate" data
    each Map() call is a "task"
  MR gathers all intermediate v2's for a given k2,
    and passes them to a Reduce call
  final output is set of <k2,v3> pairs from Reduce()
    stored in R output files
  [diagram: MapReduce API --
   map(k1, v1) -> list(k2, v2)
   reduce(k2, list(v2) -> list(k2, v3)]

Example: word count
  input is thousands of text files
  Map(k, v)
    split v into words
    for each word w
      emit(w, "1")
  Reduce(k, v)
    emit(len(v))

MapReduce hides many painful details:
  starting s/w on servers
  tracking which tasks are done
  data movement
  recovering from failures

MapReduce scales well:
  N computers gets you Nx throughput.
    Assuming M and R are >= N (i.e., lots of input files and map output keys).
    Maps()s can run in parallel, since they don't interact.
    Same for Reduce()s.
    The only interaction is via the "shuffle" in between maps and reduces.
  So you can get more throughput by buying more computers.
    Rather than special-purpose efficient parallelizations of each application.
    Computers are cheaper than programmers!

What will likely limit the performance?
  We care since that's the thing to optimize.
  CPU? memory? disk? network?
  In 2004 authors were limited by "network cross-section bandwidth".
    [diagram: servers, tree of network switches]
    Note all data goes over network, during Map->Reduce shuffle.
    Paper's root switch: 100 to 200 gigabits/second
    1800 machines, so 55 megabits/second/machine.
    Small, e.g. much less than disk (~50-100 MB/s at the time) or RAM speed.
  So they cared about minimizing movement of data over the network.
    (Datacenter networks are much faster today.)

More details (paper's Figure 1):
  master: gives tasks to workers; remembers where intermediate output is
  M Map tasks, R Reduce tasks
  input stored in GFS, 3 copies of each Map input file
  all computers run both GFS and MR workers
  many more input tasks than workers
  master gives a Map task to each worker
    hands out new tasks as old ones finish
  Map worker hashes intermediate keys into R partitions, on local disk
  Q: What's a good data structure for implementing this?
  no Reduce calls until all Maps are finished
  master tells Reducers to fetch intermediate data partitions from Map workers
  Reduce workers write final output to GFS (one file per Reduce task)

How does detailed design reduce effect of slow network?
  Map input is read from GFS replica on local disk, not over network.
  Intermediate data goes over network just once.
    Map worker writes to local disk, not GFS.
  Intermediate data partitioned into files holding many keys.
    Q: Why not stream the records to the reducer (via TCP) as they are being
       produced by the mappers?

How do they get good load balance?
  Critical to scaling -- bad for N-1 servers to wait for 1 to finish.
  But some tasks likely take longer than others.
  [diagram: packing variable-length tasks into workers]
  Solution: many more tasks than workers.
    Master hands out new tasks to workers who finish previous tasks.
    So no task is so big it dominates completion time (hopefully).
    So faster servers do more work than slower ones, finish abt the same time.

What about fault tolerance?
  I.e. what if a server crashes during a MR job?
  Hiding failures is a huge part of ease of programming!
  Q: Why not re-start the whole job from the beginning?
  MR re-runs just the failed Map()s and Reduce()s.
    MR requires them to be pure functions:
      they don't keep state across calls,
      they don't read or write files other than expected MR inputs/outputs,
      there's no hidden communication among tasks.
    So re-execution yields the same output.
  The requirement for pure functions is a major limitation of
    MR compared to other parallel programming schemes.
    But it's critical to MR's simplicity.

Details of worker crash recovery:
  * Map worker crashes:
    master sees worker no longer responds to pings
    crashed worker's intermediate Map output is lost
      but is likely needed by every Reduce task!
    master re-runs, spreads tasks over other GFS replicas of input.
    some Reduce workers may already have read failed worker's intermediate data.
      here we depend on functional and deterministic Map()!
    master need not re-run Map if Reduces have fetched all intermediate data
      though then a Reduce crash would then force re-execution of failed Map
  * Reduce worker crashes.
    finshed tasks are OK -- stored in GFS, with replicas.
    master re-starts worker's unfinished tasks on other workers.
  * Reduce worker crashes in the middle of writing its output.
    GFS has atomic rename that prevents output from being visible until complete.
    so it's safe for the master to re-run the Reduce tasks somewhere else.

Other failures/problems:
  * What if the master gives two workers the same Map() task?
    perhaps the master incorrectly thinks one worker died.
    it will tell Reduce workers about only one of them.
  * What if the master gives two workers the same Reduce() task?
    they will both try to write the same output file on GFS!
    atomic GFS rename prevents mixing; one complete file will be visible.
  * What if a single worker is very slow -- a "straggler"?
    perhaps due to flakey hardware.
    master starts a second copy of last few tasks.
  * What if a worker computes incorrect output, due to broken h/w or s/w?
    too bad! MR assumes "fail-stop" CPUs and software.
  * What if the master crashes?
    recover from check-point, or give up on job

For what applications *doesn't* MapReduce work well?
  Not everything fits the map/shuffle/reduce pattern.
  Small data, since overheads are high. E.g. not web site back-end.
  Small updates to big data, e.g. add a few documents to a big index
  Unpredictable reads (neither Map nor Reduce can choose input)
  Multiple shuffles, e.g. page-rank (can use multiple MR but not very efficient)
  More flexible systems allow these, but more complex model.

How might a real-world web company use MapReduce?
  "CatBook", a new company running a social network for cats; needs to:
  1) build a search index, so people can find other peoples' cats
  2) analyze popularity of different cats, to decide advertising value
  3) detect dogs and remove their profiles
  Can use MapReduce for all these purposes!
  - run large batch jobs over all profiles every night
  1) build inverted index: map(profile text) -> (word, cat_id)
                           reduce(word, list(cat_id) -> list(word, list(cat_id))
  2) count profile visits: map(web logs) -> (cat_id, "1")
                           reduce(cat_id, list("1")) -> list(cat_id, count)
  3) filter profiles: map(profile image) -> img analysis -> (cat_id, "dog!")
                      reduce(cat_id, list("dog!")) -> list(cat_id)

Conclusion
  MapReduce single-handedly made big cluster computation popular.
  - Not the most efficient or flexible.
  + Scales well.
  + Easy to program -- failures and data movement are hidden.
  These were good trade-offs in practice.
  We'll see some more advanced successors later in the course.
  Have fun with the lab!
==========
package main

import (
	"fmt"
	"sync"
)

//
// Several solutions to the crawler exercise from the Go tutorial
// https://tour.golang.org/concurrency/10
//

//
// Serial crawler
//

func Serial(url string, fetcher Fetcher, fetched map[string]bool) {
	if fetched[url] {
		return
	}
	fetched[url] = true
	urls, err := fetcher.Fetch(url)
	if err != nil {
		return
	}
	for _, u := range urls {
		Serial(u, fetcher, fetched)
	}
	return
}

//
// Concurrent crawler with shared state and Mutex
//

type fetchState struct {
	mu      sync.Mutex
	fetched map[string]bool
}

func ConcurrentMutex(url string, fetcher Fetcher, f *fetchState) {
	f.mu.Lock()
	if f.fetched[url] {
		f.mu.Unlock()
		return
	}
	f.fetched[url] = true
	f.mu.Unlock()

	urls, err := fetcher.Fetch(url)
	if err != nil {
		return
	}
	var done sync.WaitGroup
	for _, u := range urls {
		done.Add(1)
		go func(u string) {
			defer done.Done()
			ConcurrentMutex(u, fetcher, f)
		}(u)
	}
	done.Wait()
	return
}

func makeState() *fetchState {
	f := &fetchState{}
	f.fetched = make(map[string]bool)
	return f
}

//
// Concurrent crawler with channels
//

func worker(url string, ch chan []string, fetcher Fetcher) {
	urls, err := fetcher.Fetch(url)
	if err != nil {
		ch <- []string{}
	} else {
		ch <- urls
	}
}

func master(ch chan []string, fetcher Fetcher) {
	n := 1
	fetched := make(map[string]bool)
	for urls := range ch {
		for _, u := range urls {
			if fetched[u] == false {
				fetched[u] = true
				n += 1
				go worker(u, ch, fetcher)
			}
		}
		n -= 1
		if n == 0 {
			break
		}
	}
}

func ConcurrentChannel(url string, fetcher Fetcher) {
	ch := make(chan []string)
	go func() {
		ch <- []string{url}
	}()
	master(ch, fetcher)
}

//
// main
//

func main() {
	fmt.Printf("=== Serial===\n")
	Serial("http://golang.org/", fetcher, make(map[string]bool))

	fmt.Printf("=== ConcurrentMutex ===\n")
	ConcurrentMutex("http://golang.org/", fetcher, makeState())

	fmt.Printf("=== ConcurrentChannel ===\n")
	ConcurrentChannel("http://golang.org/", fetcher)
}

//
// Fetcher
//

type Fetcher interface {
	// Fetch returns a slice of URLs found on the page.
	Fetch(url string) (urls []string, err error)
}

// fakeFetcher is Fetcher that returns canned results.
type fakeFetcher map[string]*fakeResult

type fakeResult struct {
	body string
	urls []string
}

func (f fakeFetcher) Fetch(url string) ([]string, error) {
	if res, ok := f[url]; ok {
		fmt.Printf("found:   %s\n", url)
		return res.urls, nil
	}
	fmt.Printf("missing: %s\n", url)
	return nil, fmt.Errorf("not found: %s", url)
}

// fetcher is a populated fakeFetcher.
var fetcher = fakeFetcher{
	"http://golang.org/": &fakeResult{
		"The Go Programming Language",
		[]string{
			"http://golang.org/pkg/",
			"http://golang.org/cmd/",
		},
	},
	"http://golang.org/pkg/": &fakeResult{
		"Packages",
		[]string{
			"http://golang.org/",
			"http://golang.org/cmd/",
			"http://golang.org/pkg/fmt/",
			"http://golang.org/pkg/os/",
		},
	},
	"http://golang.org/pkg/fmt/": &fakeResult{
		"Package fmt",
		[]string{
			"http://golang.org/",
			"http://golang.org/pkg/",
		},
	},
	"http://golang.org/pkg/os/": &fakeResult{
		"Package os",
		[]string{
			"http://golang.org/",
			"http://golang.org/pkg/",
		},
	},
}
==========
package main

import (
	"fmt"
	"log"
	"net"
	"net/rpc"
	"sync"
)

//
// RPC request/reply definitions
//

const (
	OK       = "OK"
	ErrNoKey = "ErrNoKey"
)

type Err string

type PutArgs struct {
	Key   string
	Value string
}

type PutReply struct {
	Err Err
}

type GetArgs struct {
	Key string
}

type GetReply struct {
	Err   Err
	Value string
}

//
// Client
//

func connect() *rpc.Client {
	client, err := rpc.Dial("tcp", ":1234")
	if err != nil {
		log.Fatal("dialing:", err)
	}
	return client
}

func get(key string) string {
	client := connect()
	args := GetArgs{"subject"}
	reply := GetReply{}
	err := client.Call("KV.Get", &args, &reply)
	if err != nil {
		log.Fatal("error:", err)
	}
	client.Close()
	return reply.Value
}

func put(key string, val string) {
	client := connect()
	args := PutArgs{"subject", "6.824"}
	reply := PutReply{}
	err := client.Call("KV.Put", &args, &reply)
	if err != nil {
		log.Fatal("error:", err)
	}
	client.Close()
}

//
// Server
//

type KV struct {
	mu   sync.Mutex
	data map[string]string
}

func server() {
	kv := new(KV)
	kv.data = map[string]string{}
	rpcs := rpc.NewServer()
	rpcs.Register(kv)
	l, e := net.Listen("tcp", ":1234")
	if e != nil {
		log.Fatal("listen error:", e)
	}
	go func() {
		for {
			conn, err := l.Accept()
			if err == nil {
				go rpcs.ServeConn(conn)
			} else {
				break
			}
		}
		l.Close()
	}()
}

func (kv *KV) Get(args *GetArgs, reply *GetReply) error {
	kv.mu.Lock()
	defer kv.mu.Unlock()

	val, ok := kv.data[args.Key]
	if ok {
		reply.Err = OK
		reply.Value = val
	} else {
		reply.Err = ErrNoKey
		reply.Value = ""
	}
	return nil
}

func (kv *KV) Put(args *PutArgs, reply *PutReply) error {
	kv.mu.Lock()
	defer kv.mu.Unlock()

	kv.data[args.Key] = args.Value
	reply.Err = OK
	return nil
}

//
// main
//

func main() {
	server()

	put("subject", "6.824")
	fmt.Printf("Put(subject, 6.824) done\n")
	fmt.Printf("get(subject) -> %s\n", get("subject"))
}
==========
6.824 2018 Lecture 2: Infrastructure: RPC and threads

Most commonly-asked question: Why Go?
  6.824 used C++ for many years
    C++ worked out well
    but students spent time tracking down pointer and alloc/free bugs
    and there's no very satisfactory C++ RPC package
  Go is a bit better than C++ for us
    good support for concurrency (goroutines, channels, &c)
    good support for RPC
    garbage-collected (no use after freeing problems)
    type safe
    threads + GC is particularly attractive!
  We like programming in Go
    relatively simple and traditional
  After the tutorial, use https://golang.org/doc/effective_go.html
  Russ Cox will give a guest lecture March 8th

Threads
  threads are a useful structuring tool
  Go calls them goroutines; everyone else calls them threads
  they can be tricky

Why threads?
  They express concurrency, which shows up naturally in distributed systems
  I/O concurrency:
    While waiting for a response from another server, process next request
  Multicore:
    Threads run in parallel on several cores

Thread = "thread of execution"
  threads allow one program to (logically) execute many things at once
  the threads share memory
  each thread includes some per-thread state:
    program counter, registers, stack

How many threads in a program?
  Sometimes driven by structure
    e.g. one thread per client, one for background tasks
  Sometimes driven by desire for multi-core parallelism
    so one active thread per core
    the Go runtime automatically schedules runnable goroutines on available cores
  Sometimes driven by desire for I/O concurrency
    the number is determined by latency and capacity
    keep increasing until throughput stops growing
  Go threads are pretty cheap
    100s or 1000s are fine, but maybe not millions
    Creating a thread is more expensive than a method call
    
Threading challenges:
  sharing data 
    one thread reads data that another thread is changing?
    e.g. two threads do count = count + 1
    this is a "race" -- and is usually a bug
    -> use Mutexes (or other synchronization)
    -> or avoid sharing
  coordination between threads
    how to wait for all Map threads to finish?
    -> use Go channels or WaitGroup
  granularity of concurrency
    coarse-grained -> simple, but little concurrency/parallelism
    fine-grained -> more concurrency, more races and deadlocks

What is a crawler?
  goal is to fetch all web pages, e.g. to feed to an indexer
  web pages form a graph
  multiple links to each page
  graph has cycles

Crawler challenges
  Arrange for I/O concurrency
    Fetch many URLs at the same time
    To increase URLs fetched per second
    Since network latency is much more of a limit than network capacity
  Fetch each URL only *once*
    avoid wasting network bandwidth
    be nice to remote servers
    => Need to remember which URLs visited 
  Know when finished
    
Crawler solutions [crawler.go link on schedule page]

Serial crawler:
  the "fetched" map avoids repeats, breaks cycles
  it's a single map, passed by reference to recursive calls
  but: fetches only one page at a time

ConcurrentMutex crawler:
  Creates a thread for each page fetch
    Many concurrent fetches, higher fetch rate
  The threads share the fetched map
  Why the Mutex (== lock)?
    Without the lock:
      Two web pages contain links to the same URL
      Two threads simultaneouly fetch those two pages
      T1 checks fetched[url], T2 checks fetched[url]
      Both see that url hasn't been fetched
      Both fetch, which is wrong
    Simultaneous read and write (or write+write) is a "race"
      And often indicates a bug
      The bug may show up only for unlucky thread interleavings
    What will happen if I comment out the Lock()/Unlock() calls?
      go run crawler.go
      go run -race crawler.go
    The lock causes the check and update to be atomic
  How does it decide it is done?
    sync.WaitGroup
    implicitly waits for children to finish recursive fetches

ConcurrentChannel crawler
  a Go channel:
    a channel is an object; there can be many of them
      ch := make(chan int)
    a channel lets one thread send an object to another thread
    ch <- x
      the sender waits until some goroutine receives
    y := <- ch
      for y := range ch
      a receiver waits until some goroutine sends
    so you can use a channel to both communicate and synchronize
    several threads can send and receive on a channel
    remember: sender blocks until the receiver receives!
      may be dangerous to hold a lock while sending...
  ConcurrentChannel master()
    master() creates a worker goroutine to fetch each page
    worker() sends URLs on a channel
      multiple workers send on the single channel
    master() reads URLs from the channel
    [diagram: master, channel, workers]
  No need to lock the fetched map, because it isn't shared!
  Is there any shared data?
    The channel
    The slices and strings sent on the channel
    The arguments master() passes to worker()

When to use sharing and locks, versus channels?
  Most problems can be solved in either style
  What makes the most sense depends on how the programmer thinks
    state -- sharing and locks
    communication -- channels
    waiting for events -- channels
  Use Go's race detector:
    https://golang.org/doc/articles/race_detector.html
    go test -race 

Remote Procedure Call (RPC)
  a key piece of distributed system machinery; all the labs use RPC
  goal: easy-to-program client/server communication

RPC message diagram:
  Client             Server
    request--->
       <---response

RPC tries to mimic local fn call:
  Client:
    z = fn(x, y)
  Server:
    fn(x, y) {
      compute
      return z
    }
  Rarely this simple in practice...

Software structure
  client app         handlers
    stubs           dispatcher
   RPC lib           RPC lib
     net  ------------ net

Go example: kv.go link on schedule page
  A toy key/value storage server -- Put(key,value), Get(key)->value
  Uses Go's RPC library
  Common:
    You have to declare Args and Reply struct for each RPC type
  Client:
    connect()'s Dial() creates a TCP connection to the server
    Call() asks the RPC library to perform the call
      you specify server function name, arguments, place to put reply
      library marshalls args, sends request, waits, unmarshally reply
      return value from Call() indicates whether it got a reply
      usually you'll also have a reply.Err indicating service-level failure
  Server:
    Go requires you to declare an object with methods as RPC handlers
    You then register that object with the RPC library
    You accept TCP connections, give them to RPC library
    The RPC library
      reads each request
      creates a new goroutine for this request
      unmarshalls request
      calls the named method (dispatch)
      marshalls reply
      writes reply on TCP connection
    The server's Get() and Put() handlers
      Must lock, since RPC library creates per-request goroutines
      read args; modify reply
 
A few details:
  Binding: how does client know who to talk to?
    For Go's RPC, server name/port is an argument to Dial
    Big systems have some kind of name or configuration server
  Marshalling: format data into packets
    Go's RPC library can pass strings, arrays, objects, maps, &c
    Go passes pointers by copying (server can't directly use client pointer)
    Cannot pass channels or functions

RPC problem: what to do about failures?
  e.g. lost packet, broken network, slow server, crashed server

What does a failure look like to the client RPC library?
  Client never sees a response from the server
  Client does *not* know if the server saw the request!
    Maybe server never saw the request
    Maybe server executed, crashed just before sending reply
    Maybe server executed, but network died just before delivering reply
  [diagram of lost reply]

Simplest failure-handling scheme: "best effort"
  Call() waits for response for a while
  If none arrives, re-send the request
  Do this a few times
  Then give up and return an error

Q: is "best effort" easy for applications to cope with?

A particularly bad situation:
  client executes
    Put("k", 10);
    Put("k", 20);
  both succeed
  what will Get("k") yield?
  [diagram, timeout, re-send, original arrives late]

Q: is best effort ever OK?
   read-only operations
   operations that do nothing if repeated
     e.g. DB checks if record has already been inserted

Better RPC behavior: "at most once"
  idea: server RPC code detects duplicate requests
    returns previous reply instead of re-running handler
  Q: how to detect a duplicate request?
  client includes unique ID (XID) with each request
    uses same XID for re-send
  server:
    if seen[xid]:
      r = old[xid]
    else
      r = handler()
      old[xid] = r
      seen[xid] = true

some at-most-once complexities
  this will come up in lab 3
  how to ensure XID is unique?
    big random number?
    combine unique client ID (ip address?) with sequence #?
  server must eventually discard info about old RPCs
    when is discard safe?
    idea:
      each client has a unique ID (perhaps a big random number)
      per-client RPC sequence numbers
      client includes "seen all replies <= X" with every RPC
      much like TCP sequence #s and acks
    or only allow client one outstanding RPC at a time
      arrival of seq+1 allows server to discard all <= seq
  how to handle dup req while original is still executing?
    server doesn't know reply yet
    idea: "pending" flag per executing RPC; wait or ignore

What if an at-most-once server crashes and re-starts?
  if at-most-once duplicate info in memory, server will forget
    and accept duplicate requests after re-start
  maybe it should write the duplicate info to disk
  maybe replica server should also replicate duplicate info

Go RPC is a simple form of "at-most-once"
  open TCP connection
  write request to TCP connection
  Go RPC never re-sends a request
    So server won't see duplicate requests
  Go RPC code returns an error if it doesn't get a reply
    perhaps after a timeout (from TCP)
    perhaps server didn't see request
    perhaps server processed request but server/net failed before reply came back

What about "exactly once"?
  unbounded retries plus duplicate detection plus fault-tolerant service
  Lab 3
==========
6.824 2018 Lecture 3: GFS

The Google File System
Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung
SOSP 2003

Why are we reading this paper?
  the file system used for map/reduce
  main themes of 6.824 show up in this paper
    trading consistency for simplicity and performance
    motivation for subsequent designs
  good systems paper -- details from apps all the way to network
    performance, fault-tolerance, consistency
  influential
    many other systems use GFS (e.g., Bigtable, Spanner @ Google)
    HDFS (Hadoop Distributed File System) based on GFS


What is consistency?
  A correctness condition
  Important but difficult to achieve when data is replicated
    especially when application access it concurrently
    [diagram: simple example, single machine]
    if an application writes, what will a later read observe?
      what if the read is from a different application?
    but with replication, each write must also happen on other machines
    [diagram: two more machines, reads and writes go across]
    Clearly we have a problem here.
  Weak consistency
    read() may return stale data  --- not the result of the most recent write
  Strong consistency
    read() always returns the data from the most recent write()
  General tension between these:
    strong consistency is easy for application writers
    strong consistency is bad for performance
    weak consistency has good performance and is easy to scale to many servers
    weak consistency is complex to reason about
  Many trade-offs give rise to different correctness conditions
    These are called "consistency models"
    First peek today; will show up in almost every paper we read this term

"Ideal" consistency model
  Let's go back to the single-machine case
  Would be nice if a replicated FS behaved like a non-replicated file system
    [diagram: many clients on the same machine accessing files on single disk]
  If one application writes, later reads will observe that write
  What if two application concurrently write to the same file?
    Q: what happens on a single machine?
    In file systems often undefined  --- file may have some mixed content
  What if two application concurrently write to the same directory
    Q: what happens on a single machine?
    One goes first, the other goes second (use locking)

Challenges to achieving ideal consistency
  Concurrency -- as we just saw; plus there are many disks in reality
  Machine failures -- any operation can fail to complete
  Network partitions -- may not be able to reach every machine/disk
  Why are these challenges difficult to overcome?
    Requires communication between clients and servers
      May cost performance
    Protocols can become complex --- see next week
      Difficult to implement system correctly
    Many systems in 6.824 don't provide ideal
      GFS is one example

GFS goals:
  With so many machines, failures are common
    must tolerate
    assume a machine fails once per year
    w/ 1000 machines, ~3 will fail per day.
  High-performance: many concurrent readers and writers
    Map/Reduce jobs read and store final result in GFS
    Note: *not* the temporary, intermediate files
  Use network efficiently: save bandwidth
  These challenges difficult combine with "ideal" consistency

High-level design / Reads
  [Figure 1 diagram, master + chunkservers]
  Master stores directories, files, names, open/read/write
    But not POSIX
  100s of Linux chunk servers with disks
    store 64MB chunks (an ordinary Linux file for each chunk)
    each chunk replicated on three servers
    Q: Besides availability of data, what does 3x replication give us?
       load balancing for reads to hot files
       affinity
    Q: why not just store one copy of each file on a RAID'd disk?
       RAID isn't commodity
       Want fault-tolerance for whole machine; not just storage device
    Q: why are the chunks so big?
       amortizes overheads, reduces state size in the master
  GFS master server knows directory hierarchy
    for directory, wht files are in it
    for file, knows chunk servers for each 64 MB
    master keeps state in memory
      64 bytes of metadata per each chunk
    master has private recoverable database for metadata
      operation log flushed to disk
      occasional asynchronous compression info checkpoint
      N.B.: != the application checkpointing in ยง2.7.2
      master can recovery quickly from power failure
    shadow masters that lag a little behind master
      can be promoted to master
  Client read:
    send file name and chunk index to master
    master replies with set of servers that have that chunk
      response includes version # of chunk
      clients cache that information
    ask nearest chunk server
      checks version #
      if version # is wrong, re-contact master

Writes
  [Figure 2-style diagram with file offset sequence]
  Random client write to existing file
    client asks master for chunk locations + primary
    master responds with chunk servers, version #, and who is primary
      primary has (or gets) 60s lease
    client computes chain of replicas based on network topology
    client sends data to first replica, which forwards to others
      pipelines network use, distributes load
    replicas ack data receipt
    client tells primary to write
      primary assign sequence number and writes
      then tells other replicas to write
      once all done, ack to client
    what if there's another concurrent client writing to the same place?
      client 2 get sequenced after client 1, overwrites data
      now client 2 writes again, this time gets sequenced first (C1 may be slow)
      writes, but then client 1 comes and overwrites
      => all replicas have same data (= consistent), but mix parts from C1/C2
         (= NOT defined)
  Client append (not record append)
    same deal, but may put parts from C1 and C2 in any order
    consistent, but not defined
    or, if just one client writes, no problem -- both consistent and defined

Record append
  Client record append
    client asks master for chunk locations
    client pushes data to replicas, but specifies no offset
    client contacts primary when data is on all chunk servers
      primary assigns sequence number
      primary checks if append fits into chunk
        if not, pad until chunk boundary
      primary picks offset for append
      primary applies change locally
      primary forwards request to replicas
      let's saw R3 fails mid-way through applying the write
      primary detects error, tells client to try again
    client retries after contacting master
      master has perhaps brought up R4 in the meantime (or R3 came back)
      one replica now has a gap in the byte sequence, so can't just append
      pad to next available offset across all replicas
      primary and secondaries apply writes
      primary responds to client after receiving acks from all replicas

Housekeeping
  Master can appoint new primary if master doesn't refresh lease
  Master replicates chunks if number replicas drop below some number
  Master rebalances replicas

Failures
  Chunk servers are easy to replace
    failure may cause some clients to retry (& duplicate records)
  Master: down -> GFS is unavailable
    shadow master can serve read-only operations, which may return stale data
    Q: Why not write operations?
	  split-brain syndrome (see next lecture)

Does GFS achieve "ideal" consistency?
  Two cases: directories and files
  Directories: yes, but...
    Yes: strong consistency (only one copy)
    But: master not always available & scalability limit
  Files: not always
    Mutations with atomic appends
	  record can be duplicated at two offsets
    while other replicas may have a hole at one offset
    Mutations without atomic append
      data of several clients maybe intermingled
      if you care, use atomic append or a temporary file and atomically rename
  An "unlucky" client can read stale data for short period of time
    A failed mutation leaves chunks inconsistent
      The primary chunk server updated chunk
      But then failed and the replicas are out of date
    A client may read an not-up-to-date chunk
    When client refreshes lease it will learn about new version #

Authors claims weak consistency is not a big problems for apps
  Most file updates are append-only updates
    Application can use UID in append records to detect duplicates
    Application may just read less data (but not stale data)
  Application can use temporary files and atomic rename

Performance (Figure 3)
  huge aggregate throughput for read (3 copies, striping)
    125 MB/sec in aggregate
    Close to saturating network
  writes to different files lower than possible maximum
    authors blame their network stack
    it causes delays in propagating chunks from one replica to next
  concurrent appends to single file
    limited by the server that stores last chunk
  numbers and specifics have changed a lot in 15 years!

Summary
  case study of performance, fault-tolerance, consistency
    specialized for MapReduce applications
  what works well in GFS?
    huge sequential reads and writes
    appends
    huge throughput (3 copies, striping)
    fault tolerance of data (3 copies)
  what less well in GFS?
    fault-tolerance of master
    small files (master a bottleneck)
    clients may see stale data
    appends maybe duplicated

References
  http://queue.acm.org/detail.cfm?id=1594206  (discussion of gfs evolution)
  http://highscalability.com/blog/2010/9/11/googles-colossus-makes-search-real-time-by-dumping-mapreduce.html
==========
6.824 2018 Lecture 4: Primary/Backup Replication

Today
  Primary/Backup Replication for Fault Tolerance
  Case study of VMware FT, an extreme version of the idea

Fault tolerance
  we'd like a service that continues despite failures
  some ideal properties:
    available: still useable despite [some class of] failures
    strongly consistent: looks just like a single server to clients
    transparent to clients
    transparent to server software
    efficient

What failures will we try to cope with?
  Fail-stop failures
  Independent failures
  Network drops some/all packets
  Network partition

But not:
  Incorrect execution
  Correlated failures
  Configuration errors
  Malice

Behaviors
  Available (e.g. if one server halts)
  Wait (e.g. if network totally fails)
  Stop forever (e.g. if multiple servers crash)
  Malfunction (e.g. if h/w computes incorrectly, or software has a bug)

Core idea: replication
  *Two* servers (or more)
  Each replica keeps state needed for the service
  If one replica fails, others can continue

Example: fault-tolerant MapReduce master
  lab 1 workers are already fault-tolerant, but not master
    master is a "single point of failure"
  can we have two masters, in case one fails?
  [diagram: M1, M2, workers]
  state:
    worker list
    which jobs done
    which workers idle
    TCP connection state
    program memory and stack
    CPU registers

Big Questions:
  What state to replicate?
  Does primary have to wait for backup?
  When to cut over to backup?
  Are anomalies visible at cut-over?
  How to bring a replacement up to speed?

Two main approaches:
  State transfer
    "Primary" replica executes the service
    Primary sends [new] state to backups
  Replicated state machine
    All replicas execute all operations
    If same start state,
      same operations,
      same order,
      deterministic,
      then same end state

State transfer is simpler
  But state may be large, slow to transfer
  VM-FT uses replicated state machine

Replicated state machine can be more efficient
  If operations are small compared to data
  But complex to get right
  Labs 2/3/4 use replicated state machines

At what level to define a replicated state machine?
  K/V put and get?
    "application-level" RSM
    usually requires server and client modifications
    can be efficient; primary only sends high-level operations to backup
  x86 instructions?
    might allow us to replicate any existing server w/o modification!
    but requires much more detailed primary/backup synchronization
    and we have to deal with interrupts, DMA, weird x86 instructions

The design of a Practical System for Fault-Tolerant Virtual Machines
Scales, Nelson, and Venkitachalam, SIGOPS OSR Vol 44, No 4, Dec 2010

Very ambitious system:
  Goal: fault-tolerance for existing server software
  Goal: clients should not notice a failure
  Goal: no changes required to client or server software
  Very ambitious!

Overview
  [diagram: app, O/S, VM-FT underneath, shared disk, network, clients]
  words:
    hypervisor == monitor == VMM (virtual machine monitor)
    app and O/S are "guest" running inside a virtual machine
  two machines, primary and backup
  shared disk for persistent storage
    shared so that bringing up a new backup is faster
  primary sends all inputs to backup over logging channel

Why does this idea work?
  It's a replicated state machine
  Primary and backup boot with same initial state (memory, disk files)
  Same instructions, same inputs -> same execution
    All else being equal, primary and backup will remain identical

What sources of divergence must we guard against?
  Many instructions are guaranteed to execute exactly the same on primary and backup.
    As long as memory+registers are identical, which we're assuming by induction.
  When might execution on primary differ from backup?
  Inputs from external world (the network).
  Data read from storage server.
  Timing of interrupts.
  Instructions that aren't pure functions of state, such as cycle counter.
  Races.

Examples of divergence?
  They all sound like "if primary fails, clients will see inconsistent story from backup."
  Lock server grants lock to client C1, rejects later request from C2.
    Primary and backup had better agree on input order!
    Otherwise, primary fails, backup now tells clients that C2 holds the lock.
  Lock server revokes lock after one minute.
    Suppose C1 holds the lock, and the minute is almost exactly up.
    C2 requests the lock.
    Primary might see C2's request just before timer interrupt, reject.
    Backup might see C2's request just after timer interrupt, grant.
  So: backup must see same events, in same order, at same point in instruction stream.

Example: timer interrupts
  Goal: primary and backup should see interrupt at exactly the same point in execution
    i.e. between the same pair of executed instructions
  Primary:
    FT fields the timer interrupt
    FT reads instruction number from CPU
    FT sends "timer interrupt at instruction X" on logging channel
    FT delivers interrupt to primary, and resumes it
    (this relies on special support from CPU to count instructions, interrupt after X)
  Backup:
    ignores its own timer hardware
    FT sees log entry *before* backup gets to instruction X
    FT tells CPU to interrupt at instruction X
    FT mimics a timer interrupt, resumes backup

Example: disk/network input
  Primary and backup *both* ask h/w to read
    FT intercepts, ignores on backup, gives to real h/w on primary
  Primary:
    FT tells the h/w to DMA data into FT's private "bounce buffer"
    At some point h/w does DMA, then interrupts
    FT gets the interrupt
    FT pauses the primary
    FT copies the bounce buffer into the primary's memory
    FT simulates an interrupt to primary, resumes it
    FT sends the data and the instruction # to the backup
  Backup:
    FT gets data and instruction # from log stream
    FT tells CPU to interrupt at instruction X
    FT copies the data during interrupt

Why the bounce buffer?
  I.e. why wait until primary/backup aren't executing before copying the data?
  We want the data to appear in memory at exactly the same point in
    execution of the primary and backup.
  Otherwise they may diverge.

Note that the backup must lag by one event (one log entry)
  Suppose primary gets an interrupt, or input, after instruction X
  If backup has already executed past X, it cannot handle the input correctly
  So backup FT can't start executing at all until it sees the first log entry
    Then it executes just to the instruction # in that log entry
    And waits for the next log entry before restarting backup

Example: non-functional instructions
  even if primary and backup have same memory/registers,
    some instructions still execute differently
  e.g. reading the current time or cycle count or processor serial #
  Primary:
    FT sets up the CPU to interrupt if primary executes such an instruction
    FT executes the instruction and records the result
    sends result and instruction # to backup
  Backup:
    backup also interrupts when it tries to execute that instruction
    FT supplies value that the primary got

What about disk/network output?
  Primary and backup both execute instructions for output
  Primary's FT actually does the output
  Backup's FT discards the output

But: the paper's Output Rule (Section 2.2) says primary primary must
tell backup when it produces output, and delay the output until the
backup says it has received the log entry.

Why the Output Rule?
  Suppose there was no Output Rule.
  The primary emits output immediately.
  Suppose the primary has seen inputs I1 I2 I3, then emits output.
  The backup has received I1 and I2 on the log.
  The primary crashes and the packet for I3 is lost by the network.
  Now the backup will go live without having processed I3.
    But some client has seen output reflecting the primary having executed I3.
    So that client may see anomalous state if it talks to the service again.
  So: the primary doesn't emit output until it knows that the backup
    has seen all inputs up to that output.

The Output Rule is a big deal
  Occurs in some form in all replication systems
  A serious constraint on performance
  An area for application-specific cleverness
    Eg. maybe no need for primary to wait before replying to read-only operation
  FT has no application-level knowledge, must be conservative

Q: What if the primary crashes just after getting ACK from backup,
   but before the primary emits the output?
   Does this mean that the output won't ever be generated?

A: Here's what happens when the primary fails and the backup takes over.
   The backup got some log entries from the primary.
   The backup continues executing those log entries WITH OUTPUT SUPPRESSED.
   After the last log entry, the backup starts emitting output
   In our example, the last log entry is I3
   So after input I3, the client will start emitting outputs
   And thus it will emit the output that the primary failed to emit

Q: But what if the primary crashed *after* emitting the output?
   Will the backup emit the output a *second* time?

A: Yes.
   OK for TCP, since receivers ignore duplicate sequence numbers.
   OK for writes to shared disk, since backup will write same data to same block #.

Duplicate output at cut-over is pretty common in replication systems
  Not always possible for clients &c to ignore duplicates
  For example, if output is vending money from an ATM machine

Q: Does FT cope with network partition -- could it suffer from split brain?
   E.g. if primary and backup both think the other is down.
   Will they both "go live"?

A: The shared disk breaks the tie.
   Shared disk server supports atomic test-and-set.
   Only one of primary/backup can successfully test-and-set.
   If only one is alive, it will win test-and-set and go live.
   If both try, one will lose, and halt.

Shared storage is single point of failure
  If shared storage is down, service is down
  Maybe they have in mind a replicated storage system

Q: Why don't they support multi-core?

Performance (table 1)
  FT/Non-FT: impressive!
    little slow down
  Logging bandwidth
    Directly reflects disk read rate + network input rate
    18 Mbit/s for my-sql
  These numbers seem low to me
    Applications can read a disk at at least 400 megabits/second
    So their applications aren't very disk-intensive

When might FT be attractive?
  Critical but low-intensity services, e.g. name server.
  Services whose software is not convenient to modify.

What about replication for high-throughput services?
  People use application-level replicated state machines for e.g. databases.
    The state is just the DB, not all of memory+disk.
    The events are DB commands (put or get), not packets and interrupts.
  Result: less fine-grained synchronization, less overhead.
  GFS use application-level replication, as do Lab 2 &c

Summary:
  Primary-backup replication
    VM-FT: clean example
  How to cope with partition without single point of failure?
    Next lecture
  How to get better performance?
    Application-level replicated state machines
  
----

VMware KB (#1013428) talks about multi-CPU support.  VM-FT may have switched
from a replicated state machine approach to the state transfer approach, but
unclear whether that is true or not.

http://www.wooditwork.com/2014/08/26/whats-new-vsphere-6-0-fault-tolerance/
http://www.tomsitpro.com/articles/vmware-vsphere-6-fault-tolerance-multi-cpu,1-2439.html

https://labs.vmware.com/academic/publications/retrace 
==========
6.824 2017 Lecture 5: Raft (1)

why are we reading this paper?
  distributed consensus is a hard problem that people have worked on for decades
  Lab 2 and 3 are based on Raft

this lecture
  today: Raft elections and log handling (Lab 2A, 2B)
  next: Raft persistence, client behavior, snapshots (Lab 2C, Lab 3)

overall topic: fault-tolerant services using replicated state machines (RSM)
  [clients, replica servers]
  example: configuration server, like MapReduce or GFS master
  example: key/value storage server, put()/get() (lab3)
  goal: same client-visible behavior as single non-replicated server
    but available despite some number of failed servers
  strategy:
    each replica server executes same commands in same order
    so they remain replicas (i.e., identical) as they execute
    so if one fails, others can continue
    i.e. on failure, client switches to another server
  both GFS and VMware FT have this flavor

a critical question: how to avoid split brain?
  suppose client can contact replica A, but not replica B
  can client proceed with just replica A?
  if B has really crashed, client *must* proceed without B,
    otherwise the service can't tolerate faults!
  if B is up but network prevents client from contacting it,
    maybe client should *not* proceed without it,
    since it might be alive and serving other clients -- risking split brain

example of why split brain cannot be allowed:
  fault-tolerant key/value database
  C1 and C2 are in different network partitions and talk to different servers
  C1: put("k1", "v1")
  C2: put("k1", "v2")
  C1: get("k1") -> ???
  correct answer is "v2", since that's what a non-replicated server would yield
  but if two servers are independently serving C1 and C2 due to partition,
    C1's get could yield "v1"

problem: computers cannot distinguish crashed machines vs a partitioned network
  both manifest as being unable to communicate with one or more machines

We want a state-machine replication scheme that meets three goals:
  1. remains available despite any one (fail-stop) failure
  2. handles partition w/o split brain
  3. if too many failures: waits for repair, then resumes

The big insight for coping w/ partition: majority vote
  2f+1 servers to tolerate f failures, e.g. 3 servers can tolerate 1 failure
  must get majority (f+1) of servers ti agree to make progress
    failure of f servers leaves a majority of f+1, which can proceed
  why does majority help avoid split brain?
    at most one partition can have a majority
  note: majority is out of all 2f+1 servers, not just out of live ones
  the really useful thing about majorities is that any two must intersect
    servers in the intersection will only vote one way or the other
    and the intersection can convey information about previous decisions

Two partition-tolerant replication schemes were invented around 1990,
  Paxos and View-Stamped Replication
  in the last 10 years this technology has seen a lot of real-world use
  the Raft paper is a good introduction to modern techniques

*** topic: Raft overview

state machine replication with Raft -- Lab 3 as example:
  [diagram: clients, 3 replicas, k/v layer, raft layer, logs]
  server's Raft layers elect a leader
  clients send RPCs to k/v layer in leader
    Put, Get, Append
  k/v layer forwards request to Raft layer, doesn't respond to client yet
  leader's Raft layer sends each client command to all replicas
    via AppendEntries RPCs
    each follower appends to its local log (but doesn't commit yet)
    and responds to the leader to acknowledge
  entry becomes "committed" at the leader if a majority put it in their logs
    guaranteed it won't be forgotten
    majority -> will be seen by the next leader's vote requests for sure
  servers apply operation to k/v state machine once leader says it's committed
    they find out about this via the next AppendEntries RPC (via commitIndex)
  leader responds to k/v layer after it has committed
    k/v layer applies Put to DB, or fetches Get result
  then leader replies to client w/ execution result

why the logs?
  the service keeps the state machine state, e.g. key/value DB
    why isn't that enough?
  it's important to number the commands
    to help replicas agree on a single execution order
    to help the leader ensure followers have identical logs
  replicas also use the log to store commands
    until the leader commits them
    so the leader can re-send if a follower misses some
    for persistence and replay after a reboot (next time)

there are two main parts to Raft's design:
  electing a new leader
  ensuring identical logs despite failures

*** topic: leader election (Lab 2A)

why a leader?
  ensures all replicas execute the same commands, in the same order

Raft numbers the sequence of leaders using "terms"
  new leader -> new term
  a term has at most one leader; might have no leader
  each election is also associated with one particular term
    and there can only be one successful election per term
  the term numbering helps servers follow latest leader, not superseded leader

when does Raft start a leader election?
  AppendEntries are implied heartbeats; plus leader sends them periodically
  if other server(s) don't hear from current leader for an "election timeout"
    they assume the leader is down and start an election
  [state transition diagram, Figure 4: follower, candidate, leader]
  followers increment local currentTerm, become candidates, start election
  note: this can lead to un-needed elections; that's slow but safe
  note: old leader may still be alive and think it is the leader

what happens when a server becomes candidate?
  three possibilities:
    1) gets majority, converts to leader
      locally observes and counts votes
      note: not resilient to byzantine faults!
    2) fails to get a majority, hears from another leader who did
      via incoming AppendEntries RPC
      defers to the new leader's authority, and becomes follower
    3) fails to get a majority, but doesn't hear from a new leader
      e.g., if in minority network partition
      times out and starts another election (remains candidate)
  note: in case (3), it's possible to keep incrementing the term
    but cannot add log entries, since in minority and not the leader
    once partition heals, an election ensues because of the higher term
    but: either logs in majority partition are longer (so high-term
      candidate gets rejected) or they are same length if nothing happened
      in the majority partition (so high-term candidate can win, but no damage)

how to ensure at most one leader in a term?
  (Figure 2 RequestVote RPC and Rules for Servers)
  leader must get "yes" votes from a majority of servers
  each server can cast only one vote per term
  at most one server can get majority of votes for a given term
    -> at most one leader even if network partition
    -> election can succeed even if some servers have failed

how does a new leader establish itself?
  winner gets yes votes from majority
  immediately sends AppendEntries RPC (heart-beats) to everybody
    the new leader's heart-beats suppress any new election

an election may not succeed for two reasons:
  * less than a majority of servers are reachable
  * simultaneous candidates split the vote, none gets majority

what happens if an election doesn't succeed?
  another timeout (no heartbeat), another election
  higher term takes precedence, candidates for older terms quit

how to set the election timeout?
  each server picks a random election timeout
    helps avoid split votes
  randomness breaks symmetry among the servers
    one will choose lowest random delay
    avoids everybody starting elections at the same time, voting for themselves
  hopefully enough time to elect before next timeout expires
  others will see new leader's AppendEntries heartbeats and
    not become candidates
  what value?
    at least a few heartbeat intervals (network can drop or delay a heartbeat)
    random part long enough to let one candidate succeed before next starts
    short enough to allow a few re-tries before tester gets upset
      tester requires election to complete in 5 seconds or less


*** topic: the Raft log (Lab 2B)

we talked about how the leader replicates log entries
  important distinction: replicated vs. committed entries
    committed entries are guaranteed to never go away
    replicated, but uncommitted entries may be overwritten!
  helps to think of an explicit "commit frontier" at each participant

will the servers' logs always be exact replicas of each other?
  no: some replicas may lag
  no: we'll see that they can temporarily have different entries
  the good news:
    they'll eventually converge
    the commit mechanism ensures servers only execute stable entries

extra criterion: leader cannot simply replicate and commit old term's entries
  [Figure 8 example]
  S1 fails to replicate term 2 entry to majority, then fails
  S5 becomes leader in term 3, adds entry, but fails to replicate it
  S1 comes back, becomes leader again
    works on replicating old entry from term 2 to force followers to adopt its log
    are we allowed to commit once the term 2 entry is on a majority of servers?
  Turns out the answer is no! Consider what would happen if we did:
    term 2 entry replicated to S3
    S1 commits it, since it's on a majority of servers
    S1 then fails again
    S5 gets elected for term 4, since it has a term 3 entry at the end of the log
    everybody with a term 2 entry at the end of the log votes for S5
  S5 becomes leader, and now forces *its* log (with the term 3 entry) on others
    term 2 entry at index 2 will get overwritten by term 3 entry
    but it's supposed to be committed!
    therefore, contradicts Leader Completeness property
  solution: wait until S1 has also replicated and committed a term 4 entry
    ensures that S5 can no longer be elected if S1 fails
    thus it's now okay to commit term 2 entry as well

when is it legal for Raft to overwrite log entries? (cf. Figure 7 question)
  they must be uncommitted
  may truncate and overwrite a much longer log
    Figure 7 (f) is a case in point
  e.g., a leader adds many entries to its log, but fails to replicate them
    perhaps it's in a network partition
  other leaders in later terms add entries at the same indices (Fig 7 (a)-(e))
    and commit at least some of them
    now cannot change this log index any more
  outdated server receives AppendEntries, overwrites uncommitted log entries
    even if the log is much longer than the current leader's!
  this is okay, because the leader only responds to clients after entries commit
    so leader who produced the overwritten entries in (f) cannot have done so


*** addendum: lab 2 Raft interface

  rf.Start(command) (index, term, isleader)
    Lab 3 k/v server's Put()/Get() RPC handlers call Start()
    start Raft agreement on a new log entry
    Start() returns immediately -- RPC handler must then wait for commit
    might not succeed if server loses leadership before committing command
    isleader: false if this server isn't the Raft leader, client should try another
    term: currentTerm, to help caller detect if leader is demoted
    index: log entry to watch to see if the command was committed
  ApplyMsg, with Index and Command
    Raft sends a message on the "apply channel" for each
    committed log entry. the service then knows to execute the
    command, and the leader uses the ApplyMsg
    to know when/what to reply to a waiting client RPC.
==========
6.824 2018 Lecture 6: Raft (2)

Recall the big picture:
  key/value service as the example, as in Lab 3
  goal: same client-visible behavior as single non-replicated server
  goal: available despite minority of failed/disconnected servers
  watch out for network partition and split brain!
  [diagram: clients, k/v layer, k/v table, raft layer, raft log]
  [client RPC -> Start() -> majority commit protocol -> applyCh]
  "state machine", application, service

a few reminders:
  leader commits/executes after a majority replies to AppendEntries
  leader tells commit to followers, which execute (== send on applyCh)
  why wait for just a majority? why not wait for all peers?
    availability requires progress even if minority are crashed
  why is a majority sufficient?
    any two majorities overlap
    so successive leaders' majorities overlap at at least one peer
    so next leader is guaranteed to see any log entry committed by previous leader
  it's majority of all peers (dead and alive), not just majority of live peers

*** topic: the Raft log (Lab 2B)

as long as the leader stays up:
  clients only interact with the leader
  clients aren't affected by follower actions

things only get interesting when changing leaders
  e.g. after the old leader fails
  how to change leaders without clients seeing anomalies?
    stale reads, repeated operations, missing operations, different order, &c

what do we want to ensure?
  if any server executes a given command in a log entry,
    then no server executes something else for that log entry
  (Figure 3's State Machine Safety)
  why? if the servers disagree on the operations, then a
    change of leader might change the client-visible state,
    which violates our goal of mimicing a single server.
  example:
    S1: put(k1,v1) | put(k1,v2) | ...
    S2: put(k1,v1) | put(k2,x)  | ...
    can't allow both to execute their 2nd log entries!

how can logs disagree after a crash?
  a leader crashes before sending last AppendEntries to all
    S1: 3
    S2: 3 3
    S3: 3 3
  worse: logs might have different commands in same entry!
    after a series of leader crashes, e.g.
        10 11 12 13  <- log entry #
    S1:  3
    S2:  3  3  4
    S3:  3  3  5

Raft forces agreement by having followers adopt new leader's log
  example:
  S3 is chosen as new leader for term 6
  S3 sends an AppendEntries with entry 13
     prevLogIndex=12
     prevLogTerm=5
  S2 replies false (AppendEntries step 2)
  S3 decrements nextIndex[S2] to 12
  S3 sends AppendEntries w/ entries 12+13, prevLogIndex=11, prevLogTerm=3
  S2 deletes its entry 12 (AppendEntries step 3)
  similar story for S1, but have to go back one farther

the result of roll-back:
  each live follower deletes tail of log that differs from leader
  then each live follower accepts leader's entries after that point
  now followers' logs are identical to leader's log

Q: why was it OK to forget about S2's index=12 term=4 entry?

could new leader roll back *committed* entries from end of previous term?
  i.e. could a committed entry be missing from the new leader's log?
  this would be a disaster -- old leader might have already said "yes" to a client
  so: Raft needs to ensure elected leader has all committed log entries

why not elect the server with the longest log as leader?
  example:
    S1: 5 6 7
    S2: 5 8
    S3: 5 8
  first, could this scenario happen? how?
    S1 leader in term 6; crash+reboot; leader in term 7; crash and stay down
      both times it crashed after only appending to its own log
    next term will be 8, since at least one of S2/S3 learned of 7 while voting
    S2 leader in term 8, only S2+S3 alive, then crash
  all peers reboot
  who should be next leader?
    S1 has longest log, but entry 8 could have committed !!!
    so new leader can only be one of S2 or S3
    i.e. the rule cannot be simply "longest log"

end of 5.4.1 explains the "election restriction"
  RequestVote handler only votes for candidate who is "at least as up to date":
    candidate has higher term in last log entry, or
    candidate has same last term and same length or longer log
  so:
    S2 and S3 won't vote for S1
    S2 and S3 will vote for each other
  so only S2 or S3 can be leader, will force S1 to discard 6,7
    ok since 6,7 not on majority -> not committed -> no reply sent to clients
    -> clients will resend commands in 6,7

the point:
  "at least as up to date" rule ensures new leader's log contains
    all potentially committed entries
  so new leader won't roll back any committed operation

The Question (from last lecture)
  figure 7, top server is dead; which can be elected?

depending on who is elected leader in Figure 7, different entries
  will end up committed or discarded
  c's 6 and d's 7,7 may be discarded OR committed
  some will always remain committed: 111445566

how to roll back quickly
  the Figure 2 design backs up one entry per RPC -- slow!
  lab tester may require faster roll-back
  paper outlines a scheme towards end of Section 5.3
    no details; here's my guess; better schemes are possible
  S1: 4 5 5      4 4 4      4
  S2: 4 6 6  or  4 6 6  or  4 6 6
  S3: 4 6 6      4 6 6      4 6 6
  S3 is leader for term 6, S1 comes back to life
  if follower rejects AppendEntries, it includes this in reply:
    the follower's term in the conflicting entry
    the index of follower's first entry with that term
  if leader has log entries with the follower's conflicting term:
    move nextIndex[i] back to leader's last entry for the conflicting term
  else:
    move nextIndex[i] back to follower's first index for the conflicting term

*** topic: persistence (Lab 2C)

what would we like to happen after a server crashes?
  Raft can continue with one missing server
    but we must repair soon to avoid dipping below a majority
  two strategies:
  * replace with a fresh (empty) server
    requires transfer of entire log (or snapshot) to new server (slow)
    we *must* support this, in case failure is permanent
  * or reboot crashed server, re-join with state intact, catch up
    requires state that persists across crashes
    we *must* support this, for simultaneous power failure
  let's talk about the second strategy -- persistence

if a server crashes and restarts, what must Raft remember?
  Figure 2 lists "persistent state":
    log[], currentTerm, votedFor
  a Raft server can only re-join after restart if these are intact
  thus it must save them to non-volatile storage
    non-volatile = disk, SSD, battery-backed RAM, &c
    save after each change
    before sending any RPC or RPC reply
  why log[]?
    if a server was in leader's majority for committing an entry,
      must remember entry despite reboot, so any future leader is
      guaranteed to see the committed log entry
  why votedFor?
    to prevent a client from voting for one candidate, then reboot,
      then vote for a different candidate in the same (or older!) term
    could lead to two leaders for the same term
  why currentTerm?
    to ensure that term numbers only increase
    to detect RPCs from stale leaders and candidates

some Raft state is volatile
  commitIndex, lastApplied, next/matchIndex[]
  Raft's algorithms reconstruct them from initial values

persistence is often the bottleneck for performance
  a hard disk write takes 10 ms, SSD write takes 0.1 ms
  so persistence limits us to 100 to 10,000 ops/second
  (the other potential bottleneck is RPC, which takes << 1 ms on a LAN)
  lots of tricks to cope with slowness of persistence:
    batch many new log entries per disk write
    persist to battery-backed RAM, not disk

how does the service (e.g. k/v server) recover its state after a crash+reboot?
  easy approach: start with empty state, re-play Raft's entire persisted log
    lastApplied is volatile and starts at zero, so you may need no extra code!
  but re-play will be too slow for a long-lived system
  faster: use Raft snapshot and replay just the tail of the log

*** topic: log compaction and Snapshots (Lab 3B)

problem:
  log will get to be huge -- much larger than state-machine state!
  will take a long time to re-play on reboot or send to a new server

luckily:
  a server doesn't need *both* the complete log *and* the service state
    the executed part of the log is captured in the state
    clients only see the state, not the log
  service state usually much smaller, so let's keep just that

what constrains how a server discards log entries?
  can't forget un-committed entries -- might be part of leader's majority
  can't forget un-executed entries -- not yet reflected in the state
  executed entries might be needed to bring other servers up to date

solution: service periodically creates persistent "snapshot"
  [diagram: service with state, snapshot on disk, raft log, raft persistent]
  copy of entire state-machine state as of execution of a specific log entry
    e.g. k/v table
  service writes snapshot to persistent storage (disk)
  service tells Raft it is snapshotted through some log index
  Raft discards log before that index
  a server can create a snapshot and discard prefix of log at any time
    e.g. when log grows too long

relation of snapshot and log
  snapshot reflects only executed log entries
    and thus only committed entries
  so server will only discard committed prefix of log
    anything not known to be committed will remain in log

so a server's on-disk state consists of:
  service's snapshot up to a certain log entry
  Raft's persisted log w/ following log entries
  the combination is equivalent to the full log

what happens on crash+restart?
  service reads snapshot from disk
  Raft reads persisted log from disk
    sends service entries that are committed but not in snapshot

what if a follower lags and leader has discarded past end of follower's log?
  nextIndex[i] will back up to start of leader's log
  so leader can't repair that follower with AppendEntries RPCs
  thus the InstallSnapshot RPC
  (Q: why not have leader discard only entries that *all* servers have?)

what's in an InstallSnapshot RPC? Figures 12, 13
  term
  lastIncludedIndex
  lastIncludedTerm
  snapshot data

what does a follower do w/ InstallSnapshot?
  reject if term is old (not the current leader)
  reject (ignore) if follower already has last included index/term
    it's an old/delayed RPC
  empty the log, replace with fake "prev" entry
  set lastApplied to lastIncludedIndex
  replace service state (e.g. k/v table) with snapshot contents

note that the state and the operation history are roughly equivalent
  designer can choose which to send
  e.g. last few operations (log entries) for lagging replica,
    but entire state (snapshot) for a replica that has lost its disk.
  still, replica repair can be very expensive, and warrants attention

The Question:
  Could a received InstallSnapshot RPC cause the state machine to go
  backwards in time? That is, could step 8 in Figure 13 cause the state
  machine to be reset so that it reflects fewer executed operations? If
  yes, explain how this could happen. If no, explain why it can't
  happen.

*** topic: configuration change (not needed for the labs)

configuration change (Section 6)
  configuration = set of servers
  sometimes you need to
    move to a new set of servers, or
    increase/decrease the number of servers
  human initiates configuration change, Raft manages it
  we'd like Raft to cope correctly with failure during configuration change
    i.e. clients should not notice (except maybe dip in performance)

why doesn't a straightforward approach work?
  suppose each server has the list of servers in the current config
  change configuration by telling each server the new list
    using some mechanism outside of Raft
  problem: they will learn new configuration at different times
  example: want to replace S3 with S4
    we get as far as telling S1 and S4 that the new config is 1,2,4
    S1: 1,2,3  1,2,4
    S2: 1,2,3  1,2,3
    S3: 1,2,3  1,2,3
    S4:        1,2,4
  OOPS! now *two* leaders could be elected!
    S2 and S3 could elect S2
    S1 and S4 could elect S1

Raft configuration change
  idea: "joint consensus" stage that includes *both* old and new configuration
    avoids any time when both old and new can choose leader independently
  system starts with Cold
  system administrator asks the leader to switch to Cnew
  Raft has special configuration log entries (sets of server addresses)
  each server uses the last configuration in its own log
  1. leader commits Cold,new to a majority of both Cold and Cnew
  2. after Cold,new commits, leader commits Cnew to servers in Cnew

what if leader crashes at various points in this process?
  can we have two leaders for the next term?
  if that could happen, each leader must be one of these:
    A. in Cold, but does not have Cold,new in log
    B. in Cold or Cnew, has Cold,new in log
    C. in Cnew, has Cnew in log
  we know we can't have A+A or C+C by the usual rules of leader election
  A+B? no, since B needs majority from Cold as well as Cnew
  A+C? no, since can't proceed to Cnew until Cold,new committed to Cold
  B+B? no, since B needs majority from both Cold and Cnew
  B+C? no, since B needs majority from Cnew as well as Cold

good! Raft can switch to a new set of servers w/o risk of two active leaders

*** topic: performance

Note: many situations don't require high performance.
  key/value store might.
  but GFS or MapReduce master might not.

Most replication systems have similar common-case performance:
  One RPC exchange and one disk write per agreement.
  So Raft is pretty typical for message complexity.

Raft makes a few design choices that sacrifice performance for simplicity:
  Follower rejects out-of-order AppendEntries RPCs.
    Rather than saving for use after hole is filled.
    Might be important if network re-orders packets a lot.
  No provision for batching or pipelining AppendEntries.
  Snapshotting is wasteful for big states.
  A slow leader may hurt Raft, e.g. in geo-replication.

These have a big effect on performance:
  Disk writes for persistence.
  Message/packet/RPC overhead.
  Need to execute logged commands sequentially.
  Fast path for read-only operations.

Papers with more attention to performance:
  Zookeeper/ZAB; Paxos Made Live; Harp

==========
6.824 2017 Lecture 8: Zookeeper Case Study

Reading: "ZooKeeper: wait-free coordination for internet-scale systems", Patrick
Hunt, Mahadev Konar, Flavio P. Junqueira, Benjamin Reed.  Proceedings of the 2010
USENIX Annual Technical Conference.

Why are we reading this paper?
  Widely-used replicated state machine service
    Inspired by Chubby (Google's global lock service)
    Originally at Yahoo!, now outside too (Mesos, HBase, etc.)
  Open source
    As Apache project (http://zookeeper.apache.org/)
  Case study of building replicated services, given a Paxos/ZAB/Raft library
    Similar issues show up in lab 3
  API supports a wide-range of use cases
    Application that need a fault-tolerant "master" don't need to roll their own
    Zookeeper is generic enough that they should be able to use Zookeeper
  High performance
    Unlike lab 3's replicate key/value service

Motivation: many applications in datacenter cluster need to coordinate
  Example: GFS
    master has list of chunk servers for each chunk
    master decides which chunk server is primary
    etc.
  Other examples: YMB, Crawler, etc.
    YMB needs master to shard topics
    Crawler needs master that commands page fetching
      (e.g., a bit like the master in mapreduce)
  Applications also need to find each other
    MapReduce needs to know IP:PORT of GFS master
    Load balancer needs to know where web servers are
  Coordination service typically used for this purpose

Motivation: performance -- lab3
  dominated by Raft
  consider a 3-node Raft
  before returning to client, Raft performs
    leader persists log entry
    in parallel, leader send message to followers
      each follower persist log entry
      each follower responds
  -> 2 disk writes and one round trip
    if magnetic disk: 2*10msec = 50 msg/sec
    if SSD: 2*2msec+1msec = 200 msg/sec
  Zookeeper performs 21,000 msg/sec
    asynchronous calls
    allows pipelining

Alternative plan: develop fault-tolerant master for each application
  announce location via DNS
  OK, if writing master isn't complicated
  But, master often needs to be:
    fault tolerant
      every application figures how to use Raft?
    high performance
      every application figures how to make "read" operations fast?
  DNS propagation is slow
    fail-over will take a long time!
  Some application settle for single-point of failure
    E.g., GFS and MapReduce
    Less desirable

Zookeeper: a generic coordination service
  Design challenges:
    What API?
    How to make master fault tolerant?
    How to get good performance?
  Challenges interact
    good performance may influence API
    e.g., asynchronous interface to allow pipelining

Zookeeper API overview
  [diagram: ZooKeeper, client sessions, ZAB layer]
  replicated state machine
    several servers implementing the service
    operations are performed in global order
      with some exceptions, if consistency isn't important
  the replicated objects are: znodes
    hierarchy of znodes
      named by pathnames
    znodes contain *metadata* of application
      configuration information
        machines that participate in the application
        which machine is the primary
      timestamps
      version number
    types of znodes:
      regular
      empheral
      sequential: name + seqno
        If n is the new znode and p is the parent znode, then the sequence
        value of n is never smaller than the value in the name of any other
        sequential znode ever created under p.

  sessions
    clients sign into zookeeper
    session allows a client to fail-over to another Zookeeper service
      client know the term and index of last completed operation (zxid)
      send it on each request
        service performs operation only if caught up with what client has seen
    sessions can timeout
      client must refresh a session continuously
        send a heartbeat to the server (like a lease)
      ZooKeeper considers client "dead" if doesn't hear from a client
      client may keep doing its thing (e.g., network partition)
        but cannot perform other ZooKeeper ops in that session
    no analogue to this in Raft + Lab 3 KV store

Operations on znodes
  create(path, data, flags)
  delete(path, version)
      if znode.version = version, then delete
  exists(path, watch)
  getData(path, watch)
  setData(path, data, version)
    if znode.version = version, then update
  getChildren(path, watch)
  sync()
   above operations are *asynchronous*
   all operations are FIFO-ordered per client
   sync waits until all preceding operations have been "propagated"

Check: can we just do this with lab 3's KV service?
  flawed plan: GFS master on startup does Put("gfs-master", my-ip:port)
    other applications + GFS nodes do Get("gfs-master")
  problem: what if two master candidates' Put()s race?
    later Put() wins
    each presumed master needs to read the key to see if it actually is the master
      when are we assured that no delayed Put() thrashes us?
      every other client must have seen our Put() -- hard to guarantee
  problem: when master fails, who decides to remove/update the KV store entry?
    need some kind of timeout
    so master must store tuple of (my-ip:port, timestamp)
      and continuously Put() to refresh the timestamp
      others poll the entry to see if the timestamp stops changing
  lots of polling + unclear race behavior -- complex
  ZooKeeper API has a better story: watches, sessions, atomic znode creation
    + only one creation can succeed -- no Put() race
    + sessions make timeouts easy -- no need to store and refresh explicit timestamps
    + watches are lazy notifications -- avoids commiting lots of polling reads

Ordering guarantees
  all write operations are totally ordered
    if a write is performed by ZooKeeper, later writes from other clients see it
    e.g., two clients create a znode, ZooKeeper performs them in some total order
  all operations are FIFO-ordered per client
  implications:
    a read observes the result of an earlier write from the same client
    a read observes some prefix of the writes, perhaps not including most recent write
      -> read can return stale data
    if a read observes some prefix of writes, a later read observes that prefix too

Example "ready" znode:
  A failure happens
  A primary sends a stream of writes into Zookeeper
    W1...Wn C(ready)
  The final write updates ready znode
    -> all preceding writes are visible
  The final write causes the watch to go off at backup
    backup issues R(ready) R1...Rn
    however, it will observe all writes because zookeeper will delay read until
      node has seen all txn that watch observed
  Lets say failure happens during R1 .. Rn, say after return Rj to client
    primary deletes ready file -> watch goes off
    watch alert is sent to client
    client knows it must issue new R(ready) R1 ...Rn
  Nice property: high performance
    pipeline writes and reads
    can read from *any* zookeeper node

Example usage 1: slow lock
  acquire lock:
   retry:
     r = create("app/lock", "", empheral)
     if r:
       return
     else:
       getData("app/lock", watch=True)

    watch_event:
       goto retry

  release lock: (voluntarily or session timeout)
    delete("app/lock")

Example usage 2: "ticket" locks
  acquire lock:
     n = create("app/lock/request-", "", empheral|sequential)
   retry:
     requests = getChildren(l, false)
     if n is lowest znode in requests:
       return
     p = "request-%d" % n - 1
     if exists(p, watch = True)
       goto retry

    watch_event:
       goto retry

  Q: can watch_even fire before lock it is the client's turn
  A: yes
     lock/request-10 <- current lock holder
     lock/request-11 <- next one
     lock/request-12 <- my request

     if client associated with request-11 dies before it gets the lock, the
     watch even will fire but it isn't my turn yet.

Using locks
  Not straight forward: a failure may cause your lock to be revoked
    client 1 acquires lock
      starts doing its stuff
      network partitions
      zookeeper declares client 1 dead (but it isn't)
    client 2 acquires lock, but client 1 still believes it has it
      can be avoided by setting timeouts correctly
      need to disconnect client 1 session before ephemeral nodes go away
      requires session heartbeats to be replicated to majority
        N.B.: paper doesn't discuss this
  For some cases, locks are a performance optimization
    for example, client 1 has a lock on crawling some urls
    client will do it 2 now, but that is fine
  For other cases, locks are a building block
    for example, application uses it to build transaction
    the transactions are all-or-nothing
    we will see an example in the Frangipani paper

Zookeeper simplifies building applications but is not an end-to-end solution
  Plenty of hard problems left for application to deal with
  Consider using Zookeeper in GFS
    I.e., replace master with Zookeeper
  Application/GFS still needs all the other parts of GFS
    the primary/backup plan for chunks
    version numbers on chunks
    protocol for handling primary fail over
    etc.
  With Zookeeper, at least master is fault tolerant
    And, won't run into split-brain problem
    Even though it has replicated servers

Implementation overview
  Similar to lab 3 (see last lecture)
  two layers:
    ZooKeeper services  (K/V service)
    ZAB layer (Raft layer)
  Start() to insert ops in bottom layer
  Some time later ops pop out of bottom layer on each replica
    These ops are committed in the order they pop out
    on apply channel in lab 3
    the abdeliver() upcall in ZAB

Challenge: Duplicates client requests
  Scenario
    Primary receives client request, fails
    Client resends client request to new primary
  Lab 3:
    Table to detect duplicates
    Limitation: one outstanding op per client
    Problem problem: cannot pipeline client requests
  Zookeeper:
    Some ops are idempotent period
    Some ops are easy to make idempotent
      test-version-and-then-do-op
      e.g., include timestamp and version in setDataTXN

Challenge: Read operations
  Many operations are read operations
    they don't modify replicated state
  Must they go through ZAB/Raft or not?
  Can any replica execute the read op?
  Performance is slow if read ops go through Raft

Problem: read may return stale data if only master performs it
  The primary may not know that it isn't the primary anymore
    a network partition causes another node to become primary
    that partition may have processed write operations
  If the old primary serves read operations, it won't have seen those write ops
   => read returns stale data

Zookeeper solution: don't promise non-stale data (by default)
  Reads are allowed to return stale data
    Reads can be executed by any replica
    Read throughput increases as number of servers increases
    Read returns the last zxid it has seen
     So that new primary can catch up to zxid before serving the read
     Avoids reading from past
  Only sync-read() guarantees data is not stale

Sync optimization: avoid ZAB layer for sync-read
  must ensure that read observes last committed txn
  leader puts sync in queue between it and replica
    if ops ahead of in the queue commit, then leader must be leader
    otherwise, issue null transaction
  in same spirit read optimization in Raft paper
    see last par section 8 of raft paper

Performance (see table 1)
  Reads inexpensive
    Q: Why more reads as servers increase?
  Writes expensive
    Q: Why slower with increasing number of servers?
  Quick failure recovery (figure 8)
    Decent throughout even while failures happen

References:
  ZAB: http://dl.acm.org/citation.cfm?id=2056409
  https://zookeeper.apache.org/
  https://cs.brown.edu/~mph/Herlihy91/p124-herlihy.pdf  (wait free, universal
  objects, etc.)
==========
2018 Lecture 7: Raft (3) -- Snapshots, Linearizability, Duplicate Detection

this lecture:
  Raft snapshots
  linearizability
  duplicate RPCs
  faster gets

*** Raft log compaction and snapshots (Lab 3B)

problem:
  log will get to be huge -- much larger than state-machine state!
  will take a long time to re-play on reboot or send to a new server

luckily:
  a server doesn't need *both* the complete log *and* the service state
    the executed part of the log is captured in the state
    clients only see the state, not the log
  service state usually much smaller, so let's keep just that

what entries *can't* a server discard?
  un-executed entries -- not yet reflected in the state
  un-committed entries -- might be part of leader's majority

solution: service periodically creates persistent "snapshot"
  [diagram: service state, snapshot on disk, raft log, raft persistent]
  copy of service state as of execution of a specific log entry
    e.g. k/v table
  service writes snapshot to persistent storage (disk)
  service tells Raft it is snapshotted through some log index
  Raft discards log before that index
  a server can create a snapshot and discard prefix of log at any time
    e.g. when log grows too long

what happens on crash+restart?
  service reads snapshot from disk
  Raft reads persisted log from disk
  Raft log may start before the snapshot (but definitely not after)
    Raft will (re-)send committed entries on the applyCh
      since applyIndex starts at zero after reboot
    service will see repeats, must detect repeated index, ignore

what if follower's log ends before leader's log starts?
  nextIndex[i] will back up to start of leader's log
  so leader can't repair that follower with AppendEntries RPCs
  thus the InstallSnapshot RPC

what's in an InstallSnapshot RPC? Figures 12, 13
  term
  lastIncludedIndex
  lastIncludedTerm
  snapshot data

what does a follower do w/ InstallSnapshot?
  ignore if term is old (not the current leader)
  ignore if follower already has last included index/term
    it's an old/delayed RPC
  if not ignored:
    empty the log, replace with fake "prev" entry
    set lastApplied to lastIncludedIndex
    replace service state (e.g. k/v table) with snapshot contents

philosophical note:
  state is often equivalent to operation history
  you can often choose which one to store or communicate
  we'll see examples of this duality later in the course

practical notes:
  Raft layer and service layer cooperate to save/restore snapshots
  Raft's snapshot scheme is reasonable if the state is small
  for a big DB, e.g. if replicating gigabytes of data, not so good
    slow to create and write to disk
  perhaps service data should live on disk in a B-Tree
    no need to explicitly persist, since on disk already
  dealing with lagging replicas is hard, though
    leader should save the log for a while
    or save identifiers of updated records
  and fresh servers need the whole state

*** linearizability

we need a definition of "correct" for Lab 3 &c
  how should clients expect Put and Get to behave?
  often called a consistency contract
  helps us reason about how to handle complex situations correctly
    e.g. concurrency, replicas, failures, RPC retransmission,
         leader changes, optimizations
  we'll see many consistency definitions in 6.824
    e.g. Spinnaker's timeline consistency

"linearizability" is the most common and intuitive definition
  formalizes behavior expected of a single server

linearizability definition:
  an execution history is linearizable if
    one can find a total order of all operations,
    that matches real-time (for non-overlapping ops), and
    in which each read sees the value from the
    write preceding it in the order.

a history is a record of client operations, each with
  arguments, return value, time of start, time completed

example 1:
  |-Wx1-| |-Wx2-|
    |---Rx2---|
      |-Rx1-|
"Wx1" means "write value 1 to record x"
"Rx1" means "a read of record x yielded value 1"
order: Wx1 Rx1 Wx2 Rx2
  the order obeys value constraints (W -> R)
  the order obeys real-time constraints (Wx1 -> Wx2)
  so the history is linearizable

example 2:
  |-Wx1-| |-Wx2-|
    |--Rx2--|
              |-Rx1-|
Wx2 then Rx2 (value), Rx2 then Rx1 (time), Rx1 then Wx2 (value). but
that's a cycle -- so it cannot be turned into a linear order. so this
is not linearizable.

example 3:
|--Wx0--|  |--Wx1--|
            |--Wx2--|
        |-Rx2-| |-Rx1-|
order: Wx0 Wx2 Rx2 Wx1 Rx1
so it's linearizable.
note the service can pick the order for concurrent writes.
  e.g. Raft placing concurrent ops in the log.

example 4:
|--Wx0--|  |--Wx1--|
            |--Wx2--|
C1:     |-Rx2-| |-Rx1-|
C2:     |-Rx1-| |-Rx2-|
we have to be able to fit all operations into a single order
  maybe: Wx2 C1:Rx2 Wx1 C1:Rx1 C2:Rx1
    but where to put C2:Rx2?
      must come after C2:Rx1 in time
      but then it should have read value 1
  no order will work:
    C1's reads require Wx2 before Wx1
    C2's reads require Wx1 before Wx2
    that's a cycle, so there's no order
  not linearizable!
so: all clients must see concurrent writes in the same order

example 5:
ignoring recent writes is not linearizable
Wx1      Rx1
    Wx2
this rules out split brain, and forgetting committed writes

You may find this page useful:
https://www.anishathalye.com/2017/06/04/testing-distributed-systems-for-linearizability/

*** duplicate RPC detection (Lab 3)

What should a client do if a Put or Get RPC times out?
  i.e. Call() returns false
  if server is dead, or request dropped: re-send
  if server executed, but request lost: re-send is dangerous

problem:
  these two cases look the same to the client (no reply)
  if already executed, client still needs the result

idea: duplicate RPC detection
  let's have the k/v service detect duplicate client requests
  client picks an ID for each request, sends in RPC
    same ID in re-sends of same RPC
  k/v service maintains table indexed by ID
  makes an entry for each RPC
    record value after executing
  if 2nd RPC arrives with the same ID, it's a duplicate
    generate reply from the value in the table

design puzzles:
  when (if ever) can we delete table entries?
  if new leader takes over, how does it get the duplicate table?
  if server crashes, how does it restore its table?

idea to keep the duplicate table small
  one table entry per client, rather than one per RPC
  each client has only one RPC outstanding at a time
  each client numbers RPCs sequentially
  when server receives client RPC #10,
    it can forget about client's lower entries
    since this means client won't ever re-send older RPCs

some details:
  each client needs a unique client ID -- perhaps a 64-bit random number
  client sends client ID and seq # in every RPC
    repeats seq # if it re-sends
  duplicate table in k/v service indexed by client ID
    contains just seq #, and value if already executed
  RPC handler first checks table, only Start()s if seq # > table entry
  each log entry must include client ID, seq #
  when operation appears on applyCh
    update the seq # and value in the client's table entry
    wake up the waiting RPC handler (if any)

what if a duplicate request arrives before the original executes?
  could just call Start() (again)
  it will probably appear twice in the log (same client ID, same seq #)
  when cmd appears on applyCh, don't execute if table says already seen

how does a new leader get the duplicate table?
  all replicas should update their duplicate tables as they execute
  so the information is already there if they become leader

if server crashes how does it restore its table?
  if no snapshots, replay of log will populate the table
  if snapshots, snapshot must contain a copy of the table

but wait!
  the k/v server is now returning old values from the duplicate table
  what if the reply value in the table is stale?
  is that OK?

example:
  C1           C2
  --           --
  put(x,10)
               first send of get(x), 10 reply dropped
  put(x,20)
               re-sends get(x), gets 10 from table, not 20

what does linearizabilty say?
C1: |-Wx10-|          |-Wx20-|
C2:          |-Rx10-------------|
order: Wx10 Rx10 Wx20
so: returning the remembered value 10 is correct

*** read-only operations (end of Section 8)

Q: does the Raft leader have to commit read-only operations in
   the log before replying? e.g. Get(key)?

that is, could the leader respond immediately to a Get() using
  the current content of its key/value table?

A: no, not with the scheme in Figure 2 or in the labs.
   suppose S1 thinks it is the leader, and receives a Get(k).
   it might have recently lost an election, but not realize,
   due to lost network packets.
   the new leader, say S2, might have processed Put()s for the key,
   so that the value in S1's key/value table is stale.
   serving stale data is not linearizable; it's split-brain.
   
so: Figure 2 requires Get()s to be committed into the log.
    if the leader is able to commit a Get(), then (at that point
    in the log) it is still the leader. in the case of S1
    above, which unknowingly lost leadership, it won't be
    able to get the majority of positive AppendEntries replies
    required to commit the Get(), so it won't reply to the client.

but: many applications are read-heavy. committing Get()s
  takes time. is there any way to avoid commit
  for read-only operations? this is a huge consideration in
  practical systems.

idea: leases
  modify the Raft protocol as follows
  define a lease period, e.g. 5 seconds
  after each time the leader gets an AppendEntries majority,
    it is entitled to respond to read-only requests for
    a lease period without commiting read-only requests
    to the log, i.e. without sending AppendEntries.
  a new leader cannot execute Put()s until previous lease period
    has expired
  so followers keep track of the last time they responded
    to an AppendEntries, and tell the new leader (in the
    RequestVote reply).
  result: faster read-only operations, still linearizable.

note: for the Labs, you should commit Get()s into the log;
      don't implement leases.

Spinnaker optionally makes reads even faster, sacrificing linearizability
  Spinnaker's "timeline reads" are not required to reflect recent writes
    they are allowed to return an old (though committed) value
  Spinnaker uses this freedom to speed up reads:
    any replica can reply to a read, allowing read load to be parallelized
  but timeline reads are not linearizable:
    replica may not have heard recent writes
    replica may be partitioned from the leader!

in practice, people are often (but not always) willing to live with stale
  data in return for higher performance
==========
<!DOCTYPE html>
<html>
<head>
<link rel="stylesheet" href="../style.css" type="text/css">
<title>6.824 Lab 3: Fault-tolerant Key/Value Service</title>
</head>
<body>
<div align="center">
<h2><a href="../index.html">6.824</a> - Spring 2020</h2>
<h1>6.824 Lab 3: Fault-tolerant Key/Value Service</h1>
<h3>Due Part A: Mar 13 23:59</h3>
<h3>Due Part B: Apr 10 23:59</h3>
</div>
<hr>

<h3>Introduction</h3>

<p>
In this lab you will build a fault-tolerant key/value storage
service using your Raft library from
<a href="lab-raft.html">lab 2</a>. You
key/value service will be a replicated state machine, consisting of
several key/value servers that use Raft to maintain replication.
Your key/value service should continue to
process client requests as long as a majority of the servers
are alive and can communicate, in spite of other failures or
network partitions.

<p>
The service supports three operations: <tt>Put(key, value)</tt>,
<tt>Append(key, arg)</tt>, and <tt>Get(key)</tt>. It maintains a
simple database of key/value pairs. <tt>Put()</tt> replaces the value
for a particular key in the database, <tt>Append(key, arg)</tt>
appends arg to key's value, and <tt>Get()</tt> fetches the current
value for a key. An <tt>Append</tt> to a non-existant key should act
like <tt>Put</tt>. Each client talks to the service through
a <tt>Clerk</tt> with Put/Append/Get methods. A <tt>Clerk</tt> manages
RPC interactions with the servers.

<p>
Your service must provide strong consistency to applications calls to
the <tt>Clerk</tt> Get/Put/Append methods. Here's what we mean by
strong consistency. If called one at a time, the Get/Put/Append
methods should act as if the system had only one copy of its state,
and each call should observe the modifications to the state implied by
the preceding sequence of calls. For concurrent calls, the return
values and final state must be the same as if the operations had
executed one at a time in some order. Calls are concurrent if they
overlap in time, for example if client X calls <tt>Clerk.Put()</tt>,
then client Y calls <tt>Clerk.Append()</tt>, and then client X's call
returns. Furthermore, a call must observe the effects of all calls
that have completed before the call starts (so we are technically
asking for linearizability).

<p>
Strong consistency is convenient for applications because it means
that, informally, all clients see the same state and they all see the
latest state. Providing strong consistency is relatively easy for
a single server. It is harder if the service is replicated,
since all servers must choose the same execution order for
concurrent requests, and must avoid replying to clients using state
that isn't up to date.

<p>
This lab has two parts. In part A, you will implement the
service without worrying that the Raft log can grow without
bound. In part B, you will implement snapshots (Section 7 in
the paper), which will allow Raft to garbage collect old log
entries. Please submit each part by the respective deadline.

<ul class="hints">
<li>
This lab doesn't require you to write much code, but
you will most likely spend a substantial amount of time
thinking and staring at debugging logs to figure out
why your implementation doesn't work. Debugging will be
more challenging than in the Raft lab because there are
more components that work asynchronously of each other.
Start early.

<li>
You should reread the
<a href="../papers/raft-extended.pdf">extended Raft paper</a>,
in particular Sections 7 and 8. For a wider
perspective, have a look at Chubby, Raft Made Live,
Spanner, Zookeeper, Harp, Viewstamped Replication, and
<a href="http://static.usenix.org/event/nsdi11/tech/full_papers/Bolosky.pdf">Bolosky et al.</a>

<li>
You are allowed to add fields to the Raft <tt>ApplyMsg</tt>,
and to add fields to Raft RPCs such as <tt>AppendEntries</tt>.
But be sure that your code continues to pass the Lab 2 tests.

</ul>

<h3>Collaboration Policy</h3>

You must write all the code you hand in for 6.824, except for
code that we give you as part of the assignment. You are not
allowed to look at anyone else's solution, you are not allowed
to look at code from previous years, and you are not allowed to
look at other Raft implementations. You may discuss the
assignments with other students, but you may not look at or
copy each others' code.

<p>
Please do not publish your code or make
it available to current or future 6.824 students.
<tt>github.com</tt> repositories are public by default, so please
don't put your code there unless you make the repository private. You
may find it convenient to use
<a href="https://github.mit.edu/">MIT's GitHub</a>,
but be sure to create a private repository.

<h3>Getting Started</h3>

<div class="important">
<p>
Do a <tt>git pull</tt> to get the latest lab software.
</div>

<p>
We supply you with skeleton code and tests in <tt>src/kvraft</tt>. You will
need to modify <tt>kvraft/client.go</tt>, <tt>kvraft/server.go</tt>, and
perhaps <tt>kvraft/common.go</tt>.

<p>
To get up and running, execute the following commands:
<pre>
$ cd ~/6.824
$ git pull
...
$ cd src/kvraft
$ GOPATH=~/6.824
$ export GOPATH
$ go test
...
$</pre>

<h3>Part A: Key/value service without log compaction</h3>

<p>
Each of your key/value servers ("kvservers") will have an associated
Raft peer. Clerks send <tt>Put()</tt>, <tt>Append()</tt>,
and <tt>Get()</tt> RPCs to the kvserver whose associated Raft is the
leader. The kvserver code submits the Put/Append/Get operation to
Raft, so that the Raft log holds a sequence of Put/Append/Get
operations. All of the kvservers execute operations from the Raft log
in order, applying the operations to their key/value databases; the
intent is for the servers to maintain identical replicas of the
key/value database.

<p>
A <tt>Clerk</tt> sometimes doesn't
know which kvserver is the Raft leader. If the <tt>Clerk</tt> sends an
RPC to the wrong kvserver, or if it cannot reach the kvserver,
the <tt>Clerk</tt> should re-try by sending to a different kvserver.
If the key/value service commits the operation to its Raft log
(and hence applies the operation to the key/value state machine), the
leader reports the result to the <tt>Clerk</tt> by responding to its
RPC. If the operation failed to commit (for example, if the leader was
replaced), the server reports an error, and the <tt>Clerk</tt> retries with a
different server.

<div class="todo">
<p>
Your first task is to implement a solution that works when there are no dropped
messages, and no failed servers.

<p>
You'll need to add RPC-sending code to the Clerk Put/Append/Get
methods in <tt>client.go</tt>, and implement
<tt>PutAppend()</tt> and <tt>Get()</tt> RPC handlers in
<tt>server.go</tt>. These handlers should enter an
<tt>Op</tt> in the Raft log using <tt>Start()</tt>; you should fill in
the <tt>Op</tt> struct definition in <tt>server.go</tt> so that it
describes a Put/Append/Get operation. Each server should
execute <tt>Op</tt> commands as Raft commits them, i.e.
as they appear on the <tt>applyCh</tt>. An RPC handler
should notice when Raft commits its <tt>Op</tt>, and then reply to the
RPC.

<p>
You have completed this task when you
<strong>reliably</strong> pass the first test in the
test suite: "One client". You may also find that you
can pass the "concurrent clients" test, depending on
how sophisticated your implementation is.
</div>

<p class="note">
Your kvservers should not directly communicate; they
should only interact with each other through the Raft log.

<ul class="hints">
<li>
After calling <tt>Start()</tt>, your 
kvservers will need to wait for Raft to complete
agreement. Commands that have been agreed upon arrive
on the <tt>applyCh</tt>. You should think carefully
about how to arrange your code so that it will
keep reading <tt>applyCh</tt>, while
<tt>PutAppend()</tt> and <tt>Get()</tt> handlers submit
commands to the Raft log using <tt>Start()</tt>. It is
easy to achieve deadlock between the kvserver and its
Raft library.

<li>
Your solution needs to handle the case in
which a leader has called Start() for a Clerk's
RPC, but loses its leadership before the
request is committed to the log. In this case
you should arrange for the Clerk to re-send
the request to other servers until it finds
the new leader. One way to do this is for the
server to detect that it has lost leadership,
by noticing that a different request has
appeared at the index returned by Start(), or
that Raft's term has
changed. If the ex-leader is partitioned by
itself, it won't know about new leaders; but
any client in the same partition won't be able
to talk to a new leader either, so it's OK in
this case for the server and client to wait
indefinitely until the partition heals.

<li>
You will probably have to modify your 
Clerk to remember which server turned out to
be the leader for the last RPC, and send the
next RPC to that server first. This will avoid
wasting time searching for the leader on every
RPC, which may help you pass some of the tests
quickly enough.

<li>
A kvserver should not complete a <tt>Get()</tt>
RPC if it is not part of a majority (so that it does
not serve stale data). A simple solution is to enter
every <tt>Get()</tt> (as well as each <tt>Put()</tt>
and <tt>Append()</tt>) in the Raft log. You don't have
to implement the optimization for read-only operations
that is described in Section 8.

<li>
It's best to add locking from the start because the
need to avoid deadlocks sometimes affects overall code design. Check
that your code is race-free using
<tt>go test -race</tt>.

</ul>

In the face of unreliable connections and server failures, a
<tt>Clerk</tt> may send an RPC multiple times until it finds a 
kvserver that replies positively. If a leader fails just after
committing an entry to the Raft log, the <tt>Clerk</tt> may not
receive a reply, and thus may 
re-send the request to another leader.
Each call to
<tt>Clerk.Put()</tt> or <tt>Clerk.Append()</tt> should
result in just a single execution, so you will have to ensure
that the re-send doesn't result in the servers executing
the request twice.

<p class="todo">
Add code to cope with duplicate <tt>Clerk</tt> requests, including
situations where the <tt>Clerk</tt> sends a request to a kvserver leader
in one term, times out waiting for a reply, and re-sends the
request to a new leader in another term. The request
should always execute just once.
Your code should pass the <tt>go test -run 3A</tt> tests.

<ul class="hints">
<li>
You will need to uniquely identify client operations to ensure that
the key/value service executes each one just once.

<li>
Your scheme for duplicate detection should free server memory quickly,
for example by having each RPC imply that the client has seen the
reply for its previous RPC. It's OK to assume that a client will make
only one call into a Clerk at a time.

</ul>

<p>
Your code should now pass the Lab 3A tests, like this:

<pre>
$ go test -run 3A
Test: one client (3A) ...
  ... Passed --  15.1  5 12882 2587
Test: many clients (3A) ...
  ... Passed --  15.3  5  9678 3666
Test: unreliable net, many clients (3A) ...
  ... Passed --  17.1  5  4306 1002
Test: concurrent append to same key, unreliable (3A) ...
  ... Passed --   0.8  3   128   52
Test: progress in majority (3A) ...
  ... Passed --   0.9  5    58    2
Test: no progress in minority (3A) ...
  ... Passed --   1.0  5    54    3
Test: completion after heal (3A) ...
  ... Passed --   1.0  5    59    3
Test: partitions, one client (3A) ...
  ... Passed --  22.6  5 10576 2548
Test: partitions, many clients (3A) ...
  ... Passed --  22.4  5  8404 3291
Test: restarts, one client (3A) ...
  ... Passed --  19.7  5 13978 2821
Test: restarts, many clients (3A) ...
  ... Passed --  19.2  5 10498 4027
Test: unreliable net, restarts, many clients (3A) ...
  ... Passed --  20.5  5  4618  997
Test: restarts, partitions, many clients (3A) ...
  ... Passed --  26.2  5  9816 3907
Test: unreliable net, restarts, partitions, many clients (3A) ...
  ... Passed --  29.0  5  3641  708
Test: unreliable net, restarts, partitions, many clients, linearizability checks (3A) ...
  ... Passed --  26.5  7 10199  997
PASS
ok      kvraft  237.352s
</pre>

<p>
The numbers after each <tt>Passed</tt> are real time in seconds,
number of peers, number of RPCs sent (including client RPCs), and
number of key/value operations executed (<tt>Clerk</tt> Get/Put/Append
calls).

<h3>Handin procedure for lab 3A</h3>

<div class="important">
<p>
Before submitting, please run the tests for part A
one final time.
Some bugs may not appear on
every run, so run the tests multiple times.
</div>

<p>
Submit your code via the class's submission website, located at
<a href="https://6824.scripts.mit.edu/2020/handin.py/">https://6824.scripts.mit.edu/2020/handin.py/</a>.

<p>
You may use your MIT Certificate or request an API key via
email to log in for the first time. Your API key (<tt>XXX</tt>)
is displayed once you are logged in, which can be used to upload
the lab from the console as follows.

<pre>
$ cd "$GOPATH"
$ echo "XXX" &gt; api.key
$ make lab3a</pre>

<p class="important">
Check the submission website to make sure it sees your submission!

<p class="note">
You may submit multiple times. We will use the timestamp of
your <strong>last</strong> submission for the purpose of
calculating late days. Your grade is determined by the score
your solution <strong>reliably</strong> achieves when we run
the tester on our test machines.

<h3>Part B: Key/value service with log compaction</h3>

<div class="important">
<p>
Do a <tt>git pull</tt> to get the latest lab software.
</div>

<p>
As things stand now with your lab code, a rebooting server replays the
complete Raft log in order to restore its state. However, it's not
practical for a long-running server to remember the complete Raft log
forever. Instead, you'll modify Raft and kvserver to cooperate to save
space: from time to time kvserver will persistently store a "snapshot"
of its current state, and Raft will discard log entries that precede
the snapshot. When a server restarts (or falls far behind the leader
and must catch up), the server first installs a snapshot and then
replays log entries from after the point at which the snapshot
was created.
Section 7 of the
<a href="../papers/raft-extended.pdf">extended Raft paper</a>
outlines the scheme; you will have to design the details.

<p>
You should spend some time figuring out what the
interface will be between your Raft library and your
service so that your Raft library can discard log
entries. Think about how your Raft will operate while
storing only the tail of the log, and how it will
discard old log entries. You should discard them in a
way that allows the Go garbage collector to free and
re-use the memory; this requires that there be no
reachable references (pointers) to the discarded log
entries.

<p>
The tester passes <tt>maxraftstate</tt> to your
<tt>StartKVServer()</tt>. <tt>maxraftstate</tt> indicates the maximum
allowed size of your persistent Raft state in bytes (including the
log, but not including snapshots). You should
compare <tt>maxraftstate</tt> to <tt>persister.RaftStateSize()</tt>.
Whenever your key/value server detects that the Raft state size is
approaching this threshold, it should save a snapshot, and tell the
Raft library that it has snapshotted, so that Raft can discard old log
entries. If <tt>maxraftstate</tt> is -1, you do not have to snapshot.

<p class="todo">
Your <tt>raft.go</tt> probably keeps the entire log in a Go slice.
Modify it so that it can be given a log index, discard the entries
before that index, and continue operating while storing only log
entries after that index. Make sure you pass all the Raft tests after
making these changes.

<p class="todo">
Modify your kvserver so that it detects when the persisted
Raft state grows too large, and then hands a snapshot 
to Raft and tells
Raft that it can discard old log entries.
Raft should
save each snapshot with persister.SaveStateAndSnapshot()
(don't use files).
A kvserver instance should restore the snapshot from the
persister when it re-starts.

<ul class="hints">
<li>
You can test your Raft and kvserver's ability to operate with
a trimmed log, and its ability to re-start from the
combination of a kvserver snapshot and persisted Raft state,
by running the Lab 3A tests while artificially setting
<tt>maxraftstate</tt> to 1.
<li>
Think about when a kvserver should snapshot its state
and what should be included in the snapshot. Raft must
store each snapshot in the persister object using
<tt>SaveStateAndSnapshot()</tt>,
along with corresponding Raft state.
You can read the
latest stored snapshot using <tt>ReadSnapshot()</tt>.
<li>
Your kvserver must be able to detect
duplicated operations in the log across checkpoints, so any
state you are using to detect them must be included in
the snapshots. Remember to capitalize all fields of
structures stored in the snapshot.
<li>
You are allowed to add methods to your Raft so that
kvserver can manage the process of trimming the Raft
log and manage kvserver snapshots.
</ul>

<p class="todo">
Modify your Raft leader code to send an InstallSnapshot
RPC to a follower when the leader has discarded the log
entries the follower needs.
When a follower receives an InstallSnapshot RPC,
your Raft code will need to send the included snapshot to
its kvserver. You can use the <tt>applyCh</tt>
for this purpose, by adding new fields to <tt>ApplyMsg</tt>.
Your
solution is complete when it passes all of the Lab 3 tests.

<p class="note">
The <tt>maxraftstate</tt> limit applies to the GOB-encoded
bytes your Raft passes to <tt>persister.SaveRaftState()</tt>.

<ul class="hints">

<li>
You should send the entire snapshot in a single InstallSnapshot RPC.
You do not have to implement Figure 13's <tt>offset</tt> mechanism for
splitting up the snapshot.

<li>
Make sure you pass <tt>TestSnapshotRPC</tt> before
moving on to the other Snapshot tests.

<li>
A reasonable amount of time to take for the Lab 3 tests is 400 seconds
of real time and 700 seconds of CPU time. Further,
<tt>go test -run TestSnapshotSize</tt> should take less than 20
seconds of real time.

</ul>

<p>
Your code should pass the 3B tests (as in the example here) as well
as the 3A tests.
<pre>
$ go test -run 3B
Test: InstallSnapshot RPC (3B) ...
  ... Passed --   1.5  3   163   63
Test: snapshot size is reasonable (3B) ...
  ... Passed --   0.4  3  2407  800
Test: restarts, snapshots, one client (3B) ...
  ... Passed --  19.2  5 123372 24718
Test: restarts, snapshots, many clients (3B) ...
  ... Passed --  18.9  5 127387 58305
Test: unreliable net, snapshots, many clients (3B) ...
  ... Passed --  16.3  5  4485 1053
Test: unreliable net, restarts, snapshots, many clients (3B) ...
  ... Passed --  20.7  5  4802 1005
Test: unreliable net, restarts, partitions, snapshots, many clients (3B) ...
  ... Passed --  27.1  5  3281  535
Test: unreliable net, restarts, partitions, snapshots, many clients, linearizability checks (3B) ...
  ... Passed --  25.0  7 11344  748

PASS
ok      kvraft  129.114s
</pre>

<h3>Handin procedure for lab 3B</h3>

<div class="important">
<p>
Before submitting, please run <em>all</em> the tests
one final time, including the Raft tests.

<pre>$ go test</pre>
</div>

<p>
Submit your code via the class's submission website, located at
<a href="https://6824.scripts.mit.edu/2020/handin.py/">https://6824.scripts.mit.edu/2020/handin.py/</a>.

<p>
You may use your MIT Certificate or request an API key via
email to log in for the first time. Your API key (<tt>XXX</tt>)
is displayed once you logged in, which can be used to upload
the lab from the console as follows.

<pre>
$ cd "$GOPATH"
$ echo "XXX" &gt; api.key
$ make lab3b</pre>

<p class="important">
Check the submission website to make sure it sees your submission!

<p class="note">
You may submit multiple times. We will use the timestamp of
your <strong>last</strong> submission for the purpose of
calculating late days. Your grade is determined by the score
your solution <strong>reliably</strong> achieves when we run
the tester on our test machines.

<hr>

<address>
Please post questions on <a href="http://piazza.com">Piazza</a>.
</address>

</body>
</html>
<!--  LocalWords:  RPCs viewservice src pbservice cd view's PingInterval ack pb
-->
<!--  LocalWords:  DeadPings Viewservice ViewServer PingArgs Viewnum viewservice
-->
<!--  LocalWords:  Handin gzipped czvf whoami tgz GOPATH TestBasicFail GetReply
-->
<!--  LocalWords:  TestFailPut TestConcurrentSame TestPartition PutReply Raft
-->
<!--  LocalWords:  instance's Viewstamped al paxos TestBasic seq Go's
-->
<!--  LocalWords:  ndecided TestForget px int bool ok Min Raft's piggyback un
-->
<!--  LocalWords:  struct Op IDs pm structs marshall unmarshall class's
-->
<!--  LocalWords:  StartServer website API
-->
==========
6.824 2018 Lecture 10: Distributed Transactions

Topics:
  distributed transactions = concurrency control + atomic commit

what's the problem?
  lots of data records, sharded on multiple servers, lots of clients
  [diagram: clients, servers, data sharded by key]
  client application actions often involve multiple reads and writes
    bank transfer: debit and credit
    vote on an article: check if already voted, record vote, increment count
    install bi-directional links in a social graph
  we'd like to hide interleaving and failure from application writers
  this is a traditional database concern
    today's material originated with [distributed] databases
    but the ideas are used in many distributed systems

example situation
  x and y are bank balances -- records in database tables
  x and y are on different servers (maybe at different banks)
  x and y start out as $10
  client C1 is doing a transfer of $1 from x to y
  client C2 is doing an audit, to check that no money has been lost
  C1:             C2:
  add(x, 1)       tmp1 = get(x)
  add(y, -1)      tmp2 = get(y)
                  print tmp1, tmp2

what do we hope for?
  x=11
  y=9
  C2 prints 10,10 or 11,9

what can go wrong?
  unhappy interleaving of C1 and C2's operations
    e.g. C2 executes entirely between C1's two operations, printing 11,10
  server or network failure
  account x or y doesn't exist

the traditional plan: transactions
  client tells the transaction system the start and end of each transaction
  system arranges that each transaction is:
    atomic: all writes occur, or none, even if failures
    serializable: results are as if transactions executed one by one
    durable: committed writes survive crash and restart
  these are the "ACID" properties
  applications rely on these properties!
  we are interested in *distributed* transactions
    data sharded over multiple servers

the application code for our example might look like this:
  T1:
    begin_transaction()
    add(x, 1)
    add(y, -1)
    end_transaction()
  T2:
    begin_transaction()
    tmp1 = get(x)
    tmp2 = get(y)
    print tmp1, tmp2
    end_transaction()

a transaction can "abort" if something goes wrong
  an abort un-does any record modifications
  the transaction might voluntarily abort, e.g. if the account doesn't exist
  the system may force an abort, e.g. to break a locking deadlock
  some servers failures result in abort
  the application might (or might not) try the transaction again

distributed transactions have two big components:
  concurrency control
  atomic commit

first, concurrency control
  correct execution of concurrent transactions

The traditional transaction correctness definition is "serializability"
  you execute some concurrent transactions, which yield results
    "results" means new record values, and output
  the results are serializable if:
    there exists a serial execution order of the transactions
    that yields the same results as the actual execution
  (serial means one at a time -- no parallel execution)
  (this definition should remind you of linearizability)

You can test whether an execution's result is serializable by
  looking for an order that yields the same results.
  for our example, the possible serial orders are
    T1; T2
    T2; T1
  so the correct (serializable) results are:
    T1; T2 : x=11 y=9 "11,9"
    T2; T1 : x=11 y=9 "10,10"
  the results for the two differ; either is OK
  no other result is OK
  the implementation might have executed T1 and T2 in parallel
    but it must still yield results as if in a serial order

what if T1's operations run entirely between T2's two get()s?
  would the result be serializable?
  T2 would print 10,9
  but 10,9 is not one of the two serializable results!
what if T2 runs entirely between T1's two adds()s?
  T2 would print 11,10
  but 11,10 is not one of the two serializable results!

Serializability is good for programmers
  It lets them ignore concurrency

two classes of concurrency control for transactions:
  pessimistic:
    lock records before use
    conflicts cause delays (waiting for locks)
  optimistic:
    use records without locking
    commit checks if reads/writes were serializable
    conflict causes abort+retry, but faster than locking if no conflicts
    called Optimistic Concurrency Control (OCC)

today: pessimistic concurrency control
next week: optimistic concurrency control

"Two-phase locking" is one way to implement serializability
  2PL definition:
    a transaction must acquire a record's lock before using it
    a transaction must hold its locks until *after* commit or abort 

2PL for our example
  suppose T1 and T2 start at the same time
  the transaction system automatically acquires locks as needed
  so first of T1/T2 to use x will get the lock
  the other waits
  this prohibits the non-serializable interleavings

details:
  each database record has a lock
  if distributed, the lock is typically stored at the record's server
    [diagram: clients, servers, records, locks]
    (but two-phase locking isn't affected much by distribution)
  an executing transaction acquires locks as needed, at the first use
    add() and get() implicitly acquires record's lock
    end_transaction() releases all locks
  all locks are exclusive (for this discussion, no reader/writer locks)
  the full name is "strong strict two-phase locking"
  related to thread locking (e.g. Go's Mutex), but easier:
    explicit begin/end_transaction
    DB understands what's being locked (records)
    possibility of abort (e.g. to cure deadlock)

Why hold locks until after commit/abort?
  why not release as soon as done with the record?
  example of a resulting problem:
    suppose T2 releases x's lock after get(x)
    T1 could then execute between T2's get()s
    T2 would print 10,9
    oops: that is not a serializable execution: neither T1;T2 nor T2;T1
  example of a resulting problem:
    suppose T1 writes x, then releases x's lock
    T2 reads x and prints
    T1 then aborts
    oops: T2 used a value that never really existed
    we should have aborted T2, which would be a "cascading abort"; awkward

Could 2PL ever forbid a correct (serializable) execution?
  yes; example:
    T1        T2
    get(x)  
              get(x)
              put(x,2)
    put(x,1) 
  locking would forbid this interleaving
  but the result (x=1) is serializable (same as T2;T1)

Locking can produce deadlock, e.g.
  T1      T2
  get(x)  get(y)
  get(y)  get(x)
The system must detect (cycles? lock timeout?) and abort one of the transactions

The Question: describe a situation where Two-Phase Locking yields
higher performance than Simple Locking. Simple locking: lock *every*
record before *any* use; release after abort/commit. 

Next topic: distributed transactions versus failures

how can distributed transactions cope with failures?
  suppose, for our example, x and y are on different "worker" servers
  suppose x's server adds 1, but y's crashes before subtracting?
  or x's server adds 1, but y's realizes the account doesn't exist?
  or x and y both do their part, but aren't sure if the other did?

We want "atomic commit":
  A bunch of computers are cooperating on some task
  Each computer has a different role
  Want to ensure atomicity: all execute, or none execute
  Challenges: failures, performance

We're going to develop a protocol called "two-phase commit"
  Used by distributed databases for multi-server transactions
  We'll assume the database is *also* locking

Two-phase commit without failures:
  the transaction is driven from the Transaction Coordinator
  [time diagram: TC, A, B]
  TC sends put(), get(), &c RPCs to A, B
    The modifications are tentative, only to be installed if commit.
  TC sees transaction_end()
  TC sends PREPARE messages to A and B.
  If A (or B) is willing to commit,
    respond YES.
    then A/B in "prepared" state.
  otherwise, respond NO.
  If both say YES, TC sends COMMIT messages.
  If either says NO, TC sends ABORT messages.
  A/B commit if they get a COMMIT message.
    I.e. they write tentative records to the real DB.
    And release the transaction's locks on their records.

Why is this correct so far?
  Neither A or B can commit unless they both agreed.

What if B crashes and restarts?
  If B sent YES before crash, B must remember!
  Because A might have received a COMMIT and committed.
  So B must be able to commit (or not) even after a reboot.

Thus subordinates must write persistent (on-disk) state:
  B must remember on disk before saying YES, including modified data.
  If B reboots, disk says YES but no COMMIT, B must ask TC, or wait for TC to re-send.
  And meanwhile, B must continue to hold the transaction's locks.
  If TC says COMMIT, B copies modified data to real data.

What if TC crashes and restarts?
  If TC might have sent COMMIT before crash, TC must remember!
    Since one worker may already have committed.
  And repeat that if anyone asks (i.e. if A/B didn't get msg).
  Thus TC must write COMMIT to disk before sending COMMIT msgs.

What if TC never gets a YES/NO from B?
  Perhaps B crashed and didn't recover; perhaps network is broken.
  TC can time out, and abort (since has not sent any COMMIT msgs).
  Good: allows servers to release locks.

What if B times out or crashes while waiting for PREPARE from TC?
  B has not yet responded to PREPARE, so TC can't have decided commit
  so B can unilaterally abort, and release locks
  respond NO to future PREPARE

What if B replied YES to PREPARE, but doesn't receive COMMIT or ABORT?
  Can B unilaterally decide to abort?
    No! TC might have gotten YES from both,
    and sent out COMMIT to A, but crashed before sending to B.
    So then A would commit and B would abort: incorrect.
  B can't unilaterally commit, either:
    A might have voted NO.

So: if B voted YES, it must "block": wait for TC decision.
  
Two-phase commit perspective
  Used in sharded DBs when a transaction uses data on multiple shards
  But it has a bad reputation:
    slow: multiple rounds of messages
    slow: disk writes
    locks are held over the prepare/commit exchanges; blocks other xactions
    TC crash can cause indefinite blocking, with locks held
  Thus usually used only in a single small domain
    E.g. not between banks, not between airlines, not over wide area
  Faster distributed transactions are an active research area:
    Lower message and persistence cost
    Special cases that can be handled with less work
    Wide-area transactions
    Less consistency, more burden on applications

Raft and two-phase commit solve different problems!
  Use Raft to get high availability by replicating
    i.e. to be able to operate when some servers are crashed
    the servers all do the *same* thing
  Use 2PC when each subordinate does something different
    And *all* of them must do their part
  2PC does not help availability
    since all servers must be up to get anything done
  Raft does not ensure that all servers do something
    since only a majority have to be alive

What if you want high availability *and* atomic commit?
  Here's one plan.
  [diagram]
  Each "server" should be a Raft-replicated service
  And the TC should be Raft-replicated
  Run two-phase commit among the replicated services
  Then you can tolerate failures and still make progress
  You'll build something like this to transfer shards in Lab 4
  Next meeting's FaRM has a different approach
==========
6.824 2017 Lecture 10: FaRM

course note: final project proposals due this Friday!
  can do final project or lab 4
  form group of 2-3 students
  please talk to us/email us about project ideas!

why are we reading this paper?
  many people want distributed transactions
  but they are thought to be slow
  this paper suggests that needn't be true -- very surprising performance!

big performance picture
  90 million *replicated* *persistent* *transactions* per second (Figure 7)
    1 million transactions/second per machine
    each with a few messages, for replication and commit
    very impressive
  a few other systems get 1 million ops/second per machine, e.g. memcached
    but not transactions + replicated + persistent (often not any of these!)
  perspective on 90 million:
    10,000 Tweets per second
    2,000,000 e-mails per second

how do they get high performance?
  data must fit in total RAM (so no disk reads)
  non-volatile RAM (so no disk writes)
  one-sided RDMA (fast cross-network access to RAM)
  fast user-level access to NIC
  transaction+replication protocol that exploits one-sided RDMA

NVRAM
  FaRM writes go to RAM, not disk -- eliminates a huge bottleneck
  can write RAM in 200 ns, but takes 10 ms to write hard drive, 100 us for SSD
    ns = nanosecond, ms = millisecond, us = microsecond
  but RAM loses content in power failure! not persistent by itself.
  why not just write to RAM of f+1 machines, to tolerate f failures?
    might be enough if failures were always independent
    but power failure is not independent -- may strike 100% of machines!
  so:
    batteries in every rack, can run machines for a few minutes
    power h/w notifies s/w when main power fails
    s/w halts all transaction processing
    s/w writes FaRM's RAM to SSD; may take a few minutes
    then machine shuts down
    on re-start, FaRM reads saved memory image from SSD
    "non-volatile RAM"
  what if crash prevents s/w from writing SSD?
    e.g bug in FaRM or kernel, or cpu/memory/hardware error
    FaRM copes with single-machine crashes by copying data
      from RAM of machines' replicas to other machines
      to ensure always f+1 copies
    crashes (other than power failure) must be independent!

why is the network often a performance bottleneck?
  the usual setup:
    app                       app
    ---                       ---
    socket buffers            buffers
    TCP                       TCP
    NIC driver                driver
    NIC  -------------------- NIC
  lots of expensive CPU operations:
    system calls
    copy messages
    interrupts
    and all twice if RPC
  slow:
    hard to build RPC than can deliver more than a few 100,000 / second
    wire b/w (e.g. 10 gigabits/second) is rarely the limit for short RPC
    these per-packet CPU costs are the limiting factor for small messages

Kernel bypass
  application access to NIC h/w is streamlined
  application directly interacts with NIC -- no system calls, no kernel
  shared memory mapping between app and NIC
  sender gives NIC an RDMA command
  for RPC, receiver s/w polls memory which RDMA writes

FaRM's network setup
  [hosts, 56 gbit NICs, expensive switch]
  NIC does "one-sided RDMA": memory read/write, not packet delivery
  sender says "write this data at this address", or "read this address"
    NIC *hardware* executes at the far end
    returns a "hardware acknowledgement"
  no interrupt, kernel, copy, read(), &c at the far end
  one server's throughput: 10+ million/second (Figure 2)
  latency: 5 microseconds (from their NSDI 2014 paper)
  FaRM uses RDMA in three ways:
    one-sided read of objects during transaction execution (also VALIDATE)
    RPC composed of one-sided writes to primary's logs or message queues
    one-sided write into backup's log

big challenge:
  how to use one-sided read/write for transactions and replication?
  protocols we've seen require receiver CPU to actively process messages
    e.g. Raft and two-phase-commit

let's review distributed transactions

remember this example:
  x and y are bank balances, maybe on different servers
  T1:             T2:
    add(x, 1)       tmp1 = get(x)
    add(y, -1)      tmp2 = get(y)
                    print tmp1, tmp2
  x and y start at $10
  we want serializability:
    results should be as if transactions ran one at a time in some order
  only two orders are possible
    T1 then T2 yields 11, 9
    T2 then T1 yields 10, 10
    serializability allows no other result

what if T1 runs entirely between T2's two get()s?
  would print 10,9 if the transaction protocol allowed it
  but it's not allowed!
what if T2 runs entirely between T1's two adds()s?
  would print 11,10 if the transaction protocol allowed it
  but it's not allowed!

two classes of concurrency control for transactions:
  pessimistic:
    wait for lock on first use of object; hold until commit/abort
    called two-phase locking
    conflicts cause delays
  optimistic:
    access object without locking; commit "validates" to see if OK
      valid: do the writes
      invalid: abort
    called Optimistic Concurrency Control (OCC)

FaRM uses OCC
  the reason: OCC lets FaRM read using one-sided RDMA reads
    server storing the object does not need to set a lock, due to OCC
  how does FaRM validate? we'll look at Figure 4 in a minute.

every FaRM server runs application transactions and stores objects
  an application transaction is its own transaction coordinator (TC)

FaRM transaction API (simplified):
  txCreate()
  o = txRead(oid)  -- RDMA
  o.f += 1
  txWrite(oid, o)  -- purely local
  ok = txCommit()  -- Figure 4

txRead
  one-sided RDMA to fetch object direct from primary's memory -- fast!
  also fetches object's version number, to detect concurrent writes

txWrite
  must be preceded by txRead
  just writes local copy; no communication

what's in an oid?
  <region #, address>
  region # indexes a mapping to [ primary, backup1, ... ]
  target NIC can use address directly to read or write RAM
    so target CPU doesn't have to be involved

server memory layout
  regions, each an array of objects
  object layout
    header with version # and lock
  for each other server
    (written by RDMA, read by polling)
    incoming log
    incoming message queue
  all this in non-volatile RAM (i.e. written to SSD on power failure)

every region replicated on one primary, f backups -- f+1 replicas
  [diagram of a few regions, primary/backup]
  only the primary serves reads; all f+1 see commits+writes
  replication yields availability if <= f failures
    i.e. available as long as one replica stays alive; better than Raft

transaction execution / commit protocol w/o failure -- Figure 4
  let's consider steps in Figure 4 one by one
  thinking about concurrency control for now (not replication)

LOCK (first message in commit protocol)
  TC sends to primary of each written object
  TC uses RDMA to append to its log at each primary
  LOCK record contains oid, version # xaction read, new value
  primary s/w polls log, sees LOCK, validates, sends "yes" or "no" reply message
  note LOCK is both logged in primary's NVRAM *and* an RPC exchange

what does primary CPU do on receipt of LOCK?
  (for each object)
  if object locked, or version != what xaction read, reply "no"
    implemented with atomic compare-and-swap
    "locked" flag is high-order bit in version number
  otherwise set the lock flag and return "yes"
  note: does *not* block if object is already locked

TC waits for all LOCK reply messages
  if any "no", abort
    send ABORT to primaries so they can release locks
    returns "no" from txCommit()

let's ignore VALIDATE and COMMIT BACKUP for now

TC sends COMMIT-PRIMARY to primary of each written object
  uses RDMA to append to primary's log
  TC only waits for hardware ack -- does not wait for primary to process log entry
  TC returns "yes" from txCommit()

what does primary do when it processes the COMMIT-PRIMARY in its log?
  copy new value over object's memory
  increment object's version #
  clear object's lock flag

example:
  T1 and T2 both want to increment x
  both say
    tmp = txRead(x)
    tmp += 1
    txWrite(x)
    ok = txCommit()
  x should end up with 0, 1, or 2, consistent with how many successfully committed

what if T1 and T2 are exactly in step?
  T1: Rx0  Lx  Cx
  T2: Rx0  Lx  Cx
  what will happen?

or
  T1:    Rx0 Lx Cx
  T2: Rx0          Lx  Cx

or
  T1: Rx0  Lx  Cx
  T2:             Rx0  Lx  Cx

intuition for why validation checks serializability:
  i.e. checks "was execution one at a time?"
  if no conflict, versions don't change, and commit is allowed
  if conflict, one will see lock or changed version #

what about VALIDATE in Figure 4?
  it is an optimization for objects that are just read by a transaction
  VALIDATE = one-sided RDMA read to fetch object's version # and lock flag
  if lock set, or version # changed since read, TC aborts
  does not set the lock, thus faster than LOCK+COMMIT

VALIDATE example:
x and y initially zero
T1:
  if x == 0:
    y = 1
T2:
  if y == 0:
    x = 1
(this is a classic test example for consistency)
T1,T2 yields y=1,x=0
T2,T1 yields x=1,y=0
aborts could leave x=0,y=0
but serializability forbids x=1,y=1

suppose simultaneous:
  T1:  Rx  Ly  Vx  Cy
  T2:  Ry  Lx  Vy  Cx
  the LOCKs will both succeed
  the VALIDATEs will both fail, since lock bits are both set
  so both will abort -- which is OK

how about:
  T1:  Rx  Ly  Vx      Cy
  T2:  Ry          Lx  Vy  Cx
  then T1 commits, T2 still aborts since T2's Vy sees T1's lock or higher version
but we can't have *both* V's before the other L's
so VALIDATE seems correct in this example
  and fast: faster than LOCK, no COMMIT required

what about fault tolerance?
  defense against losing data?
    durable? available?
  integrity of underway transactions despite crashes?
  partitions?

high-level replication diagram
  o o region 1
  o o region 2
  o CM
  o o o ZK

f+1 copies of each region to tolerate <= f failures in each region
  TCs send all writes to all copies (TC's COMMIT-BACKUP)
  not immediately available if a server crashes
    transaction reads and commits will wait
  but CM will soon notice, make a new copy, recover transactions

reconfiguration
  one ZooKeeper cluster (a handful of replicas)
    stores just configuration #, set of servers in this config, and CM
    breaks ties if multiple servers try to become CM
    chooses the active partition if partitioned (majority partition)
  a Configuration Manager (CM) (not replicated)
    monitors liveness of all servers via rapid ping
    manages reconfiguration
      renews leases
        only activates if it gets a response from majority of machines
      checks that at least one copy of each region exists
      assigns regions to primary/backup sets
      tells servers to make new copies
      manages completion of interrupted transactions

let's look back the at the Figure 4 commit protocol to see how
  any xaction that might have committed will be visible despite failed servers.
  "might have committed":
    TC might have replied "yes" to client
    primary might have revealed update to a subsequent read

after TC sees "yes" from all LOCKs and VALIDATEs,
  TC appends COMMIT-BACKUP to each backup's log
  after all ack, appends COMMIT-PRIMARY to each primary's log
  after one ack, reports "commit" to application

note TC replicates to backups; primaries don't replicate
  COMMIT-BACKUP contains written value, enough to update backup's state

why TC sends COMMIT-PRIMARY only after acks for *all* COMMIT-BACKUPs?
  a primary may execute as soon as it sees COMMIT-PRIMARY
    and show the update to other transactions
  so by that point each object's new value must in f+1 logs (per region)
    so f can fail without losing the new value
  if there's even one backup that doesn't have the COMMIT-BACKUP
    that object's writes are in only f logs
    all f could fail along with TC
    then we'd have exposed commit but maybe permanently lost one write!

why TC waits for an ack from a COMMIT-PRIMARY?
  so that there is a complete f+1 region that's aware of the commit
  before then, only f backups per region knew (from COMMIT-BACKUPs)
  but we're assuming up to f per region to fail

the basic line of reasoning for why recovery is possible:
  if TC could have reported "commit", or a primary could have exposed value,
  then all f+1 in each region have LOCK or COMMIT-BACKUP in log,
  so f can fail from any/every region without losing writes.
  if recovery sees one or more COMMIT-*, and a COMMIT-* or LOCK
    from each region, it commits; otherwise aborts.
    i.e. evidence TC decided commit, plus each object's writes.
    (Section 5.3, Step 7)

FaRM is very impressive; does it fall short of perfection?
  * works best if few conflicts, due to OCC.
  * data must fit in total RAM.
  * the data model is low-level; would need e.g. SQL library.
  * details driven by specific NIC features; what if NIC had test-and-set?

summary
  distributed transactions have been viewed as too slow for serious use
  maybe FaRM demonstrates that needn't be true
==========
6.824 2018 Lecture 15: Frangipani

Frangipani: A Scalable Distributed File System
Thekkath, Mann, Lee
SOSP 1997

why are we reading this paper?
  performance via caching
  cache coherence
  decentralized design

what's the overall design?
  a network file system
    works transparently with existing apps (text editors &c)
    much like Athena's AFS
  users; workstations + Frangipani; network; petal
  Petal: block storage service; replicated; striped+sharded for performance
  What's in Petal?
    directories, i-nodes, file content blocks, free bitmaps
    just like an ordinary hard disk file system
  Frangipani: decentralized file service; cache for performance

what's the intended use?
  environment: single lab with collaborating engineers
    == the authors' research lab
    programming, text processing, e-mail, &c
  workstations in offices
  most file access is to user's own files
  need to potentially share any file among any workstations
    user/user collaboration
    one user logging into multiple workstations
  so:
    common case is exclusive access; want that to be fast
    but files sometimes need to be shared; want that to be correct
  this was a common scenario when the paper was written

why is Frangipani's design good for the intended use?
  it caches aggressively in each workstation, for speed
  cache is write-back
    allows updates to cached files/directories without network traffic
  all operations entirely local to workstation -- fast
    including e.g. creating files, creating directories, rename, &c
    updates proceed without any RPCs if everything already cached
    so file system code must reside in the workstation, not server
    "decentralized"
  cache also helps for scalability (many workstations)
    servers were a serious bottleneck in previous systems

what's in the Frangipani workstation cache?
  what if WS1 wants to create and write /u/rtm/grades?
  read /u/rtm information from Petal into WS1's cache
  add entry for "grades" just in the cache
  don't immediately write back to Petal!
    in case WS1 wants to do more modifications

challenges
  WS2 runs "ls /u/rtm" or "cat /u/rtm/grades"
    will WS2 see WS1's write?
    write-back cache, so WS'1 writes aren' in Petal
    caches make stale reads a serious threat
    "coherence"
  WS1 and WS2 concurrently try to create tmp/a and tmp/b
    will they overwrite each others' changes?
    there's no central file server to sort this out!
    "atomicity"
  WS1 crashes while renaming
    but other workstations are still operating
    how to ensure no-one sees the mess? how to clean up?
    "crash recovery"

"cache coherence" solves the "read sees write" problem
  the goal is linearizability AND caching
  there are lots of "coherence protocols"
  a common pattern: file servers, distributed shared memory, multi-core

Frangipani's coherence protocol (simplified):
  lock server (LS), with one lock per file/directory
    owner(lock) = WS, or nil
  workstation (WS) Frangipani cache:
    cached files and directories: present, or not present
    cached locks: locked-busy, locked-idle, unlocked
  workstation rules:
    acquire, then read from Petal
    write to Petal, then release
    don't cache unless you hold the lock
  coherence protocol messages:
    request  (WS -> LS)
    grant (LS -> WS)
    revoke (LS -> WS)
    release (WS -> LS)

the locks are named by files/directories (really i-numbers),
though the lock server doesn't actually understand anything
about file systems or Petal.

example: WS1 changes directory /u/rtm, then WS2 reads it

WS1                      LS            WS2
read /u/rtm
  --request(/u/rtm)-->
                         owner(/u/rtm)=WS1
  <--grant(/u/rtm)---
(read+cache /u/rtm data from Petal)
(create /u/rtm/grades locally)
(when done, cached lock in locked-idle state)
                                       read /u/rtm
                          <--request(/u/rtm)--
   <--revoke(/u/rtm)--
(write modified /u/rtm to Petal)
   --release(/u/rtm)-->
                         owner(/u/rtm)=WS2
                           --grant(/u/rtm)-->
                                       (read /u/rtm from Petal)

the point:
  locks and rules force reads to see last write
  locks ensure that "last write" is well-defined

coherence optimizations
  the "locked-idle" state is already an optimization
  Frangipani has shared read locks, as well as exclusive write locks
  you could imagine WS-to-WS communication, rather than via LS and Petal

next challenge: atomicity
  what if two workstations try to create the same file at the same time?
  are partially complete multi-write operations visible?
    e.g. file create initializes i-node, adds directory entry
    e.g. rename (both names visible? neither?)

Frangipani has transactions:
  WS acquires locks on all file system data that it will modify
  performs modifications with all locks held
  only releases when finished
  thus no other WS can see partially-completed operations
    and no other WS can race to perform updates (e.g. file creation)

note Frangipani's locks are doing two different things:
  cache coherence
  atomic transactions

next challenge: crash recovery

What if a Frangipani workstation dies while holding locks?
  other workstations will want to continue operating...
  can we just revoke dead WS's locks?
  what if dead WS had modified data in its cache?
  what if dead WS had started to write back modified data to Petal?
    e.g. WS wrote new directory entry to Petal, but not initialized i-node
    this is the troubling case

Is it OK to just wait until a crashed workstation reboots?

Frangipani uses write-ahead logging for crash recovery
  So if a crashed workstation has done some Petal writes for an operation,
    but not all, the writes can be completed from the log
  Very traditional -- but...
  1) Frangipani has a separate log for each workstation
     rather than the traditional log per shard of the data
     this avoids a logging bottleneck, eases decentralization
     but scatters updates to a given file over many logs
  2) Frangipani's logs are in shared Petal storage
     WS2 may read WS1's log to recover from WS1 crashing
  Separate logs is an interesting and unusual arrangement

What's in the log?
  log entry:
    (this is a bit of guess-work, paper isn't explicit)
    log sequence number
    array of updates:
      block #, new version #, offset, new bytes
    just contains meta-data updates, not file content updates
  example -- create file d/f produces a log entry:
    a two-entry update array:
      add an "f" entry to d's content block, with new i-number
      initialize the i-node for f
  initially the log entry is in WS local memory (not yet Petal)

When WS gets lock revocation on modified directory from LS:
  1) force its entire log to Petal, then
  2) send the cached updated blocks to Petal, then
  3) release the locks to the LS

Why must WS write log to Petal before updating
  i-node and directory &c in Petal?

Why delay writing the log until LS revokes locks?

What happens when WS1 crashes while holding locks?
  Not much, until WS2 requests a lock that WS1 holds
    LS sends grant to WS1, gets no response
    LS times out, tells WS2 to recover WS1 from its log in Petal
  What does WS2 do to recover from WS1's log?
    Read WS1's log from Petal
    Perform Petal writes described by logged operation
    Tell LS it is done, so LS can release WS1's locks

Note it's crucal that each WS log is in Petal so that it can
  be read by any WS for recovery.

What if WS1 crashes before it even writes recent operations to the log?
  WS1's recent operations may be totally lost if WS1 crashes.
  But the file system will be internally consistent.

Why is it safe to replay just one log, despite interleaved
  operations on same files by other workstations?
Example:
  WS1: delete(d/f)               crash
  WS2:               create(d/f)
  WS3 is recovering WS1's log -- but it doesn't look at WS2's log
  Will recovery re-play the delete? 
    This is The Question
    No -- prevented by "version number" mechanism
    Version number in each meta-data block (i-node) in Petal
    Version number(s) in each logged op is block's version plus one
    Recovery replays only if op's version > block version
      i.e. only if the block hasn't yet been updated by this op
  Does WS3 need to aquire the d or d/f lock?
    No: if version number same as before operation, WS1 couldn't
        have released the lock, so safe to update in Petal

Why is it OK that the log doesn't hold file *content*?
  If a WS crashes before writing content to Petal, it will be lost.
  Frangipani recovery defends the file system's own data structures.
  Applications can use fsync() to do their own recoverably content writes.
  It would be too expensive for Frangipani to log content writes.
  Most disk file systems (e.g. Linux) are similar, so applications
    already know how to cope with loss of writes before crash.

What if:
  WS1 holds a lock
  Network partition
  WS2 decides WS1 is dead, recovers, releases WS1's locks
  But WS1 is alive and subsequently writes data covered by the lock
  Locks have leases!
    Lock owner can't use a lock past its lease period
    LS doesn't start recovery until after lease expires

Is Paxos (== Raft) hidden somewhere here?
  Yes -- choice of lock server, choice of Petal primary/backup
  ensures a single lock server, despite partition
  ensures a single primary for each Petal shard

Performance?
  hard to judge numbers from 1997
  do they hit hardware limits? disk b/w, net b/w
  do they scale well with more hardware?
  what scenarios might we care about?
    read/write lots of little files (e.g. reading my e-mail)
    read/write huge files

Small file performance -- Figure 5
  X axis is number of active workstations
    each workstation runs a file-intensive benchmark
    workstations use different files and directories
  Y axis is completion time for a single workstation
  flat implies good scaling == no significant shared bottleneck
  presumably each workstation is just using its own cache
  possibly Petal's many disks also yield parallel performance

Big file performance
  each disk: 6 MB / sec
    Petal stripes to get more than that
  7 Petal servers, 9 disks per Petal server
    336 MB/s raw disk b/w, but only 100 MB/s via Petal
  a single Frangipani workstation, Table 3
    write: 15 MB/s -- limited by network link
    read: 10 MB/s -- limited by weak pre-fetch (?), could be 15
  lots of Frangipani workstations
    Figure 6 -- reads scale well with more machines
    Figure 7 -- writes hit hardware limits of Petal (2x for replication)

For what workloads is Frangipani likely to have poor performance?
  files bigger than cache?
  lots of read/write sharing?
  caching requires a reasonable working set size
  whole-file locking a poor fit for e.g. distributed DB
  coherence is too good for e.g. web site back-end

Petal details
  Petal provides Frangipani w/ fault-tolerant storage
    so it's worth discussing
  block read/write interface
    compatible with existing file systems
  looks like single huge disk, but many servers and many many disks
    big, high performance
    striped, 64-KB blocks
  virtual: 64-bit sparse address space, allocate on write
    address translation map
  primary/backup (one backup server)
    primary sends each write to the backup
  uses Paxos to agree on primary for each virt addr range
  what about recovery after crash?
    suppose pair is S1+S2
    S1 fails, S2 is now sole server
    S1 restarts, but has missed lots of updates
    S2 remembers a list of every block it wrote!
    so S1 only has to read those blocks, not entire disk
  logging
    virt->phys map and missed-write info

Limitations
  Most useful for e.g. programmer workstations, not so much otherwise
  Frangipani enforces permissions, so workstations must be trusted
    so Athena couldn't run Frangipani on Athena workstations
  Frangipani/Petal split is a little awkward
    both layers log
    Petal may accept updates from "down" Frangipani workstations
    more RPC messages than a simple file server
  A file system is not a great API for many applications, e.g. web site

Ideas to remember
  client-side caching for performance
  cache coherence protocols
  decentralized complex service on simple shared storage layer
  per-client log for decentralized recovery
==========
6.824 2018 Lecture 12: Spark Case Study

Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing
Zaharia, Chowdhury, Das, Dave, Ma, McCauley, Franklin, Shenker, Stoica
NSDI 2012

Today: more distributed computations
  Case study: Spark
  Why are we reading Spark?
    Widely-used for datacenter computations
    popular open-source project, hot startup (Databricks)
    Support iterative applications better than MapReduce
    Interesting fault tolerance story
    ACM doctoral thesis award

MapReduce make life of programmers easy
  It handles:
    Communication between nodes
    Distribute code
    Schedule work
    Handle failures
  But restricted programming model
    Some apps don't fit well with MapReduce

Many algorithms are iterative
  Page-rank is the classic example
    compute a rank for each document, based on how many docs point to it
    the rank is used to determine the place of the doc in the search results
  on each iteration, each document:
    sends a contribution of r/n to its neighbors
       where r is its rank and n is its number of neighbors.
    update rank to alpha/N + (1 - alpha)*Sum(c_i),
       where the sum is over the contributions it received
       N is the total number of documents.
  big computation:
    runs over all web pages in the world
    even with many machines it takes long time

MapReduce and iterative algorithms
  MapReduce would be good at one iteration of the algorithms
    Each map does part of the documents
    Reduce for update the rank of a particular doc
  But what to do for the next iteration?
    Write results to storage
    Start a new MapReduce job for the next iteration
    Expensive
    But fault tolerant

Challenges
  Better programming model for iterative computations
  Good fault tolerance story

One solution: use DSM
  Good for iterative programming
    ranks can be in shared memory
    workers can update and read
  Bad for fault tolerance
    typical plan: checkpoint state of memory
      make a checkpoint every hour of memory
    expensive in two ways:
      write shared memory to storage during computation
      redo all work since last checkpoint after failure
  Spark more MapReduce flavor
    Restricted programming model, but more powerful than MapReduce
    Good fault tolerance plan

Better solution: keep data in memory
  Pregel, Dryad, Spark, etc.
  In Spark
    Data is stored in data sets (RDDs)
    "Persist" the RDD in memory
    Next iteration can refer to the RDD

Other opportunities
  Interactive data exploration
    Run queries over the persisted RDDs
  Like to have something SQL-like
    A join operator over RDDs

Core idea in Spark: RDDs
  RDDs are immutable --- you cannot update them
  RDDs support transformations and actions
  Transformations:  compute a new RDD from existing RDDs
    map, reduceByKey, filter, join, ..
    transformations are lazy: don't compute result immediately
    just a description of the computation
  Actions: for when results are needed
    counts result, collect results, get a specific value

Example use:
  lines = spark.textFile("hdfs://...")
  errors = lines.filter(_.startsWith("ERROR"))    // lazy!
  errors.persist()    // no work yet
  errors.count()      // an action that computes a result
  // now errors is materialized in memory
  // partitioned across many nodes
  // Spark, will try to keep in RAM (will spill to disk when RAM is full)

Reuse of an RDD
  errors.filter(_.contains("MySQL")).count()
  // this will be fast because reuses results computed by previous fragment
  // Spark will schedule jobs across machines that hold partition of errors

Another reuse of RDD
  errors.filter(_.contains("HDFS")).map(_.split('\t')(3)).collect()

RDD lineage
  Spark creates a lineage graph on an action
  Graphs describe the computation using transformations
    lines -> filter w ERROR -> errors -> filter w. HDFS -> map -> timed fields
  Spark uses the lineage to schedule job
    Transformation on the same partition form a stage
      Joins, for example, are a stage boundary
      Need to reshuffle data
    A job runs a single stage
      pipeline transformation within a stage
    Schedule job where the RDD partition is

Lineage and fault tolerance
  Great opportunity for *efficient* fault tolerance
    Let's say one machine fails
    Want to recompute *only* its state
    The lineage tells us what to recompute
      Follow the lineage to identify all partitions needed
      Recompute them
  For last example, identify partitions of lines missing
    Trace back from child to parents in lineage
    All dependencies are "narrow"
      each partition is dependent on one parent partition
    Need to read the missing partition of lines
      recompute the transformations

RDD implementation
  list of partitions
  list of (parent RDD, wide/narrow dependency)
    narrow: depends on one parent partition  (e.g., map)
    wide: depends on several parent partitions (e.g., join)
  function to compute (e.g., map, join)
  partitioning scheme (e.g., for file by block)
  computation placement hint

Each transformation takes (one or more) RDDs, and outputs the transformed RDD.

Q: Why does an RDD carry metadata on its partitioning?  A: so transformations
  that depend on multiple RDDs know whether they need to shuffle data (wide
  dependency) or not (narrow). Allows users control over locality and reduces
  shuffles.

Q: Why the distinction between narrow and wide dependencies?  A: In case of
  failure.  Narrow dependency only depends on a few partitions that need to be
  recomputed.  Wide dependency might require an entire RDD

Example: PageRank (from paper):
  // Load graph as an RDD of (URL, outlinks) pairs
  val links = spark.textFile(...).map(...).persist() // (URL, outlinks)
  var ranks = // RDD of (URL, rank) pairs
  for (i <- 1 to ITERATIONS) {
    // Build an RDD of (targetURL, float) pairs
    // with the contributions sent by each page
    val contribs = links.join(ranks).flatMap {
      (url, (links, rank)) => links.map(dest => (dest, rank/links.size))
    }
    // Sum contributions by URL and get new ranks
    ranks = contribs.reduceByKey((x,y) => x+y)
     .mapValues(sum => a/N + (1-a)*sum)
  }

Lineage for PageRank
  See figure 3
  Each iteration creates two new RDDs:
    ranks0, ranks1, etc.
    contribs0, contribs1, etc.
  Long lineage graph!
    Risky for fault tolerance.
    One node fails, much recomputation
  Solution: user can replicate RDD
    Programmer pass "reliable" flag to persist()
     e.g., call ranks.persist(RELIABLE) every N iterations
    Replicates RDD in memory
    With REPLICATE flag, will write to stable storage (HDFS)
  Impact on performance
   if user frequently perist w/REPLICATE, fast recovery, but slower execution
   if infrequently, fast execution but slow recovery

Q: Is persist a transformation or an action?  A: neither. It doesn't create a
 new RDD, and doesn't cause materialization. It's an instruction to the
 scheduler.

Q: By calling persist without flags, is it guaranteed that in case of fault that
  RDD wouldn't have to be recomputed?  A: No. There is no replication, so a node
  holding a partition could fail.  Replication (either in RAM or in stable
  storage) is necessary

Currently only manual checkpointing via calls to persist.  Q: Why implement
  checkpointing? (it's expensive) A: Long lineage could cause large recovery
  time. Or when there are wide dependencies a single failure might require many
  partition re-computations.

Q: Can Spark handle network partitions? A: Nodes that cannot communicate with
  scheduler will appear dead. The part of the network that can be reached from
  scheduler can continue computation, as long as it has enough data to start the
  lineage from (if all replicas of a required partition cannot be reached,
  cluster cannot make progress)

What happens when there isn't enough memory?
  LRU (Least Recently Used) on partitions
     first on non-persisted
     then persisted (but they will be available on disk. makes sure user cannot overbook RAM)
  User can have control on order of eviction via "persistence priority"
  No reason not to discard non-persisted partitions (if they've already been used)

Performance
  Degrades to "almost" MapReduce behavior
  In figure 7, logistic regression on 100 Hadoop nodes takes 76-80 seconds
  In figure 12, logistic regression on 25 Spark nodes (with no partitions allowed in memory)
    takes 68.8
  Difference could be because MapReduce uses replicated storage after reduce, but Spark by
  default only spills to local disk
    no network latency and I/O load on replicas.
  no architectural reason why MR would be slower than Spark for non-iterative work
    or for iterative work that needs to go to disk
  no architectural reason why Spark would ever be slower than MR

Discussion
  Spark targets batch, iterative applications
  Spark can express other models
    MapReduce, Pregel
  Cannot incorporate new data as it comes in
    But see Streaming Spark
  Spark not good for building key/value store
    Like MapReduce, and others
    RDDs are immutable

References
  http://spark.apache.org/
  http://www.cs.princeton.edu/~chazelle/courses/BIB/pagerank.htm
==========
6.824 2018 Lecture 13: Naiad Case Study

Naiad: A Timely Dataflow System
Murray, McSherry, Isaacs, Isard, Barham, Abadi
SOSP 2013

Today: streaming and incremental computation
  Case study: Naiad
  Why are we reading Naiad?
    elegant design
    impressive performance
    open-source and still being developed

recap: Spark improved performance for iterative computations
  i.e., ones where the same data is used again and again
  but what if the input data changes?
    may happen, e.g., because crawler updates pages (new links)
    or because an application appends new record to a log file
  Spark (as in last lecture's paper) will actually have to start over!
    run all PageRank iterations again, even if just one link changed
    (but: Spark Streaming -- similar to Naiad, but atop Spark)

Incremental processing
  input vertices
    could be a file (as in Spark/MR)
    or a stream of events, e.g. web requests, tweets, sensor readings...
    inject (key, value, epoch) records into graph
  fixed data-flow
    records flow into vertices, which may emit zero or more records in response
  stateful vertices (long-lived)
    a bit like cached RDDs
    but mutable: state changes in response to new records arriving at the vertex
    ex: GroupBy with sum() stores one record for every group
      new record updates current aggregation value

Iteration with data-flow cycles
  contrast: Spark has no cycles (DAG), iterates by adding new RDDs
    no RDD can depend on its own child
  good for algorithms with loops
    ex: PageRank sends rank updates back to start of computation
  "root" streaming context
    special, as introduces inputs with epochs (specified by program)
  loop contexts
    can nest arbitrarily
    cannot partially overlap
  [Figure 3 example w/ nested loop]

Problem: ordering required!
  [example: delayed PR rank update, next iteration reads old rank]
  intuition: avoid time-travelling updates
    vertex sees update from past iteration when it already moved on
    vertex emits update from a future iteration it's not at yet
  key ingredient: hierarchical timestamps
    timestamp: (epoch, [c1, ..., ck]) where ci is the ith-level loop context
  ingress, egress, feedback vertices modify timestamps
    Ingress: append new loop counter at end (start into loop)
    Egress: drop loop counter from end (break out of loop)
    Feedback: increment loop counter at end (iterate)
  [Figure 3 timestamp propagation: lecture question]
    epoch 1:
    (a, 2): goes around the loop three times (value 4 -> 8 -> 16)
      A: (1, []), I: (1, [0]), ... F: (1, [1]), ... F: (1, [2]), ... E: (1, [])
    (b, 6): goes around the loop only once (value 6 -> 12)
      A: (1, []), I: (1, [0]), ... F: (1, [1]), ... E: (1, [])
    epoch 2:
    (a, 5): dropped at A due to DISTINCT.
  timestamps form a partial order
    (1, [5, 7]) < (1, [5, 8]) < (1, [6, 2]) < (2, [1, 1])
    but also: (1, [5, 7]) < (1, [5]) -- lexicographic ordering on loop counters
    and, for two top-level loops: (1, lA [5]) doesn't compare to (1, lB [2])
  allows vertices to order updates
    and to decide when it's okay to release them
    so downstream vertex gets a consistent outputs in order

Programmer API
  good news: end users mostly don't worry about the timestamps
    timely dataflow is low-level infrastructure
    allow special-purpose implementations by experts for specific uses
  [Figure 2: runtime, timely dataflow, differential dataflow, other libs]
  high-level APIs provided by libraries built on the timely dataflow abstraction
    e.g.: LINQ (SQL-like), BSP (Pregel-ish), Datalog
    similar trend towards special-purpose libraries in Spark ecosystem
      SparkSQL, GraphX, MLLib, Streaming, ..

Low-level vertex API
  need to explicitly deal with timestamps
  notifications
    "you'll no longer receive records with this timestamp or any earlier one"
    vertex may e.g., decide to compute final value and release
  two callbacks invoked by the runtime on vertex:
    OnRecv(edge, msg, timestamp) -- here are some messages
    OnNotify(timestamp)          -- no more messages <= timestamp coming
  two API calls available to vertex code:
    SendBy(edge, msg, timestamp) -- send a message at current or future time
    NotifyAt(timestamp)          -- call me back when at timestamp
  allows different strategies
    incrementally release records for a time, finish up with notification
    buffer all records for a time, then release on notification
  [code example: Distinct Count vertex]

Progress tracking
  protocol to figure out when to deliver notifications to vertices
  intuition
    ok to deliver notification once it's *impossible* for predecessors to
    generate records with earlier timestamp
  single-threaded example
    "pointstamps": just a timestamp + location (edge or vertex)
    [lattice diagram of vertices/edges on x-axis, times on y-axis]
    arrows indicate could-result-in
      follow partial order on timestamps
      ex: (1, [2]) at B in example C-R-I (1, [3]) but also (1, [])
      => not ok to finish (1, []) until everything has left the loop
    remove active pointstamp once no events for it are left
      i.e., occurrency count (OC) = 0
    frontier: pointstamp without incoming arrows
      keeps moving down the time axis, but speed differs for locations
  distributed version
    strawman 1: send all events to a single coordinator
      slow, as need to wait for this coordinator
      this is the "global frontier" -- coordinator has all information
    strawman 2: process events locally, then inform all other workers
      broadcast!
      workers now maintain "local frontier", which approximates global one
      workers can only be *behind*: progress update may still be in the network
        local frontier can never advance past global frontier
        hence safe, as will only deliver notifications late
      problem: sends enormous amounts of progress chatter!
        ~10 GB for WCC on 60 computers ("None", Figure 6(c))
    solution: aggregate events locally before broadcasting
      each event changes OC by +1 or -1 => can combine
      means we may wait a little longer for an update (always safe)
        only sends updates when set of active pointstamps changes
      global aggregator merges updates from different workers
        like global coordinator in strawman, but far fewer messages

Fault tolerance
  perhaps the weakest part of the paper
  option 1: write globally synchronous, coordinated checkpoints
    recovery loads last checkpoint (incl. timestamps, progress info)
    then starts processing from there, possibly repeating inputs
    induces pause times while making checkpoints (cf. tail latency, Fig 7c)
  option 2: log all messages to disk before sending
    no need to checkpoint
    recovery can resume from any point
    but high common-case overhead (i.e., pay price even when there's no failure)
  Q: why not use a recomputation strategy like Spark's?
  A: difficult to do with fine-grained updates. Up to what point do we recompute?
  cleverer scheme developed after this paper
    some vertices checkpoint, some log messages
    can still recover the joint graph
    see Falkirk Wheel paper -- https://arxiv.org/abs/1503.08877

Performance results
  optimized system from ground up, as illustrated by microbenchmarks (Fig 6)
  impressive PageRank performance (<1s per iteration on twitter_rv graph)
    very good in 2013, still pretty good now!
  Naiad matches or beats specialized systems in several different domains
    iterative batch: PR, SCC, WCC &c vs. DB, DryadLINQ -- >10x improvement
    graph processing: PR on twitter vs. PowerGraph -- ~10x improvement
    iterative ML: logistic regression vs. Vowpal Wabbit -- ~40% improvement

References:
  Differential Dataflow: http://cidrdb.org/cidr2013/Papers/CIDR13_Paper111.pdf
  Rust re-implementations:
    * https://github.com/frankmcsherry/timely-dataflow
    * https://github.com/frankmcsherry/differential-dataflow
==========
6.824 2013 Lecture 7: Spanner

Spanner: Google's Globally-Distributed Datase
Corbett et al, OSDI 2012

why this paper?
  modern, high performance, driven by real-world needs
  sophisticated use of paxos
  tackles consistency + performance (will be a big theme)
  Lab 4 a (hugely) simplified version of Spanner

what are the big ideas?
  shard management w/ paxos replication
  high performance despite synchronous WAN replication
  fast reads by asking only the nearest replica
  consistency despite sharding (this is the real focus)
  clever use of time for consistency
  distributed transactions

this is a dense paper!
  i've tried to boil down some of the ideas to simpler form

idea: sharding
  we've seen this before in FDS
  the real problem is managing configuration changes
  Spanner has a more convincing design for this than FDS

simplified sharding outline (lab 4):
  replica groups, paxos-replicated
    paxos log in each replica group
  master, paxos-replicated
    assigns shards to groups
    numbered configurations
  if master moves a shard, groups eventually see new config
  "start handoff Num=7" op in both groups' paxos logs
    though perhaps not at the same time
  dst can't finish handoff until it has copies of shard data at majority
    and can't wait long for possibly-dead minority
    minority must catch up, so perhaps put shard data in paxos log (!)
  "end handoff Num=7" op in both groups' logs

Q: what if a Put is concurrent w/ handoff?
   client sees new config, sends Put to new group before handoff starts?
   client has stale view and sends it to old group after handoff?
   arrives at either during handoff?

Q: what if a failure during handoff?
   e.g. old group thinks shard is handed off
        but new group fails before it thinks so

Q: can *two* groups think they are serving a shard?

Q: could old group still serve shard if can't hear master?

idea: wide-area synchronous replication
  goal: survive single-site disasters
  goal: replica near customers
  goal: don't lose any updates

considered impractical until a few years ago
  paxos too expensive, so maybe primary/backup?
  if primary waits for ACK from backup
    50ms network will limit throughput and cause palpable delay
    esp if app has to do multiple reads at 50ms each
  if primary does not wait, it will reply to client before durable
  danger of split brain; can't make network reliable

what's changed?
  other site may be only 5 ms away -- San Francisco / Los Angeles
  faster/cheaper WAN
  apps written to tolerate delays
    may make many slow read requests
    but issue them in parallel
    maybe time out quickly and try elsewhere, or redundant gets
  huge # of concurrent clients lets you get hi thruput despite high delay
    run their requests in parallel
  people appreciate paxos more and have streamlined variants
    fewer msgs
      page 9 of paxos paper: 1 round per op w/ leader + bulk preprepare
      paper's scheme a little more involved b/c they must ensure
        there's at most one leader
    read at any replica

actual performance?
  Table 3
    pretend just measuring paxos for writes, read at any replica for reads
    latency
      why doesn't write latency go up w/ more replicas?
      why does std dev of latency go down w/ more replicas?
      r/o a *lot* faster since not a paxos agreement + use closest replica
    throughput
      why does read throughput go up w/ # replicas?
      why doesn't write throughput go up?
      does write thruput seem to be going down?
    what can we conclude from Table 3?
      is the system fast? slow?
    how fast do your paxoses run?
      mine takes 10 ms per agreement
      with purely local communication and no disk
      Spanner paxos might wait for disk write
  Figure 5
    npaxos=5, all leaders in same zone
    why does killing a non-leader in each group have no effect?
    for killing all the leaders ("leader-hard")
      why flat for a few seconds?
      what causes it to start going up?
      why does it take 5 to 10 seconds to recover?
      why is slope *higher* until it rejoins?

spanner reads from any paxos replica
  read does *not* involve a paxos agreement
  just reads the data directly from replica's k/v DB
  maybe 100x faster -- same room rather than cross-country

Q: could we *write* to just one replica?

Q: is reading from any replica correct?

example of problem:
  photo sharing site
  i have photos
  i have an ACL (access control list) saying who can see my photos
  i take my mom out of my ACL, then upload new photo
  really it's web front ends doing these client reads/writes
  1. W1: I write ACL on group G1 (bare majority), then
  2. W2: I add image on G2 (bare majority), then
  3. mom reads image -- may get old data from lagging G2 replica
  4. mom reads ACL -- may get new data from G1

this system is not acting like a single server!
  there was not really any point at which the image was
    present but the ACL hadn't been updated

this problem is caused by a combination of
  * partitioning -- replica groups operate independently
  * cutting corners for performance -- read from any replica

how can we fix this?
  A. make reads see latest data
     e.g. full paxos for reads
     expensive!
  B. make reads see *consistent* data
     data as it existed at *some* previous point in time
     i.e. before #1, between #1 and #2, or after #2
     this turns out to be much cheaper
     spanner does this

here's a super-simplification of spanner's consistency story for r/o clients
  "snapshot" or "lock-free" reads
  assume for now that all the clocks agree
  server (paxos leader) tags each write with the time at which it occurred
  k/v DB stores *multiple* values for each key,
    each with a different time
  reading client picks a time t
    for each read
      ask relevant replica to do the read at time t
  how does a replica read a key at time t?
    return the stored value with highest time <= t
  but wait, the replica may be behind
    that is, there may be a write at time < t, but replica hasn't seen it
    so replica must somehow be sure it has seen all writes <= t
    idea: has it seen *any* operation from time > t?
      if yes, and paxos group always agrees on ops in time order,
        it's enough to check/wait for an op with time > t
      that is what spanner does on reads (4.1.3)
  what time should a reading client pick?
    using current time may force lagging replicas to wait
    so perhaps a little in the past
    client may miss latest updates
    but at least it will see consistent snapshot
    in our example, won't see new image w/o also seeing ACL update

how does that fix our ACL/image example?
  1. W1: I write ACL, G1 assigns it time=10, then
  2. W2: I add image, G2 assigns it time=15 (> 10 since clocks agree)
  3. mom picks a time, for example t=14
  4. mom reads ACL t=14 from lagging G1 replica
     if it hasn't seen paxos agreements up through t=14, it knows to wait
     so it will return G1
  5. mom reads image from G2 at t=14
     image may have been written on that replica
     but it will know to *not* return it since image's time is 15
  other choices of t work too

Q: is it reasonable to assume that different computers' clocks agree?
   why might they not agree?

Q: what may go wrong if servers' clocks don't agree?

a performance problem: reading client may pick time in the
  future, forcing reading replicas to wait to "catch up"

a correctness problem:
  again, the ACL/image example
  G1 and G2 disagree about what time it is
  1. W1: I write ACL on G1 -- stamped with time=15
  2. W2: I add image on G2 -- stamped with time=10
  now a client read at t=14 will see image but not ACL update

Q: why doesn't spanner just ensure that the clocks are all correct?
   after all, it has all those master GPS / atomic clocks

TrueTime (section 3)
  there is an actual "absolute" time t_abs
    but server clocks are typically off by some unknown amount
    TrueTime can bound the error
  so now() yields an interval: [earliest,latest]
  earliest and latest are ordinary scalar times
    perhaps microseconds since Jan 1 1970
  t_abs is highly likely to be between earliest and latest

Q: how does TrueTime choose the interval?

Q: why are GPS time receivers able to avoid this problem?
   do they actually avoid it?
   what about the "atomic clocks"?

spanner assigns each write a scalar time
  might not be the actual absolute time
  but is chosen to ensure consistency

the danger:
  W1 at G1, G1's interval is [20,30]
    is any time in that interval OK?
  then W2 at G2, G2's interval is [11,21]
    is any time in that interval OK?
  if they are not careful, might get s1=25 s2=15

so what we want is:
  if W2 starts after W1 finishes, then s2 > s1
  simplified "external consistency invariant" from 4.1.2
  causes snapshot reads to see data consistent w/ true order of W1, W2

how does spanner assign times to writes?
  (again, this is much simplified, see 4.1.2)
  a write request arrives at paxos leader
  s will be the write's time-stamp
  leader sets s to TrueTime now().latest
    this is "Start" in 4.1.2
  then leader *delays* until s < now().earliest
    i.e. until s is guaranteed to be in the past (compared to absolute time)
    this is "commit wait" in 4.1.2
  then leader runs paxos to cause the write to happen
  then leader replies to client

does this work for our example?
  W1 at G1, TrueTime says [20,30]
    s1 = 30
    commit wait until TrueTime says [31,41]
    reply to client
  W2 at G2, TrueTime *must* now say >= [21,31]
    (otherwise TrueTime is broken)
    s2 = 31
    commit wait until TrueTime says [32,43]
    reply to client
  it does work for this example:
    the client observed that W1 finished before S2 started,
    and indeed s2 > s1
    even though G2's TrueTime clock was slow by the most it could be
    so if my mom sees S2, she is guaranteed to also see W1

why the "Start" rule?
  i.e. why choose the time at the end of the TrueTime interval?
  previous writers waited only until their timestamps were barely < t_abs
  new writer must choose s greater than any completed write
  t_abs might be as high as now().latest
  so s = now().latest

why the "Commit Wait" rule?
  ensures that s < t_abs
  otherwise write might complete with an s in the future
    and would let Start rule give too low an s to a subsequent write

Q: why commit *wait*; why not immediately write value with chosen time?
  indirectly forces subsequent write to have high enough s
    the system has no other way to communicate minimum acceptable next s
    for writes in different replica groups
  waiting forces writes that some external agent is serializing
    to have monotonically increasing timestamps
  w/o wait, our example goes back to s1=30 s2=21
  you could imagine explicit schemes to communicate last write's TS
    to the next write

Q: how long is the commit wait?

this answers today's Question
  a large TrueTime uncertainty requires a long commit wait
  so Spanner authors are interested in accurate low-uncertainty time

let's step back
  why did we get into all this timestamp stuff?
    our replicas were 100s or 1000s of miles apart (for locality/fault tol)
    we wanted fast reads from a local replica (no full paxos)
    our data was partitioned over many replica groups w/ separate clocks
    we wanted consistency for reads:
      if W1 then W2, reads don't see W2 but not W1
  it's complex but it makes sense as a
    high-performance evolution of Lab 3 / Lab 4

why is this timestamp technique interesting?
  we want to enforce order -- things that happened in some
    order in real time are ordered the same way by the
    distributed system -- "external consistency"
  the naive approach requires a central agent, or lots of communication
  Spanner does the synchronization implicitly via time
    time can be a form of communication
    e.g. we agree in advance to meet for dinner at 6:00pm

there's a lot of additional complexity in the paper
  transactions, two phase commit, two phase locking,
    schema change, query language, &c
  some of this we'll see more of later
  in particular, the problem of ordering events in a
    distributed system will come up a lot, soon
==========
6.824 2018 Lecture 14: Parameter Server Case Study

Scaling Distributed Machine Learning with the Parameter Server
Li, Andersen, Park, Smola, Ahmed, Josifovski, Long, Shekita, Su
OSDI 2014

Today: distributed machine learning
  Case study: Parameter Server
  Why are we reading this paper?
    influential design
    relaxed consistency
    different type of computation

Machine learning primer
  models are function approximators
    true function is unknown, so we learn an approximation from data
    ex: - f(user profile) -> likelihood of ad click
        - f(picture) -> likelihood picture contains a cat
        - f(words in document) -> topics/search terms for document
    function is typically very high-dimensional
  two phases: training and inference
    during training, expose model to many examples of data
      supervised: correct answer is known, check model response against it
      unsupervised: correct answer not known, measure model quality differently
    during inference, apply trained model to get predictions for unseen data
    parameter server is about making the training phase efficient

Features & parameters
  features: properties of input data that may provide signal to algorithm
    paper does not discuss how they are chosen; whole ML subfield of its own
  ex: computer vision algorithm can detect presence of whisker-like shapes
    if present, provides strong signal (= important param) that picture is a cat
    blue sky, by contrast, provides no signal (= unimportant parameter)
    we'd like the system to learn that whiskers are more important than blue sky
      i.e., "whisker presence" parameter should converge to high weight
  parameters are compact: a single floating point or integer number (weight)
    but there are many of them (millions/billions)
    ex: terms in ad => parameters for likelihood of click
      many unique combinations, some common ("used car") some not ("OSDI 2014")
    form a giant vector of numbers
  training iterates thousands of times to incrementally tune the parameter values
    popular algorithm: gradient descent, which generates "gradient updates"
    train, compute changes, apply update, train again
    iterations are very short (sub-second or a few seconds, esp. with GPUs/TPUs)

Distributed architecture for training
  need many workers
    because training data are too large for one machine
    for parallel speedup
    parameters may not fit on a single machine either
  all workers need access to parameters
  plan: distribute parameters + training data over multiple machines
    partition training data, run in parallel on partitions
    only interaction is via parameter updates
  each worker can access *any* parameter, but typically needs only small subset
    cf. Figure 3
    determined by the training data partitioning
  how do we update the parameters?

Strawman 1: broadcast parameter changes between workers
  at end of training iteration, workers exchange their parameter changes
  then apply them once all have received all changes
  similar to MapReduce-style shuffle
  Q: what are possible problems?
  A: 1) all-to-all broadcast exchange => ridiculous network traffic
     2) need to wait for all other workers before proceeding => idle time

Strawman 2: single coordinator collects and distributes updates
  at end of training iteration, workers send their changes to coordinator
  coordinator collects, aggregates, and sends aggregated updates to workers
    workers modify their local parameters
  Q: what are possible problems?
  A: 1) single coordinator gets congested with updates
     2) single coordinator is single point of failure
     3) what if a worker needs to read parameters it doesn't have?

Strawman 3: use Spark
  parameters for each iteration are an RDD (params_i)
  run worker-side computation in .map() transformation
    then do a reduceByKey() to shuffle parameter updates and apply
  Q: what are possible problems?
  A: 1) very inefficient! need to wait for *every* worker to finish iteration
     2) what if a worker need to read parameters it doesn't have? normal
        straight partitioning with narrow dependency doesn't apply

Parameter Server operation
  [diagram: parameter servers, workers; RM, task scheduler, training data]
  start off: initialize parameters at PS, push to workers
  on each iteration: assign training tasks to worker
    worker computes parameter updates
    worker potentially completes more training tasks for the same iteration
    worker pushes parameter updates to the responsible parameter servers
    parameter servers update parameters via user-defined function
      possibly aggregating parameter changes from multiple workers
    parameter servers replicate changes
      then ack to worker
    once done, worker pulls new parameter values
  key-value interface
    parameters often abstracted as big vector w[0, ..., z] for z parameters
      may actually store differently (e.g., hash table)
    in PS, each logical vector position stores (key, value)
      can index by key
      ex: (feature ID, weight)
    but can still treat the (key, value) parameters as a vector
      e.g., do to vector addition
  what are the bottleneck resources?
    worker: CPU (training compute > parameter update compute)
    parameter server: network (talks to many workers)

Range optimizations
  PS applies operations (push/pull/update) on key ranges, not single parameters
  why? big efficiency win due to batching!
    amortizes overheads for small updates
    e.g., syscalls, packet headers, interrupts
    think what would happen if we sent updates for individual parameters
      (feature ID, weight) parameter has 16 bytes
      IP headers: at least 20 bytes, TCP headers: dito
      40 bytes of header for 16 bytes of data -- 2.5x overhead
      syscall: ~2us, packet transmission: ~5ns at 10 Gbit/s -- 400x overhead
    so send whole ranges at a time to improve throughput
  further improvements:
    skip unchanged parameters
    skip keys with value zero in data for range
      can also use threshold to drop unimportant updates
    combine keys with same value

API
  not super clear in the paper!
  push(range, dest) / pull(range, dest) to send updates and pull parameters
  programmer implements WorkerIterate and ServerIterate methods (cf. Alg. 1)
    can push/pull multiple times and in response to control flow
  each WorkerIterate invocation is a "task"
    runs asynchronously -- doesn't block when pull()/push() invoked
    instead, run another task and come back to it when the RPC returns
  programmer can declare explicit dependencies between tasks
    details unclear

Consistent hashing
  need no single lookup directory to find parameter locations
    unlike earlier PS iteration, which used memcached
  parameter location determined by hashing key: H(k) = point on circle
  ranges on circle assigned to servers by hashing server identifier, i.e., H'(S)
    domains of H and H' must be the same (but hash function must not)
    each server owns keys between its hash point and the next
  well-known technique (will see again in Chord, Dynamo)

Fault tolerance
  what if a worker crashes?
    restart on another machine; load training data, pull parameters, continue
    or just drop it -- will only lose a small part of training data set
      usually doesn't affect outcome much, or training may take a little longer
  what if a parameter server crashes?
    lose all parameters stored there
  replicate parameters on multiple servers
    use consistent hashing ring to decide where
    each server stores neighboring counter-clockwise key ranges
    on update, first update replicas, then reply to worker
      no performance issue because workers are async, meanwhile do other tasks
  on failure, neighboring backup takes over key range
    has already replicated the parameters
    workers now talk to the new master

Relaxed consistency
  many ML algorithms tolerate somewhat stale parameters in training
    intuition: if parameters only change a little, not too bad to use old ones
      won't go drastically wrong (e.g., cat likelihood 85% instead of 91%)
    still converges to a decent model
    though may take longer (more training iterations due to higher error)
  trade-off: wait (& sit idle) vs. continue working with stale parameters
    up to the user to specify (via task dependencies)

Vector clocks
  need a mechanism to synchronize
    when strong consistency is required
    even with relaxed consistency: some workers may by very slow
      want to avoid others running ahead and some parameters getting very stale
  i.e., workers need to be aware of how far along others and the servers are
  vector clocks for each key
    each "clock" indicates where *each* other machine is at for that key
  [example diagram: vector with time entries for N nodes]
  only really works because PS uses ranges!
    vector clock is O(N) size, N = 1,000+
    overhead would be ridiculous if PS stored a vector clock for every key
    but range allows amortizing it
    PS always updates vector clock for a whole range at a time

Performance
  scale: 10s of billions of parameters, 10-100k cores (Fig. 1)
  very little idle time (Fig. 10)
  network traffic optimizations help, particularly at servers (Fig. 11)
  relaxed consistency helps up to a point (Fig. 13)

Real-world use
  TensorFlow, other learning frameworks
    high performance: two PS easily saturate 5-10 GBit/s
  influential design, though APIs vary
  discussion
    ML is an example of an application where inconsistency is often okay
    allows for different and potentially more efficient designs

References
 * PS implementation in TensorFlow: https://www.tensorflow.org/deploy/distributed
==========
6.824 2018 Lecture 15: Frangipani

Frangipani: A Scalable Distributed File System
Thekkath, Mann, Lee
SOSP 1997

why are we reading this paper?
  performance via caching
  cache coherence
  decentralized design

what's the overall design?
  a network file system
    works transparently with existing apps (text editors &c)
    much like Athena's AFS
  users; workstations + Frangipani; network; petal
  Petal: block storage service; replicated; striped+sharded for performance
  What's in Petal?
    directories, i-nodes, file content blocks, free bitmaps
    just like an ordinary hard disk file system
  Frangipani: decentralized file service; cache for performance

what's the intended use?
  environment: single lab with collaborating engineers
    == the authors' research lab
    programming, text processing, e-mail, &c
  workstations in offices
  most file access is to user's own files
  need to potentially share any file among any workstations
    user/user collaboration
    one user logging into multiple workstations
  so:
    common case is exclusive access; want that to be fast
    but files sometimes need to be shared; want that to be correct
  this was a common scenario when the paper was written

why is Frangipani's design good for the intended use?
  it caches aggressively in each workstation, for speed
  cache is write-back
    allows updates to cached files/directories without network traffic
  all operations entirely local to workstation -- fast
    including e.g. creating files, creating directories, rename, &c
    updates proceed without any RPCs if everything already cached
    so file system code must reside in the workstation, not server
    "decentralized"
  cache also helps for scalability (many workstations)
    servers were a serious bottleneck in previous systems

what's in the Frangipani workstation cache?
  what if WS1 wants to create and write /u/rtm/grades?
  read /u/rtm information from Petal into WS1's cache
  add entry for "grades" just in the cache
  don't immediately write back to Petal!
    in case WS1 wants to do more modifications

challenges
  WS2 runs "ls /u/rtm" or "cat /u/rtm/grades"
    will WS2 see WS1's write?
    write-back cache, so WS'1 writes aren' in Petal
    caches make stale reads a serious threat
    "coherence"
  WS1 and WS2 concurrently try to create tmp/a and tmp/b
    will they overwrite each others' changes?
    there's no central file server to sort this out!
    "atomicity"
  WS1 crashes while renaming
    but other workstations are still operating
    how to ensure no-one sees the mess? how to clean up?
    "crash recovery"

"cache coherence" solves the "read sees write" problem
  the goal is linearizability AND caching
  there are lots of "coherence protocols"
  a common pattern: file servers, distributed shared memory, multi-core

Frangipani's coherence protocol (simplified):
  lock server (LS), with one lock per file/directory
    owner(lock) = WS, or nil
  workstation (WS) Frangipani cache:
    cached files and directories: present, or not present
    cached locks: locked-busy, locked-idle, unlocked
  workstation rules:
    acquire, then read from Petal
    write to Petal, then release
    don't cache unless you hold the lock
  coherence protocol messages:
    request  (WS -> LS)
    grant (LS -> WS)
    revoke (LS -> WS)
    release (WS -> LS)

the locks are named by files/directories (really i-numbers),
though the lock server doesn't actually understand anything
about file systems or Petal.

example: WS1 changes directory /u/rtm, then WS2 reads it

WS1                      LS            WS2
read /u/rtm
  --request(/u/rtm)-->
                         owner(/u/rtm)=WS1
  <--grant(/u/rtm)---
(read+cache /u/rtm data from Petal)
(create /u/rtm/grades locally)
(when done, cached lock in locked-idle state)
                                       read /u/rtm
                          <--request(/u/rtm)--
   <--revoke(/u/rtm)--
(write modified /u/rtm to Petal)
   --release(/u/rtm)-->
                         owner(/u/rtm)=WS2
                           --grant(/u/rtm)-->
                                       (read /u/rtm from Petal)

the point:
  locks and rules force reads to see last write
  locks ensure that "last write" is well-defined

coherence optimizations
  the "locked-idle" state is already an optimization
  Frangipani has shared read locks, as well as exclusive write locks
  you could imagine WS-to-WS communication, rather than via LS and Petal

next challenge: atomicity
  what if two workstations try to create the same file at the same time?
  are partially complete multi-write operations visible?
    e.g. file create initializes i-node, adds directory entry
    e.g. rename (both names visible? neither?)

Frangipani has transactions:
  WS acquires locks on all file system data that it will modify
  performs modifications with all locks held
  only releases when finished
  thus no other WS can see partially-completed operations
    and no other WS can race to perform updates (e.g. file creation)

note Frangipani's locks are doing two different things:
  cache coherence
  atomic transactions

next challenge: crash recovery

What if a Frangipani workstation dies while holding locks?
  other workstations will want to continue operating...
  can we just revoke dead WS's locks?
  what if dead WS had modified data in its cache?
  what if dead WS had started to write back modified data to Petal?
    e.g. WS wrote new directory entry to Petal, but not initialized i-node
    this is the troubling case

Is it OK to just wait until a crashed workstation reboots?

Frangipani uses write-ahead logging for crash recovery
  So if a crashed workstation has done some Petal writes for an operation,
    but not all, the writes can be completed from the log
  Very traditional -- but...
  1) Frangipani has a separate log for each workstation
     rather than the traditional log per shard of the data
     this avoids a logging bottleneck, eases decentralization
     but scatters updates to a given file over many logs
  2) Frangipani's logs are in shared Petal storage
     WS2 may read WS1's log to recover from WS1 crashing
  Separate logs is an interesting and unusual arrangement

What's in the log?
  log entry:
    (this is a bit of guess-work, paper isn't explicit)
    log sequence number
    array of updates:
      block #, new version #, offset, new bytes
    just contains meta-data updates, not file content updates
  example -- create file d/f produces a log entry:
    a two-entry update array:
      add an "f" entry to d's content block, with new i-number
      initialize the i-node for f
  initially the log entry is in WS local memory (not yet Petal)

When WS gets lock revocation on modified directory from LS:
  1) force its entire log to Petal, then
  2) send the cached updated blocks to Petal, then
  3) release the locks to the LS

Why must WS write log to Petal before updating
  i-node and directory &c in Petal?

Why delay writing the log until LS revokes locks?

What happens when WS1 crashes while holding locks?
  Not much, until WS2 requests a lock that WS1 holds
    LS sends grant to WS1, gets no response
    LS times out, tells WS2 to recover WS1 from its log in Petal
  What does WS2 do to recover from WS1's log?
    Read WS1's log from Petal
    Perform Petal writes described by logged operation
    Tell LS it is done, so LS can release WS1's locks

Note it's crucal that each WS log is in Petal so that it can
  be read by any WS for recovery.

What if WS1 crashes before it even writes recent operations to the log?
  WS1's recent operations may be totally lost if WS1 crashes.
  But the file system will be internally consistent.

Why is it safe to replay just one log, despite interleaved
  operations on same files by other workstations?
Example:
  WS1: delete(d/f)               crash
  WS2:               create(d/f)
  WS3 is recovering WS1's log -- but it doesn't look at WS2's log
  Will recovery re-play the delete? 
    This is The Question
    No -- prevented by "version number" mechanism
    Version number in each meta-data block (i-node) in Petal
    Version number(s) in each logged op is block's version plus one
    Recovery replays only if op's version > block version
      i.e. only if the block hasn't yet been updated by this op
  Does WS3 need to aquire the d or d/f lock?
    No: if version number same as before operation, WS1 couldn't
        have released the lock, so safe to update in Petal

Why is it OK that the log doesn't hold file *content*?
  If a WS crashes before writing content to Petal, it will be lost.
  Frangipani recovery defends the file system's own data structures.
  Applications can use fsync() to do their own recoverably content writes.
  It would be too expensive for Frangipani to log content writes.
  Most disk file systems (e.g. Linux) are similar, so applications
    already know how to cope with loss of writes before crash.

What if:
  WS1 holds a lock
  Network partition
  WS2 decides WS1 is dead, recovers, releases WS1's locks
  But WS1 is alive and subsequently writes data covered by the lock
  Locks have leases!
    Lock owner can't use a lock past its lease period
    LS doesn't start recovery until after lease expires

Is Paxos (== Raft) hidden somewhere here?
  Yes -- choice of lock server, choice of Petal primary/backup
  ensures a single lock server, despite partition
  ensures a single primary for each Petal shard

Performance?
  hard to judge numbers from 1997
  do they hit hardware limits? disk b/w, net b/w
  do they scale well with more hardware?
  what scenarios might we care about?
    read/write lots of little files (e.g. reading my e-mail)
    read/write huge files

Small file performance -- Figure 5
  X axis is number of active workstations
    each workstation runs a file-intensive benchmark
    workstations use different files and directories
  Y axis is completion time for a single workstation
  flat implies good scaling == no significant shared bottleneck
  presumably each workstation is just using its own cache
  possibly Petal's many disks also yield parallel performance

Big file performance
  each disk: 6 MB / sec
    Petal stripes to get more than that
  7 Petal servers, 9 disks per Petal server
    336 MB/s raw disk b/w, but only 100 MB/s via Petal
  a single Frangipani workstation, Table 3
    write: 15 MB/s -- limited by network link
    read: 10 MB/s -- limited by weak pre-fetch (?), could be 15
  lots of Frangipani workstations
    Figure 6 -- reads scale well with more machines
    Figure 7 -- writes hit hardware limits of Petal (2x for replication)

For what workloads is Frangipani likely to have poor performance?
  files bigger than cache?
  lots of read/write sharing?
  caching requires a reasonable working set size
  whole-file locking a poor fit for e.g. distributed DB
  coherence is too good for e.g. web site back-end

Petal details
  Petal provides Frangipani w/ fault-tolerant storage
    so it's worth discussing
  block read/write interface
    compatible with existing file systems
  looks like single huge disk, but many servers and many many disks
    big, high performance
    striped, 64-KB blocks
  virtual: 64-bit sparse address space, allocate on write
    address translation map
  primary/backup (one backup server)
    primary sends each write to the backup
  uses Paxos to agree on primary for each virt addr range
  what about recovery after crash?
    suppose pair is S1+S2
    S1 fails, S2 is now sole server
    S1 restarts, but has missed lots of updates
    S2 remembers a list of every block it wrote!
    so S1 only has to read those blocks, not entire disk
  logging
    virt->phys map and missed-write info

Limitations
  Most useful for e.g. programmer workstations, not so much otherwise
  Frangipani enforces permissions, so workstations must be trusted
    so Athena couldn't run Frangipani on Athena workstations
  Frangipani/Petal split is a little awkward
    both layers log
    Petal may accept updates from "down" Frangipani workstations
    more RPC messages than a simple file server
  A file system is not a great API for many applications, e.g. web site

Ideas to remember
  client-side caching for performance
  cache coherence protocols
  decentralized complex service on simple shared storage layer
  per-client log for decentralized recovery
==========
6.824 2015 Lecture 16: Scaling Memcache at Facebook

Scaling Memcache at Facebook, by Nishtala et al, NSDI 2013

why are we reading this paper?
  it's an experience paper, not about new ideas/techniques
  three ways to read it:
    cautionary tale of problems from not taking consistency seriously
    impressive story of super high capacity from mostly-off-the-shelf s/w
    fundamental struggle between performance and consistency
  we can argue with their design, but not their success

how do web sites scale up with growing load?
  a typical story of evolution over time:
  1. one machine, web server, application, DB
     DB stores on disk, crash recovery, transactions, SQL
     application queries DB, formats, HTML, &c
     but the load grows, your PHP application takes too much CPU time
  2. many web FEs, one shared DB
     an easy change, since web server + app already separate from storage
     FEs are stateless, all sharing (and concurrency control) via DB
     but the load grows; add more FEs; soon single DB server is bottleneck
  3. many web FEs, data sharded over cluster of DBs
     partition data by key over the DBs
       app looks at key (e.g. user), chooses the right DB
     good DB parallelism if no data is super-popular
     painful -- cross-shard transactions and queries probably don't work
       hard to partition too finely
     but DBs are slow, even for reads, why not cache read requests?
  4. many web FEs, many caches for reads, many DBs for writes
     cost-effective b/c read-heavy and memcached 10x faster than a DB
       memcached just an in-memory hash table, very simple
     complex b/c DB and memcacheds can get out of sync
     (next bottleneck will be DB writes -- hard to solve)

the big facebook infrastructure picture
  lots of users, friend lists, status, posts, likes, photos
    fresh/consistent data apparently not critical
    because humans are tolerant?
  high load: billions of operations per second
    that's 10,000x the throughput of one DB server
  multiple data centers (at least west and east coast)
  each data center -- "region":
    "real" data sharded over MySQL DBs
    memcached layer (mc)
    web servers (clients of memcached)
  each data center's DBs contain full replica
  west coast is master, others are slaves via MySQL async log replication

how do FB apps use mc?
  read:
    v = get(k) (computes hash(k) to choose mc server)
    if v is nil {
      v = fetch from DB
      set(k, v)
    }
  write:
    v = new value
    send k,v to DB
    delete(k)
  application determines relationship of mc to DB
    mc doesn't know anything about DB
  FB uses mc as a "look-aside" cache
    real data is in the DB
    cached value (if any) should be same as DB

what does FB store in mc?
  paper does not say
  maybe userID -> name; userID -> friend list; postID -> text; URL -> likes
  basically copies of data from DB

paper lessons:
  look-aside is much trickier than it looks -- consistency
    paper is trying to integrate mutually-oblivious storage layers
  cache is critical:
    not really about reducing user-visible delay
    mostly about surviving huge load!
    cache misses and failures can create intolerable DB load
  they can tolerate modest staleness: no freshness guarantee
  stale data nevertheless a big headache
    want to avoid unbounded staleness (e.g. missing a delete() entirely)
    want read-your-own-writes
    each performance fix brings a new source of staleness
  huge "fan-out" => parallel fetch, in-cast congestion

let's talk about performance first
  majority of paper is about avoiding stale data
  but staleness only arose from performance design

performance comes from parallel get()s by many mc servers
  driven by parallel processing of HTTP requests by many web servers
  two basic parallel strategies for storage: partition vs replication

will partition or replication yield most mc throughput?
  partition: server i, key k -> mc server hash(k)
  replicate: server i, key k -> mc server hash(i)
  partition is more memory efficient (one copy of each k/v)
  partition works well if no key is very popular
  partition forces each web server to talk to many mc servers (overhead)
  replication works better if a few keys are very popular

performance and regions (Section 5)

Q: what is the point of regions -- multiple complete replicas?
   lower RTT to users (east coast, west coast)
   parallel reads of popular data due to replication
   (note DB replicas help only read performance, no write performance)
   maybe hot replica for main site failure?

Q: why not partition users over regions?
   i.e. why not east-coast users' data in east-coast region, &c
   social net -> not much locality
   very different from e.g. e-mail

Q: why OK performance despite all writes forced to go to the master region?
   writes would need to be sent to all regions anyway -- replicas
   users probably wait for round-trip to update DB in master region
     only 100ms, not so bad
   users do not wait for all effects of writes to finish
     i.e. for all stale cached values to be deleted
   
performance within a region (Section 4)

multiple mc clusters *within* each region
  cluster == complete set of mc cache servers
    i.e. a replica, at least of cached data

why multiple clusters per region?
  why not add more and more mc servers to a single cluster?
  1. adding mc servers to cluster doesn't help single popular keys
     replicating (one copy per cluster) does help
  2. more mcs in cluster -> each client req talks to more servers
     and more in-cast congestion at requesting web servers
     client requests fetch 20 to 500 keys! over many mc servers
     MUST request them in parallel (otherwise total latency too large)
     so all replies come back at the same time
     network switches, NIC run out of buffers
  3. hard to build network for single big cluster
     uniform client/server access
     so cross-section b/w must be large -- expensive
     two clusters -> 1/2 the cross-section b/w

but -- replicating is a waste of RAM for less-popular items
  "regional pool" shared by all clusters
  unpopular objects (no need for many copies)
  decided by *type* of object
  frees RAM to replicate more popular objects

bringing up new mc cluster was a serious performance problem
  new cluster has 0% hit rate
  if clients use it, will generate big spike in DB load
    if ordinarily 1% miss rate, and (let's say) 2 clusters,
      adding "cold" third cluster will causes misses for 33% of ops.
    i.e. 30x spike in DB load!
  thus the clients of new cluster first get() from existing cluster (4.3)
    and set() into new cluster
    basically lazy copy of existing cluster to new cluster
  better 2x load on existing cluster than 30x load on DB

important practical networking problems:
  n^2 TCP connections is too much state
    thus UDP for client get()s
  UDP is not reliable or ordered
    thus TCP for client set()s
    and mcrouter to reduce n in n^2
  small request per packet is not efficient (for TCP or UDP)
    per-packet overhead (interrupt &c) is too high
    thus mcrouter batches many requests into each packet
    
mc server failure?
  can't have DB servers handle the misses -- too much load
  can't shift load to one other mc server -- too much
  can't re-partition all data -- time consuming
  Gutter -- pool of idle servers, clients only use after mc server fails

The Question:
  why don't clients send invalidates to Gutter servers?
  my guess: would double delete() traffic
    and send too many delete()s to small gutter pool
    since any key might be in the gutter pool

thundering herd
  one client updates DB and delete()s a key
  lots of clients get() but miss
    they all fetch from DB
    they all set()
  not good: needless DB load
  mc gives just the first missing client a "lease"
    lease = permission to refresh from DB
    mc tells others "try get() again in a few milliseconds"
  effect: only one client reads the DB and does set()
    others re-try get() later and hopefully hit

let's talk about consistency now

the big truth
  hard to get both consistency (== freshness) and performance
  performance for reads = many copies
  many copies = hard to keep them equal

what is their consistency goal?
  *not* read sees latest write
    since not guaranteed across clusters
  more like "not more than a few seconds stale"
    i.e. eventual
  *and* writers see their own writes
    read-your-own-writes is a big driving force

first, how are DB replicas kept consistent across regions?
  one region is master
  master DBs distribute log of updates to DBs in slave regions
  slave DBs apply
  slave DBs are complete replicas (not caches)
  DB replication delay can be considerable (many seconds)

how do we feel about the consistency of the DB replication scheme?
  good: eventual consistency, b/c single ordered write stream
  bad: longish replication delay -> stale reads

how do they keep mc content consistent w/ DB content?
  1. DBs send invalidates (delete()s) to all mc servers that might cache
  2. writing client also invalidates mc in local cluster
     for read-your-writes

why did they have consistency problems in mc?
  client code to copy DB to mc wasn't atomic:
    1. writes: DB update ... mc delete()
    2. read miss: DB read ... mc set()
  so *concurrent* clients had races

what were the races and fixes?

Race 1:
  k not in cache
  C1 get(k), misses
  C1 v = read k from DB
    C2 updates k in DB
    C2 and DB delete(k) -- does nothing
  C1 set(k, v)
  now mc has stale data, delete(k) has already happened
  will stay stale indefinitely, until key is next written
  solved with leases -- C1 gets a lease, but C2's delete() invalidates lease,
    so mc ignores C1's set
    key still missing, so next reader will refresh it from DB

Race 2:
  during cold cluster warm-up
  remember clients try get() in warm cluster, copy to cold cluster
  k starts with value v1
  C1 updates k to v2 in DB
  C1 delete(k) -- in cold cluster
  C2 get(k), miss -- in cold cluster
  C2 v1 = get(k) from warm cluster, hits
  C2 set(k, v1) into cold cluster
  now mc has stale v1, but delete() has already happened
    will stay stale indefinitely, until key is next written
  solved with two-second hold-off, just used on cold clusters
    after C1 delete(), cold ignores set()s for two seconds
    by then, delete() will propagate via DB to warm cluster

Race 3:
  k starts with value v1
  C1 is in a slave region
  C1 updates k=v2 in master DB
  C1 delete(k) -- local region
  C1 get(k), miss
  C1 read local DB  -- sees v1, not v2!
  later, v2 arrives from master DB
  solved by "remote mark"
    C1 delete() marks key "remote"
    get()/miss yields "remote"
      tells C1 to read from *master* region
    "remote" cleared when new data arrives from master region

Q: aren't all these problems caused by clients copying DB data to mc?
   why not instead have DB send new values to mc, so clients only read mc?
     then there would be no racing client updates &c, just ordered writes
A:
  1. DB doesn't generally know how to compute values for mc
     generally client app code computes them from DB results,
       i.e. mc content is often not simply a literal DB record
  2. would increase read-your-own writes delay
  3. DB doesn't know what's cached, would end up sending lots
     of values for keys that aren't cached

PNUTS does take this alternate approach of master-updates-all-copies

FB/mc lessons for storage system designers?
  cache is vital to throughput survival, not just a latency tweak
  need flexible tools for controlling partition vs replication
  need better ideas for integrating storage layers with consistency
==========
6.824 2018 Lecture 17: Eventual Consistency, Bayou

"Managing Update Conflicts in Bayou, a Weakly Connected Replicated
Storage System" by Terry, Theimer, Petersen, Demers, Spreitzer,
Hauser, SOSP 95. And some material from "Flexible Update Propagation
for Weakly Consistent Replication" SOSP 97 (sections 3.3, 3.5, 4.2,
4.3).

Why are we reading this paper?
  It explores an important and interesting problem space.
  It uses some specific techniques worth knowing.

Big points:
  * Disconnected / weakly connected operation is often valuable.
    iPhone sync, Dropbox, git, Amazon Dynamo, Cassandra, &c
  * Disconnected operation implies eventual (weak) consistency.
    And it takes work (i.e. ordering) to even get that.
  * Disconnected writable replicas lead to update conflicts.
  * Conflict resolution generally has to be application-specific.

Technical ideas to remember:
  Log of operations is equivalent to data.
  Log helps eventual consistency (merge, order, and re-execute).
  Log helps conflict resolution (write operations easier than data).
  Causal consistency via Lamport-clock timestamps.
  Quick log comparison via version vectors.

Paper context:
  Early 1990s
  Dawn of PDAs, laptops, tablets
    Clunky but clear potential
  They wanted devices to be useful regardless of connectivity.
    Much like today's smartphones, tablets, laptops.

Let's build a conference room scheduler
  Only one meeting allowed at a time (one room).
  Each entry has a time and a description.
  We want everyone to end up seeing the same set of entries.

Traditional approach: one server
  Server executes one client request at a time
  Checks for conflicting time, says yes or no
  Updates DB
  Proceeds to next request
  Server implicitly chooses order for concurrent requests

Why aren't authors satisfied with a central server?
  They want full disconnected operation.
    So need DB replica in each device.
    Modify on any device, as well as read.
    "Sync" devices to propagate DB changes (Bayou's anti-entropy).
  They want to be able to use point-to-point connectivity.
    Sync via bluetooth to colleague in next airplane seat.

Why not merge DB records? (Bayou doesn't do this)
 Allow any pair of devices to sync (synchronize) their DBs.
 Sync could compare DBs, adopt other device's changed records.
 Need a story for conflicting entries, e.g. two meetings at same time.
   User may not be available to decide at time of DB merge.
   So need automatic reconciliation.

There are lots of possible conflict resolution schemes.
  E.g. adopt latest update, discard others.
  But we don't want people's calendar entries to simply disappear!
 
Idea for conflicts: update functions
  Application supplies a function, not just a DB write.
  Function reads DB, decides how best to update DB.
  E.g. "Meet at 9 if room is free at 9, else 10, else 11."
    Rather than just "Meet at 9"
  Function can make reconciliation decision for absent user.
  Sync exchanges functions, not DB content.

Problem: can't just run update functions as they arrive
  A's fn: staff meeting at 10:00 or 11:00
  B's fn: hiring meeting at 10:00 or 11:00
  X syncs w/ A, then B
  Y syncs w/ B, then A
  Will X put A's meeting at 10:00, and Y put A's at 11:00?

Goal: eventual consistency
  OK for X and Y to disagree initially
  But after enough syncing, all devices' DBs should be identical

Idea: ordered update log
  Ordered log of update functions at each device.
  Syncing == ensure both devices have same log (same updates, same order).
  DB is result of applying update functions in order.
  Same log => same order => same DB content.
  Note we're relying here on equivalence of two state representations:
    DB and log of operations.
    Raft also uses this idea.

How can all devices agree on update order?
  Assign a timestamp to each update when originally created.
  Timestamp: <T, I>
  T is creating device's wall-clock time.
  I is creating device's ID.
  Ordering updates a and b:
    a < b if a.T < b.T or (a.T = b.T and a.I < b.I)

Example:
 <10,A>: staff meeting at 10:00 or 11:00
 <20,B>: hiring meeting at 10:00 or 11:00
 What's the correct eventual outcome?
   the result of executing update functions in timestamp order
   staff at 10:00, hiring at 11:00

What DB content before sync?
  A's DB: staff at 10:00
  B's DB: hiring at 10:00
  This is what A/B users will see before syncing.

Now A and B sync with each other
  Each sorts new entries into its log, order by timestamp
  Both now know the full set of updates
  A can just run B's update function
  But B has *already* run B's operation, too soon!

Roll back and replay
  B needs to to "roll back" DB, re-run both ops in the right order
  The "Undo Log" in Figure 4 allws efficient roll-back

Big point: the log holds the truth; the DB is just an optimization

Now DBs will be eventually consistent.
  If everyone syncs enough,
  and no-one creates new updates,
  every device will have the same ordered log,
  and everyone's DB will end up with identical content.

We now know enough to answer The Question.
  initially A=foo B=bar
  one device: copy A to B
  other device: copy B to A
  dependency check?
  merge procedure?
  why do all devices agree on final result?
  
Will update order be consistent with wall-clock time?
  Maybe A went first (in wall-clock time) with <10,A>
  Device clocks unlikely to be perfectly synchronized
  So B could then generate <9,B>
  B's meeting gets priority, even though A asked first

Will update order be consistent with causality?
  What if A adds a meeting, 
    then B sees A's meeting,
    then B deletes A's meeting.
  Perhaps
    <10,A> add
    <9,B> delete -- B's clock is slow
  Now delete will be ordered before add!
  So: design so far is not causally consistent.

Causal consistency means that if operation X might have caused
  or influenced operation Y, then everyone should order X before Y.

Bayou uses "Lamport logical clocks" for causal consistency
  Want to timestamp writes s.t.
    if device observes E1, then generates E2, then TS(E2) > TS(E1)
  So all devices will order E1, then E2
  Lamport clock:
    Tmax = highest timestamp seen from any device (including self)
    T = max(Tmax + 1, wall-clock time) -- to generate a timestamp
  Note properties:
    E1 then E2 on same device => TS(E1) < TS(E2)
    BUT
    TS(E1) < TS(E2) does not imply E1 came before or caused E2

Logical clock solves add/delete causality example
  When B sees <10,A>,
    B will set its Tmax to 10, so
    B will generate <11,B> for its delete

Irritating that there could be a long-delayed update with lower TS
  That can cause the results of my update to change
    User can never be sure if meeting time is final!
    Entries are "tentative"
  Would be nice if each update eventually became "stable"
    => no changes in update order up through that point
    => effect of write function now fixed, e.g. meeting time won't change
    => don't have to roll back, re-run committed updates
  We'd like to know when a write is stable, and tell the user

Idea: a fully decentralized "commit" scheme (Bayou doesn't do this)
  <10,A> is stable if I'll never see a new update w/ TS <= 10
  Once I've seen an update w/ TS > 10 from *every* device
    I'll never see any new TS < 10 (sync sends updates in TS order)
    Then <10,A> is stable
  Why doesn't Bayou use this decentralized commit scheme?

Idea: Bayou's "primary replica" to commit updates.
  One device is the "primary replica".
  Primary sees updates via sync in the ordinary way.
  Primary marks each received update with a Commit Sequence Number (CSN).
    That update is committed.
    So a complete timestamp is <CSN, logical-time, device-id>
    Uncommitted updates come after all committed updates
      i.e. have infinite CSN
  CSN notifications are synced between devices.
 
Why does the commit / CSN scheme eventually yield stability?
  Primary assigns only increasing CSNs.
  Device logs order all updates with CSN before any w/o CSN.
  So once an update has a CSN, the set of previous updates is fixed.

Will commit order match tentative order?
  Often.
  Syncs send in log order ("prefix property")
    Including updates learned from other devices.
  So if A's update log says
    <-,10,X>
    <-,20,A>
  A will send both to primary, in that order
    Primary will assign CSNs in that order
    Commit order will, in this case, match tentative order

Will commit order *always* match tentative order?
  No: primary may see newer updates before older ones.
  A has just: <-,10,A> W1
  B has just: <-,20,B> W2
  If C sees both, C's order: W1 W2
  B syncs with primary, W2 gets CSN=5.
  Later A syncs w/ primary, W1 gets CSN=6.
  When C syncs w/ primary, C will see order change to W2 W1
    <5,20,B> W2
    <6,10,A> W1
  So: committing may change order.
  
How Bayou syncs (this is anti-entropy)?
  A sending to B
  Need a quick way for B to tell A what to send
  Prefix property simplifies syncing (i.e. sync is always in log order)
    So it's meaningful for B to say "I have everything up to ..."
  Committed updates are easy:
    B sends its highest CSN to A
    A sends log entries between B's highest CSN and A's highest CSN
  What about tentative updates?
  A has:
    <-,10,X>
    <-,20,Y>
    <-,30,X>
    <-,40,X>
  B has:
    <-,10,X>
    <-,20,Y>
    <-,30,X>
  At start of sync, B tells A "X 30, Y 20"
    I.e. for each device, highest TS B has seen from that device.
    Sync prefix property means B has all X updates before 30,
      all Y before 20
  A sends all X's updates after <-,30,X>,
    all Y's updates after <-,20,Y>, &c
  "X 30, Y 20" is a version vector -- it summarizes log content
    It's the "F" vector in Figure 4
    A's F: [X:40,Y:20]
    B's F: [X:30,Y:20]

It's worth remembering the "version vector" idea
  used in many systems
  typically a summary of state known by a participant
  one entry per participant
    meaning "I have seen all updates from Pi through update number Vi"

Devices can discard committed updates from log.
  (a lot like Raft snapshots)
  Instead, keep a copy of the DB as of the highest known CSN.
  Roll back to that DB when replaying tentative update log.
  Never need to roll back farther.
    Prefix property guarantees seen CSN=x => seen CSN<x.
    No changes to update order among committed updates.

How do I sync if I've discarded part of my log?
 (a lot like Raft InstallSnapshot RPC)
 Suppose I've discarded all updates with CSNs.
 I keep a copy of the stable DB reflecting just discarded entries.
 If syncing to device X, and its highest CSN is less than mine:
   Send X my complete DB.
 In practice, Bayou devices keep the last few committed updates.
   To reduce chance of having to send whole DB during sync.

How could we cope with a new server Z joining the system?
  Could it just start generating writes, e.g. <-,1,Z> ?
  And other devices just start including Z in VVs?
  If A syncs to B, A has <-,10,Z>, but B has no Z in VV
    A should pretend B's VV was [Z:0,...]

What happens when Z retires (leaves the system)?
  We want to stop including Z in VVs!
  How to announce that Z is gone?
    Z sends update <-,?,Z> "retiring"
  If you see a retirement update, omit Z from VV
  How to deal with a VV that's missing Z?
  If A has log entries from Z, but B's VV has no Z entry:
    e.g. A has <-,25,Z>, B's VV is just [A:20, B:21]
    Maybe Z has retired, B knows, A does not
    Maybe Z is new, A knows, B does not
  Need a way to disambiguate: Z missing from VV b/c new, or b/c retired?

Bayou's retirement plan
  Z joins by contacting some server X
  Z's ID is <Tz,X>
    Tz is X's logical clock as of when Z joined
  X issues <-,Tz,X>:"new server ID=<Tz,X>"

How does ID=<Tz,X> scheme help disambiguate new vs forgotten?
  Suppose Z's ID is <20,X>
  A syncs to B
    A has log entry from Z <-,25,<20,X>>
    B's VV has no Z entry -- has B never seen Z,
      or already seen Z's retirement?
  One case:
    B's VV: [X:10, ...]
    10 < 20 implies B hasn't yet seen X's "new server Z" update
  The other case:
    B's VV: [X:30, ...]
    20 < 30 implies B once knew about Z, but then saw a retirement update

In a few lectures: Dynamo, a real-world DB with eventual consistency
==========
6.824 2018 Lecture 19: P2P, DHTs, and Chord

Reminders:
  course evaluations
  4B due friday
  project reports due friday
  project demos next week
  final exam on May 24th

Today's topic: Decentralized systems, peer-to-peer (P2P), DHTs
  potential to harness massive [free] user compute power, network b/w
  potential to build reliable systems out of many unreliable computers
  potential to shift control/power from organizations to users
  appealing, but has been hard in practice to make the ideas work well

Peer-to-peer
  [user computers, files, direct xfers]
  users computers talk directly to each other to implement service
    in contrast to user computers talking to central servers
  could be closed or open
  examples:
    bittorrent file sharing, skype, bitcoin

Why might P2P be a win?
  spreads network/caching costs over users
  absence of central server may mean:
    easier/cheaper to deploy
    less chance of overload
    single failure won't wreck the whole system
    harder to attack

Why don't all Internet services use P2P?
  can be hard to find data items over millions of users
  user computers not as reliable as managed servers
  if open, can be attacked via evil participants

The result is that P2P has been limited to a few niches:
  [Illegal] file sharing
    Popular data but owning organization has no money
  Chat/Skype
    User to user anyway; privacy and control
  Bitcoin
    No natural single owner or controller

Example: classic BitTorrent
  a cooperative, popular download system
  user clicks on download link for e.g. latest Linux kernel distribution
    gets torrent file w/ content hash and IP address of tracker
  user's BT app talks to tracker
    tracker tells it list of other users w/ downloaded file
  user't BT app talks to one or more users w/ the file
  user's BT app tells tracker it has a copy now too
  user's BT app serves the file to others for a while
  the point:
    provides huge download b/w w/o expensive server/link

But: the tracker is a weak part of the design
  makes it hard for ordinary people to distribute files (need a tracker)
  tracker may not be reliable, especially if ordinary user's PC
  single point of attack by copyright owner, people offended by content

BitTorrent can use a DHT instead of a tracker
  this is the topic of today's readings
  BT apps cooperatively implement a "DHT"
    a decentralized key/value store, DHT = distributed hash table
  the key is the torrent file content hash ("infohash")
  the value is the IP address of an BT app willing to serve the file
    Kademlia can store multiple values for a key
  app does get(infohash) to find other apps willing to serve
    and put(infohash, self) to register itself as willing to serve
    so DHT contains lots of entries with a given key:
      lots of peers willing to serve the file
  app also joins the DHT to help implement it

Why might the DHT be a win for BitTorrent?
  more reliable than single classic tracker
    keys/value spread/cached over many DHT nodes
    while classic tracker may be just an ordinary PC
  less fragmented than multiple trackers per torrent
    so apps more likely to find each other
  maybe more robust against legal and DoS attacks

How do DHTs work?

Scalable DHT lookup:
  Key/value store spread over millions of nodes
  Typical DHT interface:
    put(key, value)
    get(key) -> value
  weak consistency; likely that get(k) sees put(k), but no guarantee
  weak guarantees about keeping data alive

Why is it hard?
  Millions of participating nodes
  Could broadcast/flood each request to all nodes
    Guaranteed to find a key/value pair
    But too many messages
  Every node could know about every other node
    Then could hash key to find node that stores the value
    Just one message per get()
    But keeping a million-node table up to date is too hard
  We want modest state, and modest number of messages/lookup

Basic idea
  Impose a data structure (e.g. tree) over the nodes
    Each node has references to only a few other nodes
  Lookups traverse the data structure -- "routing"
    I.e. hop from node to node
  DHT should route get() to same node as previous put()

Example: The "Chord" peer-to-peer lookup system
  Kademlia, the DHT used by BitTorrent, is inspired by Chord

Chord's ID-space topology
  Ring: All IDs are 160-bit numbers, viewed in a ring.
  Each node has an ID, randomly chosen, or hash(IP address)
  Each key has an ID, hash(key)

Assignment of key IDs to node IDs
  A key is stored at the key ID's "successor"
    Successor = first node whose ID is >= key ID.
    Closeness is defined as the "clockwise distance"
  If node and key IDs are uniform, we get reasonable load balance.

Basic routing -- correct but slow
  Query (get(key) or put(key, value)) is at some node.
  Node needs to forward the query to a node "closer" to key.
    If we keep moving query closer, eventually we'll hit key's successor.
  Each node knows its successor on the ring.
    n.lookup(k):
      if n < k <= n.successor
        return n.successor
      else
        forward to n.successor
  I.e. forward query in a clockwise direction until done
  n.successor must be correct!
    otherwise we may skip over the responsible node
    and get(k) won't see data inserted by put(k)

Forwarding through successor is slow
  Data structure is a linked list: O(n)
  Can we make it more like a binary search?
    Need to be able to halve the distance at each step.

log(n) "finger table" routing:
  Keep track of nodes exponentially further away:
    New state: f[i] contains successor of n + 2^i
    n.lookup(k):
      if n < k <= n.successor:
        return successor
      else:
        n' = closest_preceding_node(k) -- in f[]
        forward to n'

for a six-bit system, maybe node 8's finger table looks like this:
  0: 14
  1: 14
  2: 14
  3: 21
  4: 32
  5: 42

Why do lookups now take log(n) hops?
  One of the fingers must take you roughly half-way to target

Is log(n) fast or slow?
  For a million nodes it's 10 hops (since a hop is only needed to correct a bit).
  If each hop takes 50 ms, lookups take half a second.
  If each hop has 10% chance of failure, it's a couple of timeouts.
  So: good but not great.
    Though with complexity, you can get better real-time and reliability.

Since lookups are log(n), why not use a binary tree?
  A binary tree would have a hot-spot at the root
    And its failure would be a big problem
  The finger table requires more maintenance, but distributes the load

How does a new node acquire correct tables?
  General approach:
    Assume system starts out w/ correct routing tables.
    Add new node in a way that maintains correctness.
    Use DHT lookups to populate new node's finger table.
  New node m:
    Sends a lookup for its own key, to any existing node.
      This yields m.successor
    m asks its successor for its entire finger table.
  At this point the new node can forward queries correctly
  Tweaks its own finger table in background
    By looking up each m + 2^i

Does routing *to* new node m now work?
  If m doesn't do anything,
    lookup will go to where it would have gone before m joined.
    I.e. to m's predecessor.
    Which will return its n.successor -- which is not m.
  We need to link the new node into the successor linked list.

Why is adding a new node tricky?
  Concurrent joins!
  Example:
    Initially: ... 10 20 ...
    Nodes 12 and 15 join at the same time.
    They can both tell 10+20 to add them,
      but they didn't tell each other!
  We need to ensure that 12's successor will be 15, even if concurrent.

Stabilization:
  Each node keeps track of its current predecessor.
  When m joins:
    m sets its successor via lookup.
    m tells its successor that m might be its new predecessor.
  Every node m1 periodically asks successor m2 who m2's predecessor m3 is:
    If m1 < m3 < m2, m1 switches successor to m3.
    m1 tells m3 "I'm your new predecessor"; m3 accepts if closer
      than m3's existing predecessor.
  Simple stabilization example:
    initially: ... 10 <==> 20 ...
    15 wants to join
    1) 15 tells 20 "I'm your new predecessor".
    2) 20 accepts 15 as predecessor, since 15 > 10.
    3) 10 asks 20 who 20's predecessor is, 20 answers "15".
    4) 10 sets its successor pointer to 15.
    5) 10 tells 15 "10 is your predecessor"
    6) 15 accepts 10 as predecessor (since nil predecessor before that).
    now: 10 <==> 15 <==> 20
  Concurrent join:
    initially: ... 10 <==> 20 ...
    12 and 15 join at the same time.
    * both 12 and 15 tell 20 they are 20's predecessor; when
      the dust settles, 20 accepts 15 as predecessor.
    * now 10, 12, and 15 all have 20 as successor.
    * after one stabilization round, 10 and 12 both view 15 as successor.
      and 15 will have 12 as predecessor.
    * after two rounds, correct successors: 10 12 15 20

To maintain log(n) lookups as nodes join,
  Every one periodically looks up each finger (each n + 2^i)

What about node failures?
  Nodes fail w/o warning.
  Two issues:
    Other nodes' routing tables refer to dead node.
    Dead node's predecessor has no successor.
  Recover from dead next hop by using next-closest finger-table entry.
  Now, lookups for the dead node's keys will end up at its predecessor.
  For dead successor
    We need to know what dead node's n.successor was
      Since that's now the node responsible for the dead node's keys.
    Maintain a _list_ of r successors.
    Lookup answer is first live successor >= key

Dealing with unreachable nodes during routing is important
  "Churn" is high in open p2p networks
  People close their laptops, move WiFi APs, &c pretty often
  Fast timeouts?
  Explore multiple paths through DHT in parallel?
    Perhaps keep multiple nodes in each finger table entry?
    Send final messages to multiple of r successors?
    Kademlia does this, though it increases network traffic.

Geographical/network locality -- reducing lookup time
  Lookup takes log(n) messages.
    But messages are to random nodes on the Internet!
    Will often be very far away.
  Can we route through nodes close to us on underlying network?
  This boils down to whether we have choices:
    If multiple correct next hops, we can try to choose closest.

Idea: proximity routing
  to fill a finger table entry, collect multiple nodes near n+2^i on ring
  perhaps by asking successor to n+2^i for its r successors
  use lowest-ping one as i'th finger table entry

What's the effect?
  Individual hops are lower latency.
  But less and less choice as you get close in ID space.
  So last few hops are likely to be long. 
  Though if you are reading, and any replica will do,
    you still have choice even at the end.

Any down side to locality routing?
  Harder to prove independent failure.
    Maybe no big deal, since no locality for successor lists sets.
  Easier to trick me into using malicious nodes in my tables.

What about security?
  Can someone forge data? I.e. return the wrong value?
    Defense: key = SHA1(value)
    Defense: key = owner's public key, value signed
    Defense: some external way to verify results (Bittorrent does this)
  Can a DHT node claim that data doesn't exist?
    Yes, though perhaps you can check other replicas
  Can a host join w/ IDs chosen to sit under every replica of a given key?
    Could deny that data exists, or serve old versions.
    Defense: require (and check) that node ID = SHA1(IP address)
  Can a host pretend to join millions of times?
    Could break routing with non-existant hosts, or control routing.
    Defense: node ID = SHA1(IP address), so only one node per IP addr.
    Defense: require node to respond at claimed IP address.
             this is what trackerless Bittorrent's token is about
  What if the attacker controls lots of IP addresses?
    No easy defense.
  But:
    Dynamo gets security by being closed (only Amazon's computers).
    Bitcoin gets security by proving a node exists via proof-of-work.

How to manage data?
  Here is the most popular plan.
    [diagram: Chord layer and DHT layer]
    Data management is in DHT above layer, using Chord.
  DHT doesn't guarantee durable storage
    So whoever inserted must re-insert periodically
    May want to automatically expire if data goes stale (bittorrent)
  DHT replicates each key/value item
    On the nodes with IDs closest to the key, where looks will find them
    Replication can help spread lookup load as well as tolerate faults
  When a node joins:
    successor moves some keys to it
  When a node fails:
    successor probably already has a replica
    but r'th successor now needs a copy

Summary
  DHTs attractive for finding data in large p2p systems
    Decentralization seems good for high load, fault tolerance
  But: log(n) lookup time is not very fast
  But: the security problems are difficult
  But: churn is a problem, leads to incorrect routing tables, timeouts
  Next paper: Amazon Dynamo, adapts these ideas to a closed system.

References

Kademlia: www.scs.stanford.edu/~dm/home/papers/kpos.pdf
Accordion: www.news.cs.nyu.edu/~jinyang/pub/nsdi05-accordion.pdf
Promiximity routing: https://pdos.csail.mit.edu/papers/dhash:nsdi/paper.pdf
Evolution analysis: http://nms.csail.mit.edu/papers/podc2002.pdf
Sybil attack: http://research.microsoft.com/pubs/74220/IPTPS2002.pdf
==========
6.824 2018 Lecture 19: P2P, DHTs, and Chord

Reminders:
  course evaluations
  4B due friday
  project reports due friday
  project demos next week
  final exam on May 24th

Today's topic: Decentralized systems, peer-to-peer (P2P), DHTs
  potential to harness massive [free] user compute power, network b/w
  potential to build reliable systems out of many unreliable computers
  potential to shift control/power from organizations to users
  appealing, but has been hard in practice to make the ideas work well

Peer-to-peer
  [user computers, files, direct xfers]
  users computers talk directly to each other to implement service
    in contrast to user computers talking to central servers
  could be closed or open
  examples:
    bittorrent file sharing, skype, bitcoin

Why might P2P be a win?
  spreads network/caching costs over users
  absence of central server may mean:
    easier/cheaper to deploy
    less chance of overload
    single failure won't wreck the whole system
    harder to attack

Why don't all Internet services use P2P?
  can be hard to find data items over millions of users
  user computers not as reliable as managed servers
  if open, can be attacked via evil participants

The result is that P2P has been limited to a few niches:
  [Illegal] file sharing
    Popular data but owning organization has no money
  Chat/Skype
    User to user anyway; privacy and control
  Bitcoin
    No natural single owner or controller

Example: classic BitTorrent
  a cooperative, popular download system
  user clicks on download link for e.g. latest Linux kernel distribution
    gets torrent file w/ content hash and IP address of tracker
  user's BT app talks to tracker
    tracker tells it list of other users w/ downloaded file
  user't BT app talks to one or more users w/ the file
  user's BT app tells tracker it has a copy now too
  user's BT app serves the file to others for a while
  the point:
    provides huge download b/w w/o expensive server/link

But: the tracker is a weak part of the design
  makes it hard for ordinary people to distribute files (need a tracker)
  tracker may not be reliable, especially if ordinary user's PC
  single point of attack by copyright owner, people offended by content

BitTorrent can use a DHT instead of a tracker
  this is the topic of today's readings
  BT apps cooperatively implement a "DHT"
    a decentralized key/value store, DHT = distributed hash table
  the key is the torrent file content hash ("infohash")
  the value is the IP address of an BT app willing to serve the file
    Kademlia can store multiple values for a key
  app does get(infohash) to find other apps willing to serve
    and put(infohash, self) to register itself as willing to serve
    so DHT contains lots of entries with a given key:
      lots of peers willing to serve the file
  app also joins the DHT to help implement it

Why might the DHT be a win for BitTorrent?
  more reliable than single classic tracker
    keys/value spread/cached over many DHT nodes
    while classic tracker may be just an ordinary PC
  less fragmented than multiple trackers per torrent
    so apps more likely to find each other
  maybe more robust against legal and DoS attacks

How do DHTs work?

Scalable DHT lookup:
  Key/value store spread over millions of nodes
  Typical DHT interface:
    put(key, value)
    get(key) -> value
  weak consistency; likely that get(k) sees put(k), but no guarantee
  weak guarantees about keeping data alive

Why is it hard?
  Millions of participating nodes
  Could broadcast/flood each request to all nodes
    Guaranteed to find a key/value pair
    But too many messages
  Every node could know about every other node
    Then could hash key to find node that stores the value
    Just one message per get()
    But keeping a million-node table up to date is too hard
  We want modest state, and modest number of messages/lookup

Basic idea
  Impose a data structure (e.g. tree) over the nodes
    Each node has references to only a few other nodes
  Lookups traverse the data structure -- "routing"
    I.e. hop from node to node
  DHT should route get() to same node as previous put()

Example: The "Chord" peer-to-peer lookup system
  Kademlia, the DHT used by BitTorrent, is inspired by Chord

Chord's ID-space topology
  Ring: All IDs are 160-bit numbers, viewed in a ring.
  Each node has an ID, randomly chosen, or hash(IP address)
  Each key has an ID, hash(key)

Assignment of key IDs to node IDs
  A key is stored at the key ID's "successor"
    Successor = first node whose ID is >= key ID.
    Closeness is defined as the "clockwise distance"
  If node and key IDs are uniform, we get reasonable load balance.

Basic routing -- correct but slow
  Query (get(key) or put(key, value)) is at some node.
  Node needs to forward the query to a node "closer" to key.
    If we keep moving query closer, eventually we'll hit key's successor.
  Each node knows its successor on the ring.
    n.lookup(k):
      if n < k <= n.successor
        return n.successor
      else
        forward to n.successor
  I.e. forward query in a clockwise direction until done
  n.successor must be correct!
    otherwise we may skip over the responsible node
    and get(k) won't see data inserted by put(k)

Forwarding through successor is slow
  Data structure is a linked list: O(n)
  Can we make it more like a binary search?
    Need to be able to halve the distance at each step.

log(n) "finger table" routing:
  Keep track of nodes exponentially further away:
    New state: f[i] contains successor of n + 2^i
    n.lookup(k):
      if n < k <= n.successor:
        return successor
      else:
        n' = closest_preceding_node(k) -- in f[]
        forward to n'

for a six-bit system, maybe node 8's finger table looks like this:
  0: 14
  1: 14
  2: 14
  3: 21
  4: 32
  5: 42

Why do lookups now take log(n) hops?
  One of the fingers must take you roughly half-way to target

Is log(n) fast or slow?
  For a million nodes it's 10 hops (since a hop is only needed to correct a bit).
  If each hop takes 50 ms, lookups take half a second.
  If each hop has 10% chance of failure, it's a couple of timeouts.
  So: good but not great.
    Though with complexity, you can get better real-time and reliability.

Since lookups are log(n), why not use a binary tree?
  A binary tree would have a hot-spot at the root
    And its failure would be a big problem
  The finger table requires more maintenance, but distributes the load

How does a new node acquire correct tables?
  General approach:
    Assume system starts out w/ correct routing tables.
    Add new node in a way that maintains correctness.
    Use DHT lookups to populate new node's finger table.
  New node m:
    Sends a lookup for its own key, to any existing node.
      This yields m.successor
    m asks its successor for its entire finger table.
  At this point the new node can forward queries correctly
  Tweaks its own finger table in background
    By looking up each m + 2^i

Does routing *to* new node m now work?
  If m doesn't do anything,
    lookup will go to where it would have gone before m joined.
    I.e. to m's predecessor.
    Which will return its n.successor -- which is not m.
  We need to link the new node into the successor linked list.

Why is adding a new node tricky?
  Concurrent joins!
  Example:
    Initially: ... 10 20 ...
    Nodes 12 and 15 join at the same time.
    They can both tell 10+20 to add them,
      but they didn't tell each other!
  We need to ensure that 12's successor will be 15, even if concurrent.

Stabilization:
  Each node keeps track of its current predecessor.
  When m joins:
    m sets its successor via lookup.
    m tells its successor that m might be its new predecessor.
  Every node m1 periodically asks successor m2 who m2's predecessor m3 is:
    If m1 < m3 < m2, m1 switches successor to m3.
    m1 tells m3 "I'm your new predecessor"; m3 accepts if closer
      than m3's existing predecessor.
  Simple stabilization example:
    initially: ... 10 <==> 20 ...
    15 wants to join
    1) 15 tells 20 "I'm your new predecessor".
    2) 20 accepts 15 as predecessor, since 15 > 10.
    3) 10 asks 20 who 20's predecessor is, 20 answers "15".
    4) 10 sets its successor pointer to 15.
    5) 10 tells 15 "10 is your predecessor"
    6) 15 accepts 10 as predecessor (since nil predecessor before that).
    now: 10 <==> 15 <==> 20
  Concurrent join:
    initially: ... 10 <==> 20 ...
    12 and 15 join at the same time.
    * both 12 and 15 tell 20 they are 20's predecessor; when
      the dust settles, 20 accepts 15 as predecessor.
    * now 10, 12, and 15 all have 20 as successor.
    * after one stabilization round, 10 and 12 both view 15 as successor.
      and 15 will have 12 as predecessor.
    * after two rounds, correct successors: 10 12 15 20

To maintain log(n) lookups as nodes join,
  Every one periodically looks up each finger (each n + 2^i)

What about node failures?
  Nodes fail w/o warning.
  Two issues:
    Other nodes' routing tables refer to dead node.
    Dead node's predecessor has no successor.
  Recover from dead next hop by using next-closest finger-table entry.
  Now, lookups for the dead node's keys will end up at its predecessor.
  For dead successor
    We need to know what dead node's n.successor was
      Since that's now the node responsible for the dead node's keys.
    Maintain a _list_ of r successors.
    Lookup answer is first live successor >= key

Dealing with unreachable nodes during routing is important
  "Churn" is high in open p2p networks
  People close their laptops, move WiFi APs, &c pretty often
  Fast timeouts?
  Explore multiple paths through DHT in parallel?
    Perhaps keep multiple nodes in each finger table entry?
    Send final messages to multiple of r successors?
    Kademlia does this, though it increases network traffic.

Geographical/network locality -- reducing lookup time
  Lookup takes log(n) messages.
    But messages are to random nodes on the Internet!
    Will often be very far away.
  Can we route through nodes close to us on underlying network?
  This boils down to whether we have choices:
    If multiple correct next hops, we can try to choose closest.

Idea: proximity routing
  to fill a finger table entry, collect multiple nodes near n+2^i on ring
  perhaps by asking successor to n+2^i for its r successors
  use lowest-ping one as i'th finger table entry

What's the effect?
  Individual hops are lower latency.
  But less and less choice as you get close in ID space.
  So last few hops are likely to be long. 
  Though if you are reading, and any replica will do,
    you still have choice even at the end.

Any down side to locality routing?
  Harder to prove independent failure.
    Maybe no big deal, since no locality for successor lists sets.
  Easier to trick me into using malicious nodes in my tables.

What about security?
  Can someone forge data? I.e. return the wrong value?
    Defense: key = SHA1(value)
    Defense: key = owner's public key, value signed
    Defense: some external way to verify results (Bittorrent does this)
  Can a DHT node claim that data doesn't exist?
    Yes, though perhaps you can check other replicas
  Can a host join w/ IDs chosen to sit under every replica of a given key?
    Could deny that data exists, or serve old versions.
    Defense: require (and check) that node ID = SHA1(IP address)
  Can a host pretend to join millions of times?
    Could break routing with non-existant hosts, or control routing.
    Defense: node ID = SHA1(IP address), so only one node per IP addr.
    Defense: require node to respond at claimed IP address.
             this is what trackerless Bittorrent's token is about
  What if the attacker controls lots of IP addresses?
    No easy defense.
  But:
    Dynamo gets security by being closed (only Amazon's computers).
    Bitcoin gets security by proving a node exists via proof-of-work.

How to manage data?
  Here is the most popular plan.
    [diagram: Chord layer and DHT layer]
    Data management is in DHT above layer, using Chord.
  DHT doesn't guarantee durable storage
    So whoever inserted must re-insert periodically
    May want to automatically expire if data goes stale (bittorrent)
  DHT replicates each key/value item
    On the nodes with IDs closest to the key, where looks will find them
    Replication can help spread lookup load as well as tolerate faults
  When a node joins:
    successor moves some keys to it
  When a node fails:
    successor probably already has a replica
    but r'th successor now needs a copy

Summary
  DHTs attractive for finding data in large p2p systems
    Decentralization seems good for high load, fault tolerance
  But: log(n) lookup time is not very fast
  But: the security problems are difficult
  But: churn is a problem, leads to incorrect routing tables, timeouts
  Next paper: Amazon Dynamo, adapts these ideas to a closed system.

References

Kademlia: www.scs.stanford.edu/~dm/home/papers/kpos.pdf
Accordion: www.news.cs.nyu.edu/~jinyang/pub/nsdi05-accordion.pdf
Promiximity routing: https://pdos.csail.mit.edu/papers/dhash:nsdi/paper.pdf
Evolution analysis: http://nms.csail.mit.edu/papers/podc2002.pdf
Sybil attack: http://research.microsoft.com/pubs/74220/IPTPS2002.pdf
==========
6.824 2018 Lecture 15: Dynamo
=============================

Dynamo: Amazon's Highly Available Key-value Store
DeCandia et al, SOSP 2007

Why are we reading this paper?
  Database, eventually consistent, write any replica
     Like Bayou, with reconciliation
     Like Parameter Server, but geo-distributed
     A surprising design.
  A real system: used for e.g. shopping cart at Amazon
  More available than PNUTS, Spanner, FB MySQL, &c
  Less consistent than PNUTS, Spanner, FB MySQL &c
  Influential design; inspired e.g. Cassandra
  2007: before PNUTS, before Spanner

Their Obsessions
  SLA, e.g. 99.9th percentile of delay < 300 ms
  constant failures
  "data centers being destroyed by tornadoes"
  "always writeable"

Big picture
  [lots of data centers, Dynamo nodes]
  each item replicated at a few random nodes, by key hash

Why replicas at just a few sites? Why not replica at every site?
  with two data centers, site failure takes down 1/2 of nodes
    so need to be careful that *everything* replicated at *both* sites
  with 10 data centers, site failure affects small fraction of nodes
    so just need copies at a few sites

Where to place data -- consistent hashing
  [ring, and physical view of servers]
  node ID = random
  key ID = hash(key)
  coordinator: successor of key
    clients send puts/gets to coordinator
  replicas at successors -- "preference list"
  coordinator forwards puts (and gets...) to nodes on preference list

Consequences of mostly remote access (since no guaranteed local copy)
  most puts/gets may involve WAN traffic -- high delays
    the quorums will cut the tail end --- see below
  but can survive data centers going down

Why consistent hashing?
  Pro
    naturally somewhat balanced
    decentralized -- both lookup and join/leave
  Con (section 6.2)
    not really balanced (why not?), need virtual nodes
    hard to control placement (balancing popular keys, spread over sites)
    join/leave changes partition, requires data to shift

Failures
  Tension: temporary or permanent failure?
    node unreachable -- what to do?
    if temporary, store new puts elsewhere until node is available
    if permanent, need to make new replica of all content
  Dynamo itself treats all failures as temporary

Consequences of "always writeable"
  always writeable => no master! must be able to write locally.
     idea 1: sloppy quorums
  always writeable + failures = conflicting versions
     idea 2: eventual consistency
        idea 1 avoids inconsistencies when there are no failures

Idea #1: sloppy quorum
  try to get consistency benefits of single master if no failures
    but allows progress even if coordinator fails, which PNUTS does not
  when no failures, send reads/writes through single node
    the coordinator
    causes reads to see writes in the usual case
  but don't insist! allow reads/writes to any replica if failures

Temporary failure handling: quorum
  goal: do not block waiting for unreachable nodes
  goal: put should always succeed
  goal: get should have high prob of seeing most recent put(s)
  quorum: R + W > N
    never wait for all N
    but R and W will overlap
    cuts tail off delay distribution and tolerates some failures
  N is first N *reachable* nodes in preference list
    each node pings successors to keep rough estimate of up/down
    "sloppy" quorum, since nodes may disagree on reachable
  sloppy quorum means R/W overlap *not guaranteed*

coordinator handling of put/get:
  sends put/get to first N reachable nodes, in parallel
  put: waits for W replies
  get: waits for R replies
  if failures aren't too crazy, get will see all recent put versions

When might this quorum scheme *not* provide R/W intersection?

What if a put() leaves data far down the ring?
  after failures repaired, new data is beyond N?
  that server remembers a "hint" about where data really belongs
  forwards once real home is reachable
  also -- periodic "merkle tree" sync of key range

Idea #2: eventual consistency
  accept writes at any replica
  allow divergent replicas
  allow reads to see stale or conflicting data
  resolve multiple versions when failures go away
    latest version if no conflicting updates
    if conflicts, reader must merge and then write
  like Bayou and Ficus -- but in a DB

Unhappy consequences of eventual consistency
  May be no unique "latest version"
  Read can yield multiple conflicting versions
  Application must merge and resolve conflicts
  No atomic operations (e.g. no PNUTS test-and-set-write)

How can multiple versions arise?
  Maybe a node missed the latest write due to network problem
  So it has old data, should be superseded by newer put()s
  get() consults R, will likely see newer version as well as old

How can *conflicting* versions arise?
  N=3 R=2 W=2
  shopping cart, starts out empty ""
  preference list n1, n2, n3, n4
  client 1 wants to add item X
    get() from n1, n2, yields ""
    n1 and n2 fail
    put("X") goes to n3, n4
  n1, n2 revive
  client 3 wants to add Y
    get() from n1, n2 yields ""
    put("Y") to n1, n2
  client 3 wants to display cart
    get() from n1, n3 yields two values!
      "X" and "Y"
      neither supersedes the other -- the put()s conflicted

How should clients resolve conflicts on read?
  Depends on the application
  Shopping basket: merge by taking union?
    Would un-delete items removed
    Weaker than Bayou (which gets deletion right), but simpler
  Some apps probably can use latest wall-clock time
    E.g. if I'm updating my password
    Simpler for apps than merging
  Write the merged result back to Dynamo

Programming API
  All objects are immutable
  - get(k) may return multiple versions, along with "context"
  - put(k, v, context)
    creates a new version of k, attaching context
  The context is used to merge and keep track of dependencies, and
  detect how conflicts.  It consists of a VV of the object.

Version vectors
  Example tree of versions:
    [a:1]
      |
      +-------|
           [a:1,b:2]
    VVs indicate v2 supersedes v1
    Dynamo nodes automatically drop [a:1] in favor of [a:1,b:2]
  Example:
    [a:1]
      |
      +-------|
      |    [a:1,b:2]
      |
    [a:2]
    Client must merge

Won't the VVs get big?
  Yes, but slowly, since key mostly served from same N nodes
  Dynamo deletes least-recently-updated entry if VV has > 10 elements

Impact of deleting a VV entry?
  won't realize one version subsumes another, will merge when not needed:
    put@b: [b:4]
    put@a: [a:3, b:4]
    forget b:4: [a:3]
    now, if you sync w/ [b:4], looks like a merge is required
  forgetting the oldest is clever
    since that's the element most likely to be present in other branches
    so if it's missing, forces a merge
    forgetting *newest* would erase evidence of recent difference

Is client merge of conflicting versions always possible?
  Suppose we're keeping a counter, x
  x starts out 0
  incremented twice
  but failures prevent clients from seeing each others' writes
  After heal, client sees two versions, both x=1
  What's the correct merge result?
  Can the client figure it out?

What if two clients concurrently write w/o failure?
  e.g. two clients add diff items to same cart at same time
  Each does get-modify-put
  They both see the same initial version
  And they both send put() to same coordinator
  Will coordinator create two versions with conflicting VVs?
    We want that outcome, otherwise one was thrown away
    Paper doesn't say, but coordinator could detect problem via put() context

Permanent server failures / additions?
  Admin manually modifies the list of servers
  System shuffles data around -- this takes a long time!

The Question:
  It takes a while for notice of added/deleted server to become known
    to all other servers. Does this cause trouble?
  Deleted server might get put()s meant for its replacement.
  Deleted server might receive get()s after missing some put()s.
  Added server might miss some put()s b/c not known to coordinator.
  Added server might serve get()s before fully initialized.
  Dynamo probably will do the right thing:
    Quorum likely causes get() to see fresh data as well as stale.
    Replica sync (4.7) will fix missed get()s.

Is the design inherently low delay?
  No: client may be forced to contact distant coordinator
  No: some of the R/W nodes may be distant, coordinator must wait

What parts of design are likely to help limit 99.9th pctile delay?
  This is a question about variance, not mean
  Bad news: waiting for multiple servers takes *max* of delays, not e.g. avg
  Good news: Dynamo only waits for W or R out of N
    cuts off tail of delay distribution
    e.g. if nodes have 1% chance of being busy with something else
    or if a few nodes are broken, network overloaded, &c

No real Eval section, only Experience

How does Amazon use Dynamo?
  shopping cart (merge)
  session info (maybe Recently Visited &c?) (most recent TS)
  product list (mostly r/o, replication for high read throughput)

They claim main advantage of Dynamo is flexible N, R, W
  What do you get by varying them?
  N-R-W
  3-2-2 : default, reasonable fast R/W, reasonable durability
  3-3-1 : fast W, slow R, not very durable, not useful?
  3-1-3 : fast R, slow W, durable
  3-3-3 : ??? reduce chance of R missing W?
  3-1-1 : not useful?

They had to fiddle with the partitioning / placement / load balance (6.2)
  Old scheme:
    Random choice of node ID meant new node had to split old nodes' ranges
    Which required expensive scans of on-disk DBs
  New scheme:
    Pre-determined set of Q evenly divided ranges
    Each node is coordinator for a few of them
    New node takes over a few entire ranges
    Store each range in a file, can xfer whole file

How useful is ability to have multiple versions? (6.3)
  I.e. how useful is eventual consistency
  This is a Big Question for them
  6.3 claims 0.001% of reads see divergent versions
    I believe they mean conflicting versions (not benign multiple versions)
    Is that a lot, or a little?
  So perhaps 0.001% of writes benefitted from always-writeable?
    I.e. would have blocked in primary/backup scheme?
  Very hard to guess:
    They hint that the problem was concurrent writers, for which
      better solution is single master
    But also maybe their measurement doesn't count situations where
      availability would have been worse if single master

Performance / throughput (Figure 4, 6.1)
  Figure 4 says average 10ms read, 20 ms writes
    the 20 ms must include a disk write
    10 ms probably includes waiting for R/W of N
  Figure 4 says 99.9th pctil is about 100 or 200 ms
    Why?
    "request load, object sizes, locality patterns"
    does this mean sometimes they had to wait for coast-coast msg?

Puzzle: why are the average delays in Figure 4 and Table 2 so low?
  Implies they rarely wait for WAN delays
  But Section 6 says "multiple datacenters"
    You'd expect *most* coordinators and most nodes to be remote!
    Maybe all datacenters are near Seattle?
    Maybe because coordinators can be any node in the preference list?
      See last paragraph of 5
    Maybe W-1 copies in N are close by?

Wrap-up
  Big ideas:
    eventual consistency
    always writeable despite failures
    allow conflicting writes, client merges
  Awkward model for some applications (stale reads, merges)
    this is hard for us to tell from paper
  Maybe a good way to get high availability + no blocking on WAN
  Parameter Server uses similar ideas for ML applications
    no single master, conflicting writes okay
  No agreement on whether eventual consistency is good for storage systems
==========
6.824 2018 Lecture 20: Bitcoin

Bitcoin: A Peer-to-Peer Electronic Cash System, by Satoshi Nakamoto, 2008

bitcoin:
  a digital currency
  a public ledger to prevent double-spending
  no centralized trust or mechanism <-- this is hard!
  malicious users ("Byzantine faults")

why might people want a digital currency?
  might make online payments easier
  credit cards have worked well but aren't perfect
    insecure -> fraud -> fees, restrictions, reversals
    record of all your purchases

what's hard technically?
  forgery
  double spending
  theft

what's hard socially/economically?
  why do Bitcoins have value?
  how to pay for infrastructure?
  monetary policy (intentional inflation &c)
  laws (taxes, laundering, drugs, terrorists)

idea: signed sequence of transactions
  (this is the straightforward part of Bitcoin)
  there are a bunch of coins, each owned by someone
  every coin has a sequence of transaction records
    one for each time this coin was transferred as payment
  a coin's latest transaction indicates who owns it now

what's in a transaction record?
  pub(user1): public key of new owner
  hash(prev): hash of this coin's previous transaction record
  sig(user2): signature over transaction by previous owner's private key
  (BitCoin is much more complex: amount (fractional), multiple in/out, ...)

transaction example:
  Y owns a coin, previously given to it by X:
    T7: pub(Y), hash(T6), sig(X)
  Y buys a hamburger from Z and pays with this coin
    Z sends public key to Y
    Y creates a new transaction and signs it
    T8: pub(Z), hash(T7), sig(Y)
  Y sends transaction record to Z
  Z verifies:
    T8's sig() corresponds to T7's pub()
  Z gives hamburger to Y

Z's "balance" is set of unspent transactions for which Z knows private key
  the "identity" of a coin is the (hash of) its most recent xaction
  Z "owns" a coin = Z knows private key for "new owner" public key in latest xaction

can anyone other than the owner spend a coin?
  current owner's private key needed to sign next transaction
  danger: attacker can steal Z's private key, e.g. from PC or smartphone

can a coin's owner spend it twice in this scheme?
  Y creates two transactions for same coin: Y->Z, Y->Q
    both with hash(T7)
  Y shows different transactions to Z and Q
  both transactions look good, including signatures and hash
  now both Z and Q will give hamburgers to Y

why was double-spending possible?
  b/c Z and Q didn't know complete set of transactions

what do we need?
  publish log of all transactions to everyone, in same order
    so Q knows about Y->Z, and will reject Y->Q
    a "public ledger"
  ensure Y can't un-publish a transaction

why not rely on CitiBank, or Federal Reserve, to publish transactions?
  not everyone trusts them
  they might be tempted to reverse or restrict

why not publish transactions like this:
  1000s of peers, run by anybody, no trust required in any one peer
  peers flood new transactions over "overlay"
  transaction Y->Z only acceptable if majority of peers think it is valid
    i.e. they don't know of any Y->Q
    hopefully majority overlap ensures double-spend is detected
  how to count votes?
    how to even count peers so you know what a majority is?
    perhaps distinct IP addresses?
  problem: "sybil attack"
    IP addresses are not secure -- easy to forge
    attacker pretends to have 10,000 computers -- majority
    when Z asks, attacker's majority says "we only know of Y->Z"
    when Q asks, attacker's majority says "we only know of Y->Q"
  voting is hard in "open" p2p schemes

the BitCoin block chain
  the goal: agreement on transaction log to prevent double-spending
  the block chain contains transactions on all coins
  many peers
    each with a complete copy of the whole chain
    proposed transactions flooded to all peers
    new blocks flooded to all peers
  each block:
    hash(prevblock)
    set of transactions
    "nonce" (not quite a nonce in the usual cryptographic sense)
    current time (wall clock timestamp)
  new block every 10 minutes containing xactions since prev block
  payee doesn't believe transaction until it's in the block chain

who creates each new block?
  this is "mining"
  all peers try
  requirement: hash(block) has N leading zeros
  each peer tries nonce values until this works out
  trying one nonce is fast, but most nonces won't work
    it's like flipping a zillion-sided coin until it comes up heads
    each flip has an independent (small) chance of success
    mining a block *not* a specific fixed amount of work
  it would likely take one CPU months to create one block
  but thousands of peers are working on it
  such that expected time to first to find is about 10 minutes
  the winner floods the new block to all peers

how does a Y->Z transaction work w/ block chain?
  start: all peers know ...<-B5
    and are working on B6 (trying different nonces)
  Y sends Y->Z transaction to peers, which flood it
  peers buffer the transaction until B6 computed
  peers that heard Y->Z include it in next block
  so eventually ...<-B5<-B6<-B7, where B7 includes Y->Z

Q: could there be *two* different successors to B6?
A: yes, in (at least) two situations:
   1) two peers both get lucky (unlikely, given variance of block time)
   2) network partition
  in both cases, the blockchain temporarily forks
    peers work on whichever block they heard about before
    but switch to longer chain if they become aware of one

how is a forked chain resolved?
  each peer initially believes whichever of BZ/BQ it saw first
  tries to create a successor
  if many more saw BZ than BQ, more will mine for BZ,
    so BZ successor likely to be created first
  if exactly half-and-half, one fork likely to be extended first
    since significant variance in mining success time
  peers always switch to mining the longest fork, re-inforcing agreement

what if Y sends out Y->Z and Y->Q at the same time?
  i.e. Y attempts to double-spend
  no correct peer will accept both, so a block will have one but not both

what happens if Y tells some peers about Y->Z, others about Y->Q?
  perhaps use network DoS to prevent full flooding of either
  perhaps there will be a fork: B6<-BZ and B6<-BQ

thus:
  temporary double spending is possible, due to forks
  but one side or the other of the fork highly likely to disappear
  thus if Z sees Y->Z with a few blocks after it,
    it's very unlikely that it could be overtaken by a
    different fork containing Y->Q
  if Z is selling a high-value item, Z should wait for a few
    blocks before shipping it
  if Z is selling something cheap, maybe OK to wait just for some peers
    to see Y->Z and validate it (but not in block)

can an attacker modify a block in the middle of the block chain?
  not directly, since subsequent block holds block's hash

could attacker start a fork from an old block, with Y->Q instead of Y->Z?
  yes -- but fork must be longer in order for peers to accept it
  so if attacker starts N blocks behind, it must generate N+M+1
    blocks on its fork before main fork is extended by M
  i.e. attacker must mine blocks *faster* than the other peers
  with just one CPU, will take months to create even a few blocks
    by that time the main chain will be much longer
    no peer will switch to the attacker's shorter chain
  if the attacker has 1000s of CPUs -- more than all the honest
    bitcoin peers -- then the attacker can create the longest fork,
    everyone will switch to it, allowing the attacker to double-spend

there's a majority voting system hiding here
  peers cast votes by mining to extend the longest chain

summary:
  if attacker controls majority of CPU power, can force honest
    peers to switch from real chain to one created by the attacker
  otherwise not

validation checks:
  peer, new xaction:
    no other transaction spends the same previous transaction
    signature is by private key of pub key in previous transaction
    then will add transaction to txn list for next block to mine
  peer, new block:
    hash value has enough leading zeroes (i.e. nonce is right, proves work)
    previous block hash exists
    all transactions in block are valid
    peer switches to new chain if longer than current longest
  Z:
    (some clients rely on peers to do above checks, some don't)
    Y->Z is in a block
    Z's public key / address is in the transaction
    there's several more blocks in the chain
  (other stuff has to be checked as well, lots of details)

where does each bitcoin originally come from?
  each time a peer mines a block, it gets 12.5 bitcoins (currently)
  it puts its public key in a special transaction in the block
  this is incentive for people to operate bitcoin peers

Q: what if lots of miners join, so blocks are created faster?

Q: 10 minutes is annoying; could it be made much shorter?

Q: are transactions anonymous?

Q: if I steal bitcoins, is it safe to spend them?

Q: can bitcoins be forged, i.e. a totally fake coin created?

Q: what can adversary do with a majority of CPU power in the world?
   can double-spend and un-spend, by forking
   cannot steal others' bitcoins
   can prevent xaction from entering chain

Q: what if the block format needs to be changed?
   esp if new format wouldn't be acceptable to previous s/w version?

Q: how do peers find each other?

Q: what if a peer has been tricked into only talking to corrupt peers?
   how about if it talks to one good peer and many colluding bad peers?

Q: could a brand-new peer be tricked into using the wrong chain entirely?
   what if a peer rejoins after a few years disconnection?
   a few days of disconnection?

Q: how rich are you likely to get with one machine mining?

Q: why does it make sense for the mining reward to decrease with time?

Q: is it a problem that there will be a fixed number of coins?
   what if the real economy grows (or shrinks)?

Q: why do bitcoins have value?
   e.g., 1 BTC appears to be around $8700 on may 14 2018.

Q: will bitcoin scale well?
   as transaction rate increases?
     claim CPU limits to 4,000 tps (signature checks)
     more than Visa but less than cash
   as block chain length increases?
     do you ever need to look at very old blocks?
     do you ever need to xfer the whole block chain?
     merkle tree: block headers vs txn data.
   sadly, the maximum block size is limited to one megabyte

Q: could Bitcoin have been just a ledger w/o a new currency?
   e.g. have dollars be the currency?
   since the currency part is pretty awkward.
   (settlement... mining incentive...)

key idea: block chain
  public ledger is a great idea
  decentralization might be good
  mining is a clever way to avoid sybil attacks
  tieing ledger to new currency seems awkward, maybe necessary
