6.824 2017 Lecture 8: Zookeeper Case Study

Reading: "ZooKeeper: wait-free coordination for internet-scale systems", Patrick
Hunt, Mahadev Konar, Flavio P. Junqueira, Benjamin Reed.  Proceedings of the 2010
USENIX Annual Technical Conference.

為何需要讀這篇論文？
  這是大量使用的 replicated state machine 服務
    受 Chubby(Google's global lock service) 啟發
    原本在 Yahoo!, 現在其他地方也有（Mesos, HBase, etc.)
  開源
    這是一個 Apache 專案(http://zookeeper.apache.org/)
  案例研究如何搭建replicated service, 使用 Paxos/ZAB/Raft 程式庫
    類似問題會出現在 Lab 3
  有提供 API 給更廣泛的使用案例
    需要能容錯的 master，應用不需要自幹功能
    Zookeeper 十分泛用，應用可以直接使用 zookeeper
  高效能
    不像 Lab 3 的 replicated key/value 服務

動機：在資料中心集群中，許多應用需要彼此協調
  例如：GFS
    master 針對每個 chunk 紀錄一個 chunk server 清單
    master 決定哪個 chunk server 是 primary
    etc.
  其他例子：YMB, 爬蟲, 等等
    YMB 需要 master 來 shard topics
    爬蟲需要 master 來指揮頁面的抓取
      （e.g. 有點像 MapReduce 的 master）
  應用也需要發現彼此
    MapReduce 需要知道 GFS master 的 IP:PORT
    負載均衡器需要之後後面的 web server 位置
  居中協調的服務常用在以上需求

動機：效能 -- lab3
  主要是 Raft
  考慮一個 3-節點的 Raft
  在回覆客戶端前，Raft 要做
    leader 保存 log 紀錄
    leader 平行送出訊息給 followers
      每個 follower 保存 log 紀錄
      每個 follower 回覆 leader
  -> 2 次硬碟寫入，1 趟來回
    如果是磁力硬碟： 2*10msec = 50 msg/秒
    如果是固態硬碟：2*2msec＋1msec = 200 msg/秒
  Zookeeper 效能可達 21,000 msg/秒
    非同步 call
    支援 pipelining

取代方案：為每一個應用開發容錯 master 的功能
  透過 DNS 發布元件的位置
  OK, 如果 master 的程式不是太複雜
  但，master 常常需要：
    容錯
      每個應用都導入 Raft?
    高效能
      每個應用都要考慮如何加速 "read" 操作
  DNS 散步(propagation) 很慢
    如果錯誤要切換（fail-over）會花很多時間
  有些應用可以接受單點錯誤
    E.g., GFS 與 MapReduce
    需求更低

Zookeeper: 一個泛用的協調服務
  設計的挑戰：
    如何設計 API？
    如何讓 master 容錯？
    如何達到高效能？
  挑戰彼此牽動
    高效能會影響 API 設計
    e.g., 允許 pipelining 的非同步界面

Zookeeper API 概觀
  [流程圖: Zookeeper, 客戶端 session, ZAB 層]
  replicated state machine
    多台伺服器實做
    指令依照全域順序（global order）執行
      少數例外，如果一致性不重要的情形
  複製的物件：znodes
    znodes 的樹狀結構
      命名依照路徑名稱（pathname）
    znodes 內含應用的元資料（metadata）
      設定資訊
        參與應用的機器
        哪個機器是 primary
      時間搓記
      版本號
    znodes 類別：
      一般（regular）
      短暫（empheral）
      依序（sequential）: 名稱 + 序列號（seq no）
        如果 n 是新產生的 znode，p 是上層 znode，
        則 n 的序列號不小於其他在 p 底下的依序 znodes

  sessions
    客戶端連入 zookeeper
    session 讓客戶端可以錯誤切換（failover）到其他 zookeeper 服務
      客戶知道上一個完成指令（zxid）的 term 與 index
      每次請求都傳送
        只有在服務端的進度跟上客戶端已見的進度，服務端才會執行指令
    session 會逾時
      客戶端持續更新 session
        傳送心跳（heartbeat）到伺服器（像是租約）
      Zookeeper 沒有收到客戶端訊息，會將客戶端視為失效（"dead"）
      客戶端仍然可以工作（e.g. 網路分割）
        但無法在 session 中實行 zookeeper 的指令行為
    不同於 Raft + Lab3 KV store

Znodes 的指令行為
  create(path, data, flags)
  delete(path, version)
      if znode.version = version, then delete
  exists(path, watch)
  getData(path, watch)
  setData(path, data, version)
    if znode.version = version, then update
  getChildren(path, watch)
  sync()
    上述指令都是非同步
    每個指令都依照先進先出（FIFO）順序
    sync 會等待所有前面的指令都傳遞（propagate）完成

檢查：我們是否可以藉此完成 lab 3 的 KV 服務？
  有缺陷的計畫：GFS master 在啟動時 Put("gfs-,aster", my-ip:port)
    其他應用 + GFS node 執行 Get("gfs-master")
  問題：如果兩個 master 候選的 Put()s race 呢
    後者 Put 獲勝
    每個 master 都需要讀取 key 值來確認自己真的是 master
      然而我們何時才能確認後面沒有 Put() 了?
      其他的客戶端都需要看到 master 的 Put() -- 這點很難保證
  問題：當 master 失效後，誰能決定移除 / 更新 KV store 的值？
    需要 timeout 機制
    所以 master 要在儲存 (my-ip:port, timestamp) 的資料對
      並且要持續地 Put() 來更新 timestamp
      其他應用則要持續取得資料來確定 timestamp 有更新
    很多 polling 加上不確定的 race 行為 -- 太複雜
  Zookeeper API 有更好的歷程：watches, sessions, atomic znode creation
    + 只有一個 creation 成功 -- 沒有 Put() race
    + sessions 來作到 timeout -- 不用儲存/更新 timestamp
    + watches 是 lazy 通知 -- 避免花費太多在一堆 polling 讀取上

順序保證
  所有 write 執行都是完全依序
    如果 Zookeeper 執行 write，後面執行其他 write 的用戶端會看到前面的 write
    e.g. 兩個客戶端 create znode，Zookeeper 會依序執行
  每個 client 的所有執行都是 FIFO 順序
  可推導：
    一個 read 會看到同一個客戶端前一個 write 的結果
    一個 read 會看見某部份的 writes，也許不是最近的 write（其他客戶端）
      -> 一個 read 可能會回傳過時資料
    如果一個 read 看見某部份的 writes，後面的 read 也會看見這些 writes

範例 "ready" znode:
  錯誤發生
  一個 primary 傳送一個串流的 write 到 Zookeeper
    W1 W2 ... Wn C(ready)
  最後一個 write 更新 ready znode
    -> 所有前面的 write 都可見
  最後一個 write 導致 watch 開始備份
    備份 issue R(ready) R1 R2...Rn
    然而，笨份操作會看到所有的 writes，因為 Zookeeper 會延遲 read 直到
      node 看見所有 watch 看過得 txn
    假設錯誤發生在 R1..Rn，最後一筆成功回傳給客戶的是 Rj
      primary 移除 ready 檔案 -> watch 啟動
      watch 的警示傳送給客戶端
      客戶端知道需要重新發送 R(ready) R1 ... Rn
    好處：高效能
      pipeline 讀取與寫入
      可以從 *任意* Zookeeper node 讀取

使用範例 1：slow lock
  取得 lock：
   retry:
     r = create("app/lock", "", empheral)
     if r:
       return
     else:
       getData("app/lock", watch=True)

    watch_event:
       goto retry

  釋放 lock：(主動釋放或是 session timeout)
    delete("app/lock")

使用範例 2："ticket" locks
  取得 lock：
      n = create("app/lock/request-", "", empheral|sequential)
    retry:
      requests = getChildren(l, false)
      if n is lowest znode in requests:
        return
      p = "request-%d" % n - 1
      if exists(p, watch = True)
        goto retry

    watch_event:
      goto retry

  問題：watch_even 可能會在客戶取得 lock 取得前觸發嗎？
  答案：會
     lock/request-10 <- 當前 lock 擁有者
     lock/request-11 <- 下個擁有者
     lock/request-12 <- 我的請求

     如果 request-11 的客戶取得 lock 前死了
     watch_even 會觸發，在輪到我的請求之前

使用 locks
  不直覺：一個錯誤可能造成你的 lock 被取消
    客戶 1 取得 lock
      做事情
      網路分割
      zookeeper 宣佈客戶 1 已死（但其實沒死）
    客戶 2 取得 lock，但客戶 1 仍然覺得自己有 lock
      如果 timeouts 正常的話可以避免
      need to disconnect client 1 session before ephemeral nodes go away
      需要 session 心跳複製到多數節點
        N.B. 論文沒有討論這點
  某些用例中，lock 其實是效能的優化
    例如客戶 1 取得 lock 來執行某些 url 的爬蟲
    client will do it 2 now, but that is fine
  其他用例中，locks 來建造區塊
    例如應用使用 lock 來產生 transaction
    transaction 是全有或全無
    我們會在 Frangipani 論文裡看到

Zookeeper 簡化搭建應用，使用不是 end-to-end 的解決方案
  很多難題留給應用端處理
  思考在 GFS 中使用 Zookeeper
    I.e. 把 GFS 的 master 換成 Zookeeper
  應用/GFS 仍然需要其他 GFS 的元件
    chunk 的 primary/備份計畫
    chunk 的 version number
    處理 primary failover 的協定
    etc.
  使用 Zookeeper 至少 master 是可容錯的
    而且不會造成多頭馬車
    就算有多台複本伺服器

實做概觀
  類似 lab 3
  兩層：
    Zookeeper 服務 (K/V 服務)
    ZAB 層 (Raft 層)
  Start() 來 insert ops 到底層
  有時候 ops 會在每個複本上 pop out
    這些 ops 依照 pop out 順序 commit
    在 lab 3 的 apply channel
    在 ZAB 裡 abdelver() upcall

挑戰：重複的客戶端請求
  情境
    Primary 收到客戶端請求，然後失效
    客戶端重送請求到新的 primary
  Lab 3:
    Table 來觀察重複
    限制：每個客戶同時只有一個 outstanding op
    問題：不能 pipeline 客戶請求
  Zookeeper：
    有些 ops 是 idempotent period
    有些 ops 很容易變成 idenpotent
      檢查版本然後再執行
      e.g. 把 timestamp 與版本號加到 setDataTXN

挑戰：讀取操作
  很多操作都是讀取操作
    不更改 replicated state
  這些操作要通過 ZAB/Raft 層嗎？
  複本可以直接執行 read 操作嗎？
  如果讀取都經過 Raft，效能會很慢

問題：讀取可能會讀到過期資料，只有 master 執行的話
  primary 可能會不知道自己不再是 primary 了
    網路分割導致其他 node 變成 primary
    另外的分割可能會際遇執行寫入操作
  如果舊的 primary 繼續服務讀取操作，就看不到另外一邊的寫入操作
    => 回傳過期資料

Zookeeper 的解法：（預設）不保證 non-stale 資料
  讀取允許回傳逾期資料
    所有複本都可以執行讀取
    讀取的輸出隨服務棄數量增加
    讀取會回傳見過的最後一個 zxid
      新的 primary 會知道要追上 zxid 的進度，然後才開始接受讀取
      避免讀取舊節點
    只有 sync-read() 保證資料不是陳舊的

Sync 操作：避免 sync-read 通過 ZAB 層
  必須確保 read 有看到最後 commit 的 txn
  leader 在自己與複本上放 sync 到工作佇列
    如果操作是在工作佇列 commit 前面，leader 就仍然是 leader
    否則就發出 null transaction
  與 Raft 論文的讀取操作，精神相同
    看 Raft 論文 section 8 的最後一部分

效能（看 table 1)
  讀取很便宜
    問題：為何讀取效能隨服務器數量增加？
  寫入很貴：
    問題：為何寫入效能隨服務企數量下降？
  快速錯誤復原（figure 8)
    錯誤發生時仍有很棒的輸出

References:
  ZAB: http://dl.acm.org/citation.cfm?id=2056409
  https://zookeeper.apache.org/
  https://cs.brown.edu/~mph/Herlihy91/p124-herlihy.pdf  (wait free, universal
  objects, etc.)
