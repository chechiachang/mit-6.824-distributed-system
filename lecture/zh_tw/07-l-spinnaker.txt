2018 Lecture 7: Raft (3) -- 快照, 可線性化, 重複資料檢測

這堂課：
  Raft 快照
  可線性化
  重複的 RPCs
  更快速的 get

*** 主題：log 壓縮與快照(Lab 3B)

問題：
  log 會不斷長大 -- 終究會比 state-machine 的狀態大非常多
  重新執行 log 會花費超多時間

幸運的：
  一個 server 不會同時需要完整 log ，又需要當前的狀態
    因為當前狀態，就是完整 log 執行後的結果
    客戶端只想看狀態，而不是 log
  服務的狀態通常小很多，我們繼續保持就好

server 拋棄 log 紀錄時有什麼限制？
  不能忘記 un-commmited 的紀錄 -- 這些可能已經透過 leader 達到多數
  不能忘記尚未執行(un-executed) 的紀錄 -- 因為還沒正確反應應有的狀態
  執行完的紀錄可能還會用上，用來把其他 server 更新到最新

解法：服務要定期產生持久的快照
  [流程圖: 服務與狀態，快照到硬碟，raft log，raft 持久層]
  把整個 state-machine 的狀態複製，作為一個 log 紀錄執行完的結果
    e.g. k/v table
  服務把快照寫到硬碟
  服務告訴 Raft 它快照在某個 log index
  Raft 拋棄 index 以前的 log
  一個 server 可以在任何時間產生快照，並拋棄 log
    e.g. 當 log 長太大時

快照與 log 的關係
  快照僅僅反應執行完成的 log
    也就是只有 commited 紀錄
  所以 server 也只會拋棄 commited 的部份
    還不知道是否 commited 的 log ，都會繼續保持

所以一個 server 的硬碟狀態包含：
  對一個特定 log 紀錄的服務快照
  Raft 的持久 log 與之後的 log 紀錄
  兩者合併，等效於完整的 log

失效+重啟後發生什麼事？
  服務從硬碟讀取快照
  Raft 從硬碟讀取持久的 log
    傳送 commited 但沒有在快照內的 log

如果 follower 延遲，而 leader 已經拋棄 follower 上最後的 log 了？
  nextIndex[i] 會備份紀錄 leader log 的開頭
  所以 leader 無法透過 AppendNEtries RPC 修復這個 follower
  所以使用 InstallSnashot RPC
  [Q: 為何不讓 leader 只能拋棄所有 server 都持有的 log 紀錄？]

InstallSnapshot RPC 裡有什麼？Figure 12, 13
  term
  lastIncludedIndex
  lastIncludedTerm
  snapshot data

follower 收到 InstallSnapshot 做什麼？
  拒絕任期太舊的（不是當前 leader 給的）
  拒絕（忽略）如果 follower 已經取得最後一個包含在 snapshot 的 index/任期
    因為這是一個延遲的/舊的 RPC
  清空 log ，並使用假的 "prev" 紀錄取代
  將 lastAppied 設為 lastIncludedIndex
  使用快照取代服務的狀態(e.g. k/v table)

思想筆記：
  狀態通常等於命令歷史紀錄
  使用者可以選擇使用何者來存放或是溝通
  後面課程會看到兩者互通(duality)的例子

實務筆記：
  Raft 層與服務層協作產生/復原快照
  如果狀態資料小，Raft 快照架構合理
  對於大型的 DB，如果複製好幾 Gb 的資料就不太好
    產生與寫入硬碟很慢
  也許服務的資料應該用二元樹(B-Tree)放在硬碟上
    不用在明確的保存，既然已經在硬碟上
  處理落後的複本還是很困難
    leader 要保存 log 一段時間
    或是儲存更新的紀錄的 id
  刷新整台伺服器需要圓整的狀態

*** 可線性化

我們需要定義 Lab 3 &c 的"正確"
  客戶端預期的 Put 與 Get 行為是什麼？
  通常稱作一致性合istency contract）
  幫助我們討論如何處理正確處理複雜的狀況
    e.g. 併發，複本，錯誤，RPC 重複發送，leader 改選，最佳化
  我們在 6.824 中會看到更多一致性的定義
    e.g. Spinnaker 的時間軸一致性（timeline consistency）

"可線性化"是最常用直覺的定義
  公式化單一伺服器的預期行為

可線性化的定義：
  一個執行歷史是可線性化的，如果：
    可以找到所有指令的整體歷史，
    歷史（對無重疊的執行 non-overlapping ops）符合即時(real-time)，並且
    每個 read 都看到順序中前一個 write 的值

歷史是一串客戶端指令的紀錄，每個紀錄都有
  參數，回傳值，開始時間，結束時間

範例 1：
  |寫入x1| |寫入x2|
    |--讀取x2--|
      |讀取x1|
"寫入x1" 表示對紀錄 x 寫入值 1
"讀取x1" 表示讀取紀錄 x 獲得值 1
順序： 寫入x1 讀取x1 寫入x2 讀取x2
  遵守值的限制(W -> R)
  遵守即時限制(Wx1 -> Wx2)
  這個歷史是可線性化

範例二：
  |寫入x1| |寫入x2|
    |讀取x2|
              |讀取x1|
寫入x2 然後讀取x2(值限制)，讀取x2 然後讀取x1(時間限制)，讀取x1 然後寫入x2(值限制)，但
這樣就產生迴圈 -- 所以這邊沒法轉換成為線性的順序，所以這個不是可線性化

範例三：
|寫入x0|   |寫入x1|
             |寫入x2|
       |讀取x2| |讀取x1|
順序：寫入x0 寫入x2 讀取x2 寫入x1 讀取x1
所以這是可線性化
注意：併發寫入時，服務可以選擇順序
  e.g. Raft 將併發的指令寫入 log 中

範例四：
|寫入x0|   |寫入x1|
             |寫入x2|
客戶1  |讀取x2| |讀取x1|
客戶2  |讀取x1| |讀取x2|
所有指令都需要符合單一順序
  也許： 寫x2 C1:讀x2 寫x1 C1:讀x1 C2:讀x1
    但這樣 C2:讀x2 會在哪裡？
      必須要放在 C2:讀x1 之後
      但之後又需要讀值為 1
  沒有找到合適的順序：
    C1 的讀取要求 寫x2 在 寫x1 之前
    C2 的讀取要求 寫x1 在 寫x2 之前
    陷入迴圈，沒有適合的順序
  不是可線性化
所以：併發寫入，所有的客戶端需要看見相同順序

範例五：
如果忽略前一筆寫入，就不是可線性化
寫x1      讀x1
    寫x2
這點就排除多頭馬車，以及遺失 committed 寫入紀錄

參考這個網頁：
https://www.anishathalye.com/2017/06/04/testing-distributed-systems-for-linearizability/

*** 重複的 RPC 偵測 (Lab 3)

如果 Put 或 Get RPC 逾時，客戶端要如何處置？
  i.e. Call() 回傳 false
  如果伺服器死了，或是請求被忽略：客戶端重送
  如果伺服器執行了，但請求遺失：重送就會出問題

問題：
  但是上面兩者從客戶端看都是一樣（無回應）
  就算執行完成，但客戶還是需要收到結果

想法：重複的 RPC 檢測
  我們使用 k/v 服務檢測客戶端的請求
  客戶端每次請求選擇一個 ID ，隨 RPC 傳送
    重送則使用相同 ID
  k/v 服務維護一個 ID 索引的資料表
  每個 RPC 紀錄一筆
    紀錄執行的結果
  如果第二次 RPC ID 一樣，表示是重複的 RPC
    透過資料表的結果產生回傳值

設計的謎題：
  何時我們可以清除資料表的資料？
  如果新的 leader 選出，他要如何取得重複的資料表？
  如果伺服器錯誤，要如何復原資料表？

盡量讓資料表保持小量
  每個客戶端一筆紀錄，而不是每個 RPC 一筆紀錄
  每個客戶端只有一個 RPC 同時有效
  每個客戶端的 RPC 會標記序列號
  如果伺服器收到客戶端的 RPC #10
    伺服器就可以遺棄舊的紀錄
    因為客戶端不會重送過去的 RPC

一些細節：
  每個客戶端需要有獨特的客戶端 ID -- 也許是 64-位元的隨機數字
  客戶端每個 RPC 中傳送客戶端 ID 與序列號
    重送時傳送序列號
  k/v 服務中的重複資料表用 ID 索引
    內容為序列號，以及執行結果（如果執行完成）
  RPC handler 先檢查資料表，收到的序列號大於資料表的資料時，執行 Start()
  每個 log 紀錄都要包含客戶端 ID 與序列號
  每個指令出現在 applyCh 中
    更新序列號和資料表中的資料
    然後叫醒等待中的 RPC handler (如果有的話)

如果重複的請求，比原先的請求早收到呢？
  可能會再次觸發 Start()
  log 中可能會出現兩次（相同客戶端 ID 與相同序列號）
  當 cmd 出現在 applyCh，由於資料表內已經有一樣的值，就不會再次執行

新的 leader 如何取得複製資料表？
  所有複本執行指令時，都需要更新重複資料表
  所以當被選為 leader 時，資料表已經是最新的

如果伺服器錯誤，如何復原資料表？
  如果沒有快照，就要透過 log 的回放來逐漸更新資料表
  如果有快照，快照必須包含資料表的內容

但等等！
  k/v 伺服器會回傳複製資料表中的舊的紀錄
  如果資料表中的回傳直視就的資料
  這樣 OK 嗎？

範例：
  客戶1       客戶2
  --          --
  put(x,10)
              第一次送 get(x), 回傳 10 時遺失了
  put(x,20)
              重送 get(x), 從資料表中取得 10，而非 20

可線性化判斷是：
客戶1: |寫x10|        |寫x20|
客戶2:          |讀x10------------|
順序: 寫x10 讀x10 寫x20
所以：回傳資料表記得的 10 是正確的

*** 唯讀的指令 (Section 8 最後)

問題：Raft leader 回覆客戶前，需要先 commit 唯讀的指令到 log 中嗎？
      e.g. Get(key)

意思是，leader 收到 Get() 請求時，可以直接使用目前的 k/v 資料表的資料回傳嗎？

答案：不行，除非使用 Figure 2 的架構，或是 Lab 的架構
  假設 S1 自認為是 leader，然後收到 Get(k)
  S1 有可能才剛落選（其他人勝選），但他自己還不知道
  可能因為網路封包遺失
  新的 leader, S2 可能已經處理 Put(k) 了
  所以 S1 的 key/value 是舊的
  回傳舊的資料是非可線性化的，這是多頭馬車

所以：Figure 2 要求 Get() 需要 commit 到 log 中
      如果 leader 可以 commit Get()，這表示
      （當下）它仍然是有效的 leader，以免 S1 的情形發生
      不知道自己失去 leader 資格
      要是S1 無法取得多數個 AppendEntries，也無法 commit Get()，就不會自己回覆客戶端   

但：很多應用是讀取沈重（read-heavy），commit Get() 很花時間，有方法可以避免嗎？
    這在現實系統中是很重要的考量

想法：租約期限（lease）
  更改 Raft 協議：
  定義一個租約期限，例如 5 秒
  每次 leader 取得 AppendEntries 多數後生效
    允許期限內直接回覆唯讀請求，不用 commit 唯讀請求到 log，i.e. 不用送 AppendEntries
  新的 leader 在上個租期內，不能執行 Put()
    直到租期結束
  follwer 紀錄自己的上一筆回覆 AppendEntries 的時間
    透過 RequestVote 中回傳給新的 leader
  結果：唯讀指令更快速，仍然維持可線性化

筆記：Labs 中，我們還是實做寫入 Get() 到 log 中，不用實做租期

Spinnaker 原本是讓讀取更快，犧牲可線性化
  Spinnaker 的 "時間軸讀取" 並不需要反應最新的寫入紀錄
    允許回傳陳舊（但 commit ）的資料
  Spinnaker 藉此換取速度：
    每個複本都可以回傳唯讀指令，允許讀取的負載平行化
  但時間軸讀取不是可線性化：
    複本可能還沒收到最新的寫入
    複本可能被分割到沒有 leader 的那邊！

實務上，人們往往允許陳舊資料，來交換高效能（非適用所有情形）
