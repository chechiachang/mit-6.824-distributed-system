6.824 2018 Lecture 3: GFS

The Google File System
Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung
SOSP 2003

我們為何要讀這篇論文?
  這是 map/reduce 使用的 file system
  6.824 課程的主要議題都出現在論文中
    犧牲一致性(consistency) 交換簡易性(simplicity) 與效能(performance)
    啟發後續的(分散式)設計
  非常好的系統論文(systems paper)，內容從應用層延續到網路
    效能，容錯，一致性
  這篇論文很有影響性
    許多系統都使用 GFS(e.g. bigtable, Spanner @ Google)
    HDFS(Hadoop Distributed File System) 也是基於 GFS

何為一致性(consistency)?
  描述一個正確性的狀態 
  重要但很難達成，由於資料有許多複本
    當應用層是併發(concurrently) 存取資料，更顯困難
    [流程圖: 簡單的例子，一台機器]
    如果一個應用寫入，稍後的讀取會觀察到什麼？
      如果是另外一個不同應用來讀取呢？
    然而當有複本時，每個寫入，都需要發生在所有其他機器上
    [流程圖: 增加兩台機器，多次讀取跟寫入到三台機器]
    明顯地這裡會有問題
  弱一致性(Weak consistency)
    read() 可能會回傳舊的資料 -- 不是最近一次 write() 的結果
  強一致性(Strong consistency)
    read() 總是回傳最近一次 write() 的結果
  這兩者常會出現取捨:
    應用的寫入方(writer)容易達成強一致性
    但強一致性會嚴重影響效能
    弱一致性有很好的效能，也很容易擴展
    弱一致性討論起來比較複雜
  基於不同的正確性(correctness)條件，有很多取捨(trade-offs)
    這些取捨稱為『一致性模型』(consistency models)
    幾乎每篇我們閱讀的論文，都會出現

"理想" 的一致性模型
  先回到單一機器
  好想有一個複本式的FS(replicated file system) 但行為像非複本式的 FS
    [流程圖: 同一台機器上，多個客戶端透過單一硬碟存取檔案]
  一個應用寫入，隨後讀取，會讀到寫入的結果
  如果兩個應用併發寫入相同檔案呢?
    Q: 在同一台機器上，會發生什麼?
    多數的 FS 沒有定義(undefined) -- 檔案可能會出現混雜的內容
  如果兩個應用併發寫入相同資料夾呢?
    Q: 在同一台機器上，會發生什麼?
    一個先寫入，另一個稍後執行(使用鎻(locking))

達成理想帶來的挑戰
  併發(concurrency) -- 如同我們上面看到的，現實中會有很多硬碟
  機器故障 -- 任何步驟都有可能在完成前故障
  網路分割(network partitions) -- 不一定聯繫的上所有的機器/硬碟
  這些挑戰為何很難克服?
    客戶端與伺服器端需要溝通
      這會消耗效能
    協定(protocol) 可能會很複雜 -- 詳見下週
      複雜的系統很難正確實做出來
    大部分 6.824 討論的系統都不"理想"
      例如 GFS

GFS 的目標:
  手上有很多機器，錯誤難免
    需要容錯
    假設一台機器一年只壞一次
    手上有 1000 台，平均每天壞三台
  高效能: 希望有很多併發的讀取與寫入
    MR 的任務在 GFS 上讀取，並寫入最終結果
    Note: 中介資料不會寫到 GFS
  有效率的使用網路: 節省帶寬
  這些挑戰，加上"理想"的一致性，更顯困難

上層的設計: 讀取
  [圖片 1 流程圖, master + chunk servers]
  master 儲存資料夾，檔案，名稱，開啟/讀取/寫入
    不是 POSIX
  100s 台 Linux chunk servers 有硬碟
    儲存 64MB 為單位的 chunk(每個 chunk 是一個普通的 Linux file)
    每個 chunk 都會複製共三份複本，到不同機器上

    Q: 除了增加可用性外，三個 replication 還有什麼好處?
      熱門檔案讀取的負載均衡
      親和度(affinity) 本地/就近讀取
    Q: 為何不用 RAID'd 硬碟複製每一份檔案?
      RAID 不是商用商品
      需要整台機器都能容錯(允許整台機器壞掉)，而不是只有硬碟容錯
    Q: 為何 chunk 要這麼大?
      平攤基礎開銷(overhead)，減少 master 需要儲存的狀態資料
  GFS master 知道資料夾的樹狀結構
    資料夾，知道裡面的檔案
    檔案，知道每個 64MB 檔案放在那寫 chunk server 上
    master 把上述狀態存放在記憶體中
      每個 chunk 有存放 64 bytes 的 metadata
    master 有私有的 metadata 資料庫(可復原的)
      各項操作的 log flush 到硬碟中
      偶爾會非同步(asynchronous)壓縮資料到儲存點(checkpoint)
      N.B: 不等於2.7.2 提到的應用層 checkpoint
      master 可以從電源失效中快速復原
    shadow master 稍微落後 master 一點
      可以提昇作為 master
  客戶端讀取:
    傳送檔案名稱與 chunk 索引給 master
    master 回傳存放 chunk 的 servers set
      回傳 chunk 的版本號(version #)
      客戶端會快取這些資訊
    客戶端詢問最近的 chunk server
      檢查 version # (master 與 chunk server 上的)
      如果 version # 不同，重新問 master

寫入:
  [Figure 2-檔案 offset sequence的流程圖]
  隨機的客戶端寫入既有檔案
    客戶端問 master chunk 位置 + 誰是主要(primary) chunk
    master 回應 chunk server, version #, 以及那個 chunk server 是 primary
      primary 會取得 60 秒的有效期限
    客戶端透過網路拓樸(network topology) 計算複本鍊(chain of replicas)
      pipelines network use, distributes load
    各個複本s都確認(ack) 收到即將寫入的資料
    客戶端請求 primary 寫入
      primary 指派序列號(sequence number)，並執行寫入
      命令其他複本也執行寫入
      所有動作完成後，primary 回覆確認給客戶端
    萬一有另一個併發的客戶端寫入相同的地方呢?
      C2 取得的序列號會排在 C1 之後，覆寫(overwrite)資料
      C2 又再次寫入，這次取得較優先的序列號(優於 C1，可能因為 C1 比較慢)
      C2 寫入完成，輪到 C1，一樣複寫剛才的資料
      => 所有複本都有想同資料(= 一致性)，但同時混和了 C1/C2 的資料
        (= 未定義)
  客戶端的附加(append)，(不是資料的 append)
    同樣的問題，但可以不管 C1, C2 的順序，都可以工作
    一致性，但仍然未定義
    如果只有一個客戶寫入 -- 同時滿足一致性與已定義(defined)

資料附加(append)
  客戶端附加資料
    客戶端詢問 master chunk 的位置
    客戶端把資料推送到所有複本上，指定 no offset
    資料到所有 chunk server 上之後，客戶端通知 primary
      primary 指定序列號
      primary 檢查附加的資料能否放進 chunk
        如果size 不合，pad 到填滿 chunk
      primary 選擇附加的 offset
      primary 在本地套用(apply)這些變更
      primary 轉發要求(request)到其他複本上
      假設 R3 (複本3) 在 primary 套用的途中壞了
      primary 檢測到錯誤，告訴客戶端再試一次
    客戶端先聯絡 master，然後重試
      master 可能已經添加一個 R4 到線上了(或是 R3 復原了)
      其中一個複本現在有一段落後的 byte sequence，所以沒辦法直接附加新的資料
      pad 下個 offset 到所有的複本
      primary 與 secondaries 都套用資料的變更
      primary 收到所有複本的 ack 後，回覆客戶端

Housekeeping
  master 如果沒有更新 primary 的有效期限，可以指派新的 primary
  如果有效複本的數量低於需求數量，master 會複製 chunk
  master 會調整複本的負載(rebalance) -- 分散 primary 在大部分的機器

錯誤
  chuck server 很容易就可以取代
    錯誤可能讓客戶端重試(或是一些多餘的資料)
  master 下線的話，整個 GFS 不可用
    shadow master 可以負擔 read-only 行為，但這樣可能會使用到陳舊的資料
    Q: 為何不也執行寫入工作?
    會產生多頭馬車症候群(split-brain syndrome) 詳見下堂課

所以 GFS 達到 "理想" 的一致性了嗎?
  兩個案例: 資料夾，檔案
  資料夾: 基本上達到，但是...
    Yes: 強一致性(樹狀結構只有一份紀錄在 master)
    但: master 不一定總是可用，而且擴展十分侷限
  檔案: 沒有總是達到一致性
    mutation 使用原子性附加(atomic append)
      資料可能使用兩個不同的 offset 複製
      其他複本可能在一個 offset 上出現空洞
    mutation 不使用原子性附加
      不同客戶端的資料可能混在一起
      如果在意資料混雜的問題，可以使用原子性附加，或是寫入暫存檔案，寫入完成再原子性重新命名
  不幸的客戶端可能會短時間內讀到陳舊的資料
    如果 mutation 失敗，可能會讓 chunk 資料不一致
      primary chunk server 更新 chunk
      但是出現錯誤，導致複本都沒有更新到最新資料
    客戶端可能會讀取不是最新狀態的 chunk
    直到客戶端刷新有效期限，客戶端才會知道最新的版本號(version #)

論文作者表示弱一致性不會給應用端產生太多問題
  大部分的檔案更新都是附加類型
    應用可以在附加資料中使用 UUID，來辨識重複的資料
    應用可以降低讀取的資料量(避免陳舊的資料)
  應用可以使用暫存檔案與原子性重新命名

效能(Figure 3)
  讀取有強大的輸出(資料3 個複本，且分割到不同機器上(striping))
    骨幹有 125MB/s
    網路流量幾乎飽和
  寫入到不同檔案的表現，低於最大可能性
    作者覺得是網路棧的問題
    讓複本傳播到 chunk 到其他複本時出現延遲
  併發附加到單一檔案
    則會受限儲存最後一個 chunk 的機器
  上述數據都是 15 年前的數據了!

結論
  案例討論效能，容錯，一致性
    GFS 特化給 MapReduce 應用
  哪些案例適合 GFS?
    大量的讀取跟寫入
    附加資料
    需要大量的吞吐量(3 個複本, striping)
    資料上的容錯(3 複本)
  哪些不太適合 GFS?
    需要 master 的容錯
    多而小的檔案(master 整理檔案路徑會有瓶頸)
    不允許客戶端看到陳舊資料
    不允許附加的資料可能會多餘複製

References
  http://queue.acm.org/detail.cfm?id=1594206  (discussion of gfs evolution)
  http://highscalability.com/blog/2010/9/11/googles-colossus-makes-search-real-time-by-dumping-mapreduce.html
