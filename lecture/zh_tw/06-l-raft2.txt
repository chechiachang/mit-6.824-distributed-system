6.824 2018 Lecture 6: Raft (2)

回憶一下：
  key/value 服務，像範例與 Lab3
  目標：客戶端看起來像單一非複本的 server
  目標：少數機器失效也維持可用
  注意網路分割與多頭馬車問題
  [流程圖：客戶端，k/v 層，k/v 表，Raft 層，Raft log]
  [客戶端 RPC -> Start() -> majority commit protocol -> apply change]
  "state machine", 應用，服務

一些筆記：
  多數節點回覆 AppendEntries 後 leader 才會commits/執行
  leader 發布 commit 給 follower，follower 會執行(==applyCh)
  為何只等多數節點？不等全部回覆？
    就算少數節點失效，也維持可用性的進程
  為何一次多數決就足夠？
    前後兩次多數決會重複
    繼任的 leader 取得的多數節點，一定會有小一部分節點與上次 leader 的多數節點重複
    確保繼任 leader 一定看過至少所有前任 leader 的 log
  這是全體節點的多數決（不論工作節點或失效節點），不只是活著節點的多數決

*** 議題： Raft Log (Lab 2B)

只要 leader 維持有效：
  客戶端只會與 leader 溝通
  客戶端不被 follower 行為影響

更改 leader 時的事情才需要注意
  e.g. 前任 leader 失效
  客戶端不會看到異常，要如何更改 leader
    異常例如：讀到舊資料，重複執行命令，遺失命令，命令的順序有誤, &c

我們要確保什麼？
  如果節點執行了一個 log 紀錄中的 command，其他節點在相同 log 紀錄中不會執行別的
  (Figure 3's State Machine Safety)
  為何？
    如果節點們對一個命令的意見不同，
    一次 leader 變更可能會改變客戶端看見的狀態
    這就違反我們希望看起來像單一 server 的目標
  範例：
    S1: put(k1,v1) | put(k1,v2) | ...
    S2: put(k1,v1) | put(k2,x)  | ...
    不然接受不同 server 執行不同的第二個指令

什麼情形會導致重啟後 log 分歧？
  一個 leader 在送出最後的 AppendEntries 前失效
    S1: 3
    S2: 3 3
    S3: 3 3
  更遭：相同的 log 紀錄，command 卻不同
    因為一連串的 leader 失效，e.g.
    S1:  3
    S2:  3  3  4
    S3:  3  3  5

Raft 要求 follower 接受 leade 的 log，達到強制的共識
  範例：
  S3 在任期 6 被選為 leader
  S3 送出 entri 13 的 AppendEntries
    prevLogIndex=12
    prevLogTerm=5
  S2 回覆 false (AppendEntries step 2)
  S3 遞減 nextIndex[S2] 為 12
  S3 送 AppendEntries w/ entry 12+13, prevLogIndex=11, prevLogTerm=3
  S2 刪除自己的 entry 12 (AppendEntries step 3)
  類似的狀況發生在 S1 上，只是要做的更遠

回滾（roll-back）的結果：
  每個有效的 follower 刪除與 leader 不同的最後 log
  每個有效的 follower 接受 leader 的 log 紀錄
  follower 的 log 與 leader 相同

Q: 為何忘記 S2 的 index=12 term=4 紀錄是允許的？

新的 leader 可能回滾上個任期的 commited 紀錄嗎？
  i.e. 有可能 commited 的紀錄，在新 leader 選舉過程中遺失嗎？
  這可能是嚴重災難 -- 舊的 leader 可能已經回覆 "yes" 給客戶端了
  so: Raft 需要確保選出的 leader 保有所有的 commited log

為何不直接選 log 最長的 server 作為 leader？
  example:
    S1: 5 6 7
    S2: 5 8
    S3: 5 8
  上面這個情形可能發生嗎？如何發生的？
    S1 leader 在任期 6; 失效+重啟; leader 在任期 7; 失效+維持失效
      兩次都是只完成 Append 本地的 log 就死了 
    下個任期 8; S2/S3 其中一個收到任期 7 的消息
    S2 是任期 8 的 leader，只有 S2+S3 在線，然後失效
    i.e. 規則不能只用最長 log 的節點

5.4.1 後面解釋了"選舉限制"
  RequestVote handler 只會投給 "至少有跟上進度" 的候選人
    候選人手上的 log 任期比較新，或是
    候選人有相同的任期，log 比較長或是至少一樣長
  so:
    S2 與 S3 不會投給 S1
    S2 與 S3 會投給對方
  只有 S2 或 S3 可以當 leader，這點會強迫 S1 放棄 6,7
    這點 ok，因為 6,7 沒有取得多數，也沒有 commited，尚未回覆客戶端
    -> 客戶端會再重送 6,7 的命令

重點：
  "至少有跟上進度" 的規定確保新 leader 的 log 持有所有可能已經 commited 的紀錄
  所以新的 leader 不會回滾而遺失 commited 的命令

上堂課的問題
  figure 7, top server 死了，誰可以被選出？

不同的紀錄結果，取決於 Figure 7 中最後誰勝選
  結果是 commited 或是拋棄 log
  c 的 6 與 d 的 7, 7 可能被拋棄或是 commited
  有些則會保持 comited: 111445566

如何快速的回滾
  Figure 2 設計為每個 RPC 紀錄備份 -- 太慢了!
  lab tester 可能需要更快的回滾
  論文在 Section 5.3 後面指出一個架構
    沒有太多細節，這些是我的想像，可能有更好的架構
  S1: 4 5 5      4 4 4      4
  S2: 4 6 6  or  4 6 6  or  4 6 6
  S3: 4 6 6      4 6 6      4 6 6
  S3 是任期 6 的 leader，S1 回復
  如果 follower 拒絕 AppendEntries，會回覆：
    衝突紀錄的follower 的任期
    此任期 follower 第一個紀錄的 index
  如果 leader 在衝突的任期中有 log 紀錄：
    將 nextIndex[i] 退回到 leader 衝突任期的最後一個紀錄
  不然：
    將 nextIndex[i] 退回 follower 衝突任期的第一個紀錄

*** 主題：持久化(Lab 2C)

如果 server 失效會發生什麼事？
  遺失一台 server，Raft 會繼續工作
    但我們需要盡快修復 server，以避免活著的機器低於需要的多數
  兩個策略：
  * 使用全新的 server 取代（遺失的）
    需要將完整的 log 複製到新 server 上（很慢）
    為了處理永久失效的機器，這功能當然會支援
  * 或是重啟失效的 server，回復到失效前的狀態，然後再跟上
    失效時也需要有持久層保留資料
    為了度過偶發的電源失效，這功能需要支援
  我們來談第二的策略 -- 持久層

如果 server 失效又重啟，哪些東西 Raft 需要保留？
  Figure 2 列出持久化狀態：
    log[], currentTerm, votedFor
  一個 Raft server 重啟，只有在維持當時狀態下才能重新加入
  所以必須放在持續性的(non-volatile) 的儲存
    non-volatile = 硬碟，SSD，有電池的 RAM, &c
    每次變更都要存下來
    傳送每次 RPC 或 RPC 回覆之前
  為何要紀錄 log[]?
    如果一台 server 在 commit 一個紀錄的多數方
      重啟後依然須保持紀錄，所以未來的 leader 才保證會看到這筆 commited log 紀錄
  為何要紀錄 votedFor?
    避免投票給候選人後，重啟就忘記了，然後在相同任期跑票，又投給領外一個候選人
    可能產生同任期有兩個 leader
  為何要紀錄 currentTerm?
    來確保任期數目持續遞增
    來檢查 RPC 是否來自老舊的 leader 或候選人

有些 Raft 狀態是不持久的
  commitIndex, lastApplied, next/matchIndex[]
  Raft 的算法會透過初始值重建

持久化往往是效能瓶頸
  一個硬碟寫入需要 10ms，SSD 需要 0.1ms
  持久層限制我們只能 100 ~ 10,000 ops/秒
  [其他可能的瓶頸是 RPC，透過 LAN 需要 <<1ms]
  處理慢持久層有一些方法
    每次寫入批次寫入
    使用有電池的 RAM 而非硬碟

服務（e.g. k/v server）如何在失效+重啟後回復狀態?
  簡單的方法：從空的狀態開始，重新執行整個持久的 log
    lastAppied 是非持久的，會從 0 開始，不需要額外的程式
  但全部重新執行對長時間運行的系統來說，太過緩慢
  快速：使用 Raft 快照，然後只執行後面的 log

*** 主題：log 壓縮與快照(Lab 3B)

問題：
  log 會不斷長大 -- 終究會比 state-machine 的狀態大非常多
  重新執行 log 會花費超多時間

幸運的：
  一個 server 不會同時需要完整 log ，又需要當前的狀態
    因為當前狀態，就是完整 log 執行後的結果
    客戶端只想看狀態，而不是 log
  服務的狀態通常小很多，我們繼續保持就好

server 拋棄 log 紀錄時有什麼限制？
  不能忘記 un-commmited 的紀錄 -- 這些可能已經透過 leader 達到多數
  不能忘記尚未執行(un-executed) 的紀錄 -- 因為還沒正確反應應有的狀態
  執行完的紀錄可能還會用上，用來把其他 server 更新到最新

解法：服務要定期產生持久的快照
  [流程圖: 服務與狀態，快照到硬碟，raft log，raft 持久層]
  把整個 state-machine 的狀態複製，作為一個 log 紀錄執行完的結果
    e.g. k/v table
  服務把快照寫到硬碟
  服務告訴 Raft 它快照在某個 log index
  Raft 拋棄 index 以前的 log
  一個 server 可以在任何時間產生快照，並拋棄 log
    e.g. 當 log 長太大時

快照與 log 的關係
  快照僅僅反應執行完成的 log
    也就是只有 commited 紀錄
  所以 server 也只會拋棄 commited 的部份
    還不知道是否 commited 的 log ，都會繼續保持

所以一個 server 的硬碟狀態包含：
  對一個特定 log 紀錄的服務快照
  Raft 的持久 log 與之後的 log 紀錄
  兩者合併，等效於完整的 log

失效+重啟後發生什麼事？
  服務從硬碟讀取快照
  Raft 從硬碟讀取持久的 log
    傳送 commited 但沒有在快照內的 log

如果 follower 延遲，而 leader 已經拋棄 follower 上最後的 log 了？
  nextIndex[i] 會備份紀錄 leader log 的開頭
  所以 leader 無法透過 AppendNEtries RPC 修復這個 follower
  所以使用 InstallSnashot RPC
  [Q: 為何不讓 leader 只能拋棄所有 server 都持有的 log 紀錄？]

InstallSnapshot RPC 裡有什麼？Figure 12, 13
  term
  lastIncludedIndex
  lastIncludedTerm
  snapshot data

follower 收到 InstallSnapshot 做什麼？
  拒絕任期太舊的（不是當前 leader 給的）
  拒絕（忽略）如果 follower 已經取得最後一個包含在 snapshot 的 index/任期
    因為這是一個延遲的/舊的 RPC
  清空 log ，並使用假的 "prev" 紀錄取代
  將 lastAppied 設為 lastIncludedIndex
  使用快照取代服務的狀態(e.g. k/v table)

注意狀態與執行歷史只是大約相等
  設計者可以選擇要傳送那一個
  e.g. 最後幾個命令(log 紀錄) ，給延遲落後的複本
    或是複本的整個狀態（快照），給失去硬碟的複本
  當然，複本修復可能會非常昂貴，需要特別注意

問題：
  收到 InstallSnapshot RPC 可能會導致 server 退回之前的時間嗎？
  Figure 13 step 8 可能會讓 state machine 重設回幾個命令之前的狀態嗎？
  如果是，解釋這是如何發生的，如果否，解釋為何不會發生。

*** 主題：設定變更（lab 裡不需要）

設定變更(Section 6)
  設定 = 一組 server
  有時會需要
    移動一組新的 server
    增加/減少 server 的數量
  人為啟動設定變更，Raft 來管理變更
  我們希望 Raft 正確的處理，就算過程中發生失效
    i.e. 客戶端不會發現（頂多只是稍微慢一點)

直接做為何不行？
  假設每台 server 在現在的設定，有一個 server 清單
  更改設定，告訴每台 server 新的 list
    在Raft 外，使用相同的機制
  問題：機器會在不同時間點收到變更
  範例：想要用 S4 取代 S3
    我們最多告訴 S1 與 S4 新的設定是 1,2,4
    S1: 1,2,3  1,2,4
    S2: 1,2,3  1,2,3
    S3: 1,2,3  1,2,3
    S4:        1,2,4
  囧！有兩個 leader 被選出來
    S2 與 S3 可以投 S2
    S1 與 S4 可以投 S1

Raft 的設定變更
  想法："整合共識"(joint consensus) 階段，包含新的與舊的設定
    避免某個時間點上，新的與舊的設定各自選出 leader
  系統啟動舊的 config Cold
  系統管理員詢問 leader 換到新的 config Cnew
  Rast 針對設定，有特別的 log 紀錄（一串 server 的地址)
  每個 server 使用自己手上最新一筆 log
  1. leader commit Cold 到有 Cold 的多數機器上，commit Cnew 到有 Cnew 的多數機器上
  2. Cold 與 Cnew commit 後，leader commit Cnew 到有 Cnew 的 server 上
  1. leader commits Cold,new to a majority of both Cold and Cnew
  2. after Cold,new commits, leader commits Cnew to servers in Cnew

如果 leader 在過程中不同時間失效？
  可能下個任期產生 2 個 leader 嗎？
  如果這個發生，每個 leader 需要是下列之一：
    A. 設定是 Cold，但 log 中沒有 Cold 與 Cnew
    B. 設定是 Cold 或 Cnew，log 中有 Cold 與 Cnew
    C. 設定是 Cnew，log 中有 Cnew
  我們知道選舉規則中無法產生 A+A 或 C+C 的情形
  A+B? 無法，因為 B 會同時需要 Cold 與 Cnew 的多數方
  A+C? 無法，Cold 與 Cnew commit 到 Cold 機器之前，無法應用 Cnew
  B+B? 無法，B 會同時需要 Cold 與 Cnew 的多數方
  B+C? 無法，B 會同時需要 Cold 與 Cnew 的多數方

太神拉！Raft 可以切換 server 清單，而不會產生兩個 leader

*** 主題：效能

注意：許多狀況下，並不需要高效能
  key/value 儲存可能是
  但 GFS 與 MapReduce master 就不行

多數 replicattion system 都有類似同樣狀態的效能：
  每個共識，需要一個 RPC 交換與一個硬碟寫入
  Raft 在 message complexity 上很典型

Raft 在某些設計上，選擇簡單，犧牲效能
  Follower 拒絕不照順序的 AppendEntries RPC
    而不是存下來，等到順序接上了在使用
    如果網路悚意發生亂序，這點可能很重要
  沒有針對批次做 provision，沒有 pipeline AppendEntries
  快照在大型狀態下，很浪費
  緩慢的 master 會劇烈影響效能，e.g. 地理上的複製

以下對想能影響很大：
  硬碟寫入的持久化
  Message/封包/RPC 的 overhead
  需要安照順序執行 log 的命令
  Fast path for read-only operations.

對效能有較多著墨的論文：
  Zookeeper/ZAB; Paxos Made Live; Harp
